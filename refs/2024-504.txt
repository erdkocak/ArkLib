Polylogarithmic Proofs for Multilinears over Binary Towers
Benjamin E. Diamond

Jim Posen

Irreducible

Irreducible

bdiamond@irreducible.com

jposen@irreducible.com

Abstract
The use of small fields has come to typify the design of modern, production-oriented SNARKs. In
this work, we treat multilinear polynomial commitment over tiny fields. A tiny-field polynomial —in the
nomenclature of Diamond and Posen (EUROCRYPT ’25)—is defined over a field that has fewer elements
than the polynomial itself has coefficients. We focus on multilinears over the field with just two elements.
In this work, we generically reduce the problem of tiny-field commitment to that of large-field commitment. We introduce a sumcheck-based compiler—called “ring-switching”—which, upon being fed a
multilinear polynomial commitment scheme over some large extension field, yields a further scheme over
that field’s ground field. The resulting scheme lacks embedding overhead, in that its commitment cost,
on each input, equals that of the large-field scheme on each input of identical size (in bits). Its evaluation
protocol’s overhead is linear for the prover and logarithmic for the verifier, and is essentially optimal.
Instantiating our ring-switching reduction on the BaseFold (CRYPTO ’24) large-field multilinear
polynomial commitment scheme—or more precisely on a characteristic-2 adaptation of that scheme that
we develop at length—we obtain an extremely fast polynomial commitment scheme for bit-valued multilinears. Our scheme outperforms its state-of-the-art peers, a fact we demonstrate experimentally.

1

Introduction

Today’s fastest, production-oriented SNARKs universally use small fields. Before 2022, virtually all SNARKs
operated over just one—cryptographically large—field. The small-field revolution began in earnest with
ethSTARK and Plonky2 . Those systems decouple the respective fields used within their arithmetization
and cryptography portions. That is, they use small fields—sized roughly like a 64-bit register—for their
arithmetizations; each, during its security-critical portions, passes to a cryptographically large field extension
of its arithmetization field. Subsequent production-oriented SNARKs like Plonky3 and RISC Zero have
embraced similar designs, based on 32-bit prime fields; S-two has adopted a related architecture based on
Haböck, Levit and Papini’s Circle STARK [HLP24]. By using small fields, these systems have managed
to deliver industry-leading provers, which outperform those grounded in elliptic-curve-based SNARKs (like
Sonic [MBKM19], PlonK [GWC19] and Marlin [Chi+20]).
These SNARKs all use arithmetization fields which, though relatively small, are nonetheless at least as
large as the statements they’re capable of proving. This fact is not a coincidence. Indeed, all of them operate
by, roughly, arranging their witness data into a trace table, Reed–Solomon-encoding that table’s columns,
and finally invoking a low-degree test based on FRI [BBHR18a]. Reed–Solomon codes demand alphabets
that are at least as large as their block lengths are.
A recent work of Diamond and Posen [DP25] breaks this trace-length barrier, in that it treats even
polynomials over tiny fields—fields smaller than the statement’s trace length. Crucially, that work does so
without embedding overhead, a phenomenon we now recall. One might trivially attempt to commit to a tinyfield multilinear simply by embedding its coefficients into an extension, before blackbox-invoking a standard
scheme on the resulting object. That approach would face at least two deficiencies. On the efficiency front,
it would secure no gain from the tininess of its input’s coefficients. Instead, it would impose an artificial
penalty proportional to the ratio between its input’s original coefficient bitwidth and the scheme’s native
field’s bitwidth. On the security front, it would fail to guarantee the tininess of the prover’s input, a security
desideratum which, in practice, turns out to be essential. As Diamond and Posen [DP25] argue, most
production-oriented SNARKs deployed today face embedding overhead in some form.
1

In this work, we introduce a generic reduction from the problem of tiny-field multilinear polynomial
commitment to that of large-field multilinear commitment. Our techniques are rather different from those of
Diamond–Posen [DP25]. Our reduction, applied to any large-field scheme, yields a corresponding tiny-field
scheme, which moreover lacks embedding overhead. It is agnostic to the large-field scheme used. In fact,
our reduction works even on recently introduced, state-of-the-art large-field schemes like Blaze [Bre+25] and
WHIR [ACFY25], and even on large-field schemes that haven’t been created yet. Its overhead over the
underlying large-field scheme given to it is essentially optimal, and beats that associated with alternative
constructions, like Hashcaster (we explain this further in Subsection 1.3 below).

1.1

Some Historical Remarks

Diamond and Posen [DP25] break the trace-length barrier by further decoupling two fields which, in all
of the small-field schemes noted above, coincide: the arithmetization field and the alphabet field. All of
those schemes use just one prime field—again, sized just under 32 or 64 bits—both as the coefficient field
of the polynomials committed and as the alphabet of the Reed–Solomon code used to encode them. The
scheme [DP25] makes possible the simultaneous use of a tiny arithmetization field and a small alphabet
field. Separately, that work reintroduces the use of binary fields, fields of characteristic 2; these fields have
figured in various previous works, like FRI [BBHR18a] and STARK [BBHR18b]. Finally, that work treats
exclusively multilinear polynomials; in this capacity, it extends an important line of work which includes
Libra [Xie+19], Virgo [ZXZS20], Spartan [Set20], Brakedown [Gol+23], and HyperPlonk [CBBZ23].
To make their technique work, Diamond and Posen [DP25] tie together these various threads. They
introduce a data-casting operation—which they call packing, and which is based on field extensions—which
serves to recast a witness defined over F2 , say, into a shorter witness over the larger field F232 . They then
apply a Brakedown-like multilinear commitment procedure to the resulting witness, whose coefficient field,
crucially, is large enough to be used as a Reed–Solomon alphabet. Using various mathematical techniques,
those authors manage to make that scheme work (using Brakedown in a naı̈ve, fire-and-forget manner on the
packed, F232 -witness would lead to information loss). That work, therefore, treats three generally distinct
fields at once: the tiny coefficient field, the small alphabet field, and the huge cryptographic field.
We mention a further observation essential to that work. In those small-field schemes above—which are
themselves based on the DEEP-ALI [BGKS19] paradigm—the Reed–Solomon code plays two separate roles
at once. On the one hand, it plays the role of an error-correcting linear block code, a mathematical object
that amplifies errors and corruptions and makes them efficiently detectable. On the other hand, it serves
the distinct end of polynomial extrapolation. It is essential to those DEEP-ALI-based schemes that the
Reed–Solomon codewords that arise within them be, semantically, evaluations of polynomials. In particular,
those constraint polynomials which, if the prover is honest, must vanish identically over its witnesses must
likewise vanish identically over the Reed–Solomon encodings of those witnesses.
As Diamond and Posen [DP25] implicitly observe, unlike DEEP-ALI [BGKS19], Brakedown [Gol+23]—
like its predecessor works Bootle, Chiesa and Groth [BCG20] and Ron-Zewi and Rothblum [RR24] do—
decouples the coding-theoretic specifications of its code from the semantics of its code. That is, Brakedown’s
Ligero-inspired [AHIV23] polynomial commitment scheme uses its code only for error-amplification; the
semantics of that code are irrelevant to it. (That protocol could freely substitute in place of its code an
otherwise-arbitrary code of identical alphabet, message length, block length, and distance, to no effect.)
This decoupling makes Diamond and Posen’s packing procedure coherent, since that procedure garbles the
semantics of Reed–Solomon extrapolation.
On the other hand, most transparent, hash-based proofs that achieve polylogarithmic verifiers—like
Fractal [COS20], or those based on DEEP-ALI [BGKS19]—use univariate quotienting. (We refer also to
Haböck [Hab22] for a useful survey of these techniques.) As Diamond and Posen [DP25] note, quotienting
seems incompatible with their packing technique.
Zeilberger, Chen and Fisch’s BaseFold PCS [ZCF24, § 5] seems to be the first multilinear polynomial
commitment scheme with a polylogarithmic verifier that doesn’t use quotienting. That work proves Reed–
Solomon codes foldable just in the odd-characteristic, multiplicative case. BaseFold is simple, elegant, and
efficient, and is a compelling candidate for adaptation to the binary case.
This work’s small-field PCS works in a drop-in way with the PIOP of [DP25]. By combining this work’s
PCS with that work’s PIOP, one stands to obtain an efficient, full-blown SNARK for binary witnesses.

2

1.2

Our Contributions

We sketch our contributions here; in Subsections 1.3 and 1.4 below, we explain them in more detail.
A reduction from tiny-field commitment to huge-field commitment. Ring-switching is a generic
compiler, which, on input a multilinear polynomial commitment scheme over some large extension field
L / K, yields a new multilinear polynomial commitment scheme over the ground field K. Here, L can be
of any characteristic, including 2. Each scheme our compiler outputs lacks embedding overhead, in that its
commitment cost on each K-multilinear equals the original scheme’s commitment cost on an L-multilinear
of equal size in bits.
We write 2κ for the degree of L over K, which we assume is a power of 2. Diamond and Posen’s [DP25, § 4]
packing procedure takes in an ℓ-variate multilinear t(X0 , . . . , Xℓ−1 ) over K and yields an ℓ − κ-variate multilinear t′ (X0 , . . . , Xℓ−κ−1 ) over L. (Packing works by interpreting each 2κ -element chunk of t(X0 , . . . , Xℓ−1 )’s
Lagrange coefficient vector as an L-element, by basis-combination.) The multilinears t(X0 , . . . , Xℓ−1 ) and
t′ (X0 , . . . , Xℓ−κ−1 ) are of the same size in bits; they contain “the same amount of information”. Our Kscheme’s commitment procedure, on the input t(X0 , . . . , Xℓ−1 ), simply packs t(X0 , . . . , Xℓ−1 ) and invokes
the underlying L-scheme’s commitment procedure on t′ (X0 , . . . , Xℓ−κ−1 ), the result. The hard part is relating an evaluation claim on t(X0 , . . . , Xℓ−1 ) to one on t′ (X0 , . . . , Xℓ−κ−1 ). To do this, we devise an unusual
matrix-transposition trick, and write down an ℓ − κ-round sumcheck between t′ (X0 , . . . , Xℓ−κ−1 ) and a new
sort of polynomial we call a ring-switch equality indicator. Polynomials of this latter sort can be evaluated
succinctly by the verifier, but showing that they can is tricky. The evaluation algorithm at issue again entails
matrix-transposition, this time carried out ℓ − κ times, in alternation with L-scaling operations. The idea
is that a list of 2κ L-elements amounts to a 2κ × 2κ matrix of K-elements. By transposing this matrix, we
obtain a “dual” list of L-elements, again of length 2κ . We record further details in Subsection 1.3.
An improved multilinear PCS for large binary fields, based on BaseFold. Zeilberger, Chen and
Fisch’s BaseFold PCS [ZCF24, § 5] is an important multilinear polynomial commitment scheme for large
prime fields. In order to spin up a target for our ring-switching reduction, we develop a characteristic-2
variant of BaseFold PCS, i.e. for large binary fields. We also improve that scheme by incorporating into it
higher-arity folding, an optimization that reduces its proof sizes by more than half. The incompatibility of
BaseFold—as written—with higher-arity folding has been noted by previous authors (see e.g. Arnon, Chiesa,
Fenzi and Yogev’s WHIR [ACFY25, § 1.1]).
To make higher-arity folding work with BaseFold, we introduce oracle-skipping, a new higher-arity folding
mechanism. In contrast with FRI’s original approach [BBHR18a, § 3.2] to higher-arity folding—based on
univariate polynomials—oracle-skipping simply carries out the usual, 2-to-1 folding procedure repeatedly,
skipping intermediate oracles. Our security proof necessitates a different sort of proximity gap than FRI’s
approach does, one pertaining not to low-degree curves but instead to tensor combinations. Precisely this
kind of proximity gap is established in recent work of Diamond and Gruen [DG25], which we leverage.
Oracle-skipping narrows the proof-size gap between BaseFold and WHIR in the proven-security, unique
decoding regime. For example, at the 100-bit security level and using the rate ρ = 12 , we obtain a proof size
of 488 KiB for large-field multilinears on ℓ = 24 variables, compared to WHIR-UD’s 390 KiB. In the ℓ = 26
large-field case, we obtain 563 KiB, while WHIR-UD reports 441 KiB [ACFY25, § 6.3.1]. We discuss our
techniques further in Subsection 1.4.
A state-of-the-art PCS for multilinears over tiny binary fields. Putting the two parts above together, we obtain an extremely fast PCS for tiny-field multilinears. We outline our combined scheme in
Section 5 below. We benchmark our scheme against Plonky3 , a state-of-the-art competitor, using Binius,
an open-source SNARK that implements this work’s PCS. On a 28-variate multilinear over F2 , our scheme
achieves singlethreaded commitment and opening times of just 144 and 160 milliseconds, respectively; our
multithreaded commitment and opening times are 44 and 71 milliseconds, respectively (see Table 2). Our
proofs for polynomials of this size are 0.351 MiB, and our verifier runs in under a millisecond.
Our measurements beat Plonky3’s by between one and two orders of magnitude. Plonky3 takes 25 and
13 seconds, respectively, to commit a comparable amount of data, in the singlethreaded setting, and 4 and
2 seconds in multithreaded mode. Its proofs are also larger, and its verifier is around 20 milliseconds.

3

1.3

Ring-Switching

In this subsection, we gently introduce ring-switching, prioritizing technical simplicity and accuracy. We fix
a field extension L / K of power-of-2 degree 2κ . Though ring-switching works for any such field extension, of
any characteristic, the special case K = F2 and L = F2128 is especially important and exemplary. We write
Bκ := {0, 1}κ for the κ-dimensional unit cube.
The problem. We assume access to a large-field multilinear polynomial commitment scheme for multilinears over L. How might we obtain a small-field commitment scheme for multilinears over K, assuming
access to that large-field scheme?
We begin with a small-field multilinear, say t(X0 , . . . , Xℓ−1 ) ∈ K[X0 , . . . , Xℓ−1 ]⪯1 , that we’d like to
commit to. Following Diamond and Posen [DP25], we fix a basis (βv )v∈Bκ of L over K, write ℓ′ := ℓ − κ,
and define the packed multilinear :
X
t′ (X0 , . . . , Xℓ′ −1 ) :=
t(v0 , . . . , vκ−1 , X0 , . . . , Xℓ′ −1 ) · βv .
(1)
v∈Bκ

The multilinear t′ (X0 , . . . , Xℓ′ −1 ) is the packing of t(X0 , . . . , Xℓ−1 ), as Diamond and Posen explain. This
relationship is easiest to see at the level of Lagrange coefficient vectors. For each w ∈ Bℓ′ , the wth Lagrange
th κ
coefficient t′ (w) is the basis-combination, over
P (βv )v∈Bκ , of the w 2 -element chunk of t(X0 , . . . , Xℓ−1 )’s
′
Lagrange coefficient vector; that is, t (w) = v∈Bκ t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ) · βv .
How should we commit to t(X0 , . . . , Xℓ−1 )? Our commitment procedure is simple: it just packs
t(X0 , . . . , Xℓ−1 ) and invokes the underlying large-field scheme’s commitment procedure on t′ (X0 , . . . , Xℓ′ −1 ),
the result. This procedure lacks embedding overhead, since t(X0 , . . . , Xℓ−1 ) and t′ (X0 , . . . , Xℓ′ −1 ) are of the
same size (in bits).
To check an evaluation claim on t(X0 , . . . , Xℓ−1 ), we must reduce it to one on t′ (X0 , . . . , Xℓ′ −1 ). We fix
?

a point (r0 , . . . , rℓ−1 ) and an evaluation claim s = t(r0 , . . . , rℓ−1 ). We emphasize that the evaluation point r
is defined over L, and not over K (we refer to [DP25, § 3.2], as well as to Subsection 2.7 below, for security
definitions).
A strawman approach. We begin with a tempting “strawman” approach, which exhibits the difficulty
of the problem. This first approach, though correct, is insecure. On the other hand, it prefigures a few of
our techniques, and serves as a jumping-off point.
This simple technique proceeds in the following way. First, the prover sends values (ŝv )v∈Bκ which—it
claims—respectively satisfy
?
ŝv = t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ),
(2)
for each v ∈ Bκ . The verifier begins by checking whether
X
?
s=
e
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · ŝv

(3)

v∈Bκ

holds. This equality will certainly hold if the prover is honest, since
X
t(r0 , . . . , rℓ−1 ) =
e
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ).
v∈Bκ

On the other hand, if s ̸= t(r0 , . . . , rℓ−1 ), then the prover can cause (3) to pass only by sending claims
(ŝv )v∈Bκ for which at least one of the equalities (2) does not hold.
The linear combination, over the basis (βv )v∈Bκ , of all 2κ instances of (2) is:
X
X
?
ŝv · βv =
t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ) · βv .
(4)
v∈Bκ

v∈Bκ

On the other hand, by (1), the right-hand side of (4) is simply t′ (rκ , . . . , rℓ−1 ), which P
the verifier has
direct access to. The verifier may thus use the underlying large-field scheme to compare v∈Bκ βv · ŝv to
t′ (rκ , . . . , rℓ−1 ). We summarize this approach in Figure 1 below.
4

P(r, s; t)

V(r, s)

for each v ∈ Bκ , set ŝv := t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ).

(ŝv )v∈Bκ

?

X

check s =

eq(v
e 0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · ŝv .

v∈Bκ
′
set s :=

X

ŝv · βv .

v∈Bκ
′ ?

′

check s = t (rκ , . . . , rℓ−1 ) using large-field scheme.

Figure 1: A simple—but insecure—strawman variant of ring-switching.
While this protocol is complete, it’s not secure. The problem is the linear combination (4) of (2). The
set (βv )v∈Bκ is a basis of L over K. This means that unequal K-vectors yield unequal combinations. This
only works, though, for coefficient vectors over K. In other words, (βv )v∈Bκ is linearly independent over K,
but not over L. It’s easy to write down two unequal L-vectors whose combinations with (βv )v∈Bκ yield the
κ
κ
same L-element. Put differently, (βv )v∈Bκ induces an injection from K 2 → L, but not from L2 → L.
The problem is that the individual equations (2) are defined over L, and not K. Thus, combining them is
not secure. In particular, the prover can easily contriveP
to construct values (ŝv )v∈Bκ that don’t individually
equal (t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ))v∈Bκ , but for which v∈Bκ ŝv · βv = t′ (rκ , . . . , rℓ−1 ) nonetheless holds.
Our solution. Our idea is—very roughly—to further decompose the claims (2), until they are defined
over K. We may then apply the “tempting” linear combination (4) to the resulting decomposed claims,
proceeding “slice-wise”. We explain the details now.
For each v ∈ Bκ , the verifier can freely basis-decompose the prover’s quantity ŝv , writing
X
ŝv =
ŝu,v · βu
(5)
u∈Bκ

for appropriate K-elements (ŝu,v )u∈Bκ . Moreover, for each v ∈ Bκ ,
t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ) =

X

e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ).

(6)

w∈Bℓ′

Combining (5) and (6), we basis-decompose (2) as follows. For each v ∈ Bκ , we have the claim:
X
X
?
ŝu,v · βu =
e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ).
u∈Bκ

(7)

w∈Bℓ′

We’re not quite done. While, for each w ∈ Bκ , t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ) is of course a K-element,
e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) is not. On the other hand, we can basis-decompose these latter quantities
too. For each w ∈ Bℓ′ , we can freely write down K-elements (Aw,u )u∈Bκ for which
e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) =

X

Aw,u · βu .

(8)

u∈Bκ

Using these, we further re-express (7) in the following way. For each v ∈ Bκ , we obtain the claim:
X
X
?
ŝu,v · βu =
e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 )
(this is (7).)
u∈Bκ

w∈Bℓ′

!
=

X

X

w∈Bℓ′

u∈Bκ

Aw,u · βu

· t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 )


=

X


X


u∈Bκ

(by definition of (Aw,u )u∈Bκ (8).)

Aw,u · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ) · βu .

w∈Bℓ′

We’ve finally reached something we can basis-decompose!
5

(rearrange the sum.)

Indeed, everything in the leftmost and rightmost sides of the above identity is defined over K, except
for the basis-elements (βu )u∈Bκ . To check the claim just above for each v ∈ Bκ , it’s thus equivalent for the
verifier to check whether, for each u ∈ Bκ and each v ∈ Bκ ,
X
?
ŝu,v =
Aw,u · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ).
(9)
w∈Bℓ′

Here is the key point: unlike (2), the claims (9) are purely defined over K. We can thus basis-combine the
claims (9) using (βv )v∈Bκ , as opposed to the claims (2). We do exactly this. For each u ∈ Bκ , combining (9)
over v ∈ Bκ , we obtain the claim:


X
X
X
?

ŝu,v · βv =
Aw,u · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ) · βv
(combine (9) over (βv )v∈Bκ .)
v∈Bκ

v∈Bκ

w∈Bℓ′

!
=

X

Aw,u ·

w∈Bℓ′

=

X

X

t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ) · βv

(rearrange the sum.)

v∈Bκ

Aw,u · t′ (w0 , . . . , wℓ′ −1 ).

(by the definition (1).)

w∈Bℓ′

Up to defining
X

ŝu,v · βv ,

(10)

Aw,u · t′ (w0 , . . . , wℓ′ −1 )

(11)

ŝu :=

v∈Bκ

we thus have the claims

?

ŝu =

X
w∈Bℓ′

for each u ∈ Bκ . This combination is secure. Moreover, its right-hand side depends only on t′ (X0 , . . . , Xℓ′ −1 ),
as well as on (Aw,u )w∈Bℓ′ . We’re getting close to something that we can run the sumcheck on.
The verifier must check (11) for each u ∈ Bκ . In light of standard sumcheck batching techniques,
κ
though, this fact presents no obstacle. In practice, up to a soundness error of just |L|
, the verifier
′′
′′
can simply sample further random
scalars (r0 , . . . , rκ−1 ), and batch both sides of (11) by the L-vector

′′
e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) u∈Bκ , over varying u ∈ Bκ . That is, the verifier may check whether
!
X
X X
?
′′
′′
e
fq(u, r ) · Aw,u · t′ (w)
(12)
e
fq(u, r ) · ŝu =
u∈Bκ

w∈Bℓ′

u∈Bκ

holds. We summarize our amended protocol in Figure 2 below.
V(r, s)

P(r, s; t)
for each v ∈ Bκ , set ŝv := t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ).

(ŝv )v∈Bκ

?

check s =

X

eq(v
e 0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · ŝv .

v∈Bκ

for each v ∈ Bκ , decompose ŝv =

X

ŝu,v · βu .

u∈Bκ

for each u ∈ Bκ , combine ŝu :=

X

ŝu,v · βv .

v∈Bκ
′′

′′

(r0 , . . . , rκ−1 )

′′

′′

κ

sample batching scalars (r0 , . . . , rκ−1 ) ← L .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . P and V run the sumcheck (12). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
at the end of the sumcheck, evaluate
X
′′ 
′
′
′
eq
e u, r
· Au (r ) and t (r ),
u∈Bκ
′

′

where r = (r0 , . . . , rℓ′ −1 ) is the sumcheck challenge.

Figure 2: A rough sketch of our full ring-switching protocol.
6

There is one, final issue that we’ve glossed over. In Figure 2 above, for each u ∈ Bκ , we abbreviate
Au (X0 , . . . , Xℓ′ −1 ) for the multilinear extension of the function Au : w 7→ Aw,u on Bℓ′ . At the end of the sumcheck, the verifier may learn t′ (r0′ , . . . , rℓ′ ′ −1 ) by invoking the underlying large-field scheme’s evaluation pro
tocol once. How might the verifier locally—and efficiently—obtain the evaluations Au (r0′ , . . . , rℓ′ ′ −1 ) u∈B ?
κ
This issue brings us to the trickiest part of our theory.
We claim that the multilinears

(Au (X0 , . . . , Xℓ′ −1 ))u∈Bκ are succinct. In fact, the verifier may learn the whole list Au (r0′ , . . . , rℓ′ ′ −1 ) u∈B ,
κ
in one shot, by expending just 2 · 2κ · ℓ′ L-multiplications. This fact is non-obvious; we sketch it now.
To do this, we recall how the multilinears Au (X0 , . . . , Xℓ′ −1 ) are defined. Indeed, for each u ∈ Bκ ,
Au (X0 , . . . , Xℓ′ −1 ) is defined, in the Lagrange basis, as the uth “coordinate slice” of the partially-specialized
equality indicator e
fq(rκ , . . . , rℓ−1 , X0 , . . . , Xℓ′ −1 ). Moreover, for each u ∈ Bκ ,
X
Aw,u · e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 )
(13)
Au (r0′ , . . . , rℓ′ ′ −1 ) =
w∈Bℓ′
κ



























.
.
.

eq(1,
e
. . . , 1, r0′ , . . . , rℓ′ ′ −1 )

eq(0,
e
. . . , 0, r0′ , . . . , rℓ′ ′ −1 )

A(0,...,0) (r0′ , . . . , rℓ′ ′ −1 )

=

A(1,...,1) (r0′ , . . . , rℓ′ ′ −1 )

+···+

eq(r
e κ , . . . , rℓ−1 , 1, . . . , 1)




























eq(r
e κ , . . . , rℓ−1 , 0, . . . , 0)

u ∈ Bκ varying

obviously holds. Writing all 2 copies—i.e., over varying u ∈ Bκ —of the sum (13) into the rows of a square,
2κ × 2κ matrix, we obtain the sum expression depicted in Figure 3 below.

w ∈ Bℓ′ varying


Figure 3: Vertically stacking (13), we express Au (r0′ , . . . , rℓ′ ′ −1 ) u∈B as the rows of an unusual sum.
κ

In Figure 3, for each w ∈ Bℓ′ , we’ve drawn a square in which e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) is written vertically and e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 ) horizontally. By each such square, we mean the 2κ × 2κ K-matrix
whose cells are the “exterior product” of e
fq(rκ . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) and e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 ).
I.e., we basis-decompose both L-elements, and take all cross-products between the resulting two K-vectors.
Why did we do such a thing? Because this is simply the definition of (13)! Indeed, if we “zoom into” a
single row u ∈ Bκ of Figure 3, then we obtain (13) on the nose (we recall that Aw,u is defined to be the uth
coordinate slice of e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 )).

The point of Figure 3 is that it lets us explain our succinct algorithm for Au (r0′ , . . . , rℓ′ ′ −1 ) u∈B . If we
κ
write ⋆ for the “exterior product” operation between K-vectors, then Figure 3 shows us that:
X

Au (r0′ , . . . , rℓ′ ′ −1 ) u∈Bκ =
e
fq(rκ . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) ⋆ e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 ).
(14)
w∈Bℓ′

The right-hand side of (14) almost looks like the equality indicator expression e
fq(rκ , . . . , rℓ−1 , r0′ , . . . , rℓ′ ′ −1 ),
which we know is succinct. The problem is the ⋆ operation.
We state without proof
 (though see Section 3) that, in characteristic 2, the following efficient procedure
yields Au (r0′ , . . . , rℓ′ ′ −1 ) u∈B . The verifier should maintain a 2κ × 2κ K-matrix, initialized to be 1 in the
κ
top-left cell and 0 elsewhere. For each i ∈ {0, . . . , ℓ′ − 1}, the verifier should L-multiply each column of this
matrix by rκ+i and each row of it by ri′ , and update its running matrix by adding to it these two scalings.
We interpret rows and columns as L-elements by basis-combination. The result of this procedure will be
Au (r0′ , . . . , rℓ′ ′ −1 ) u∈B . To explain why this works, we must get into a bit of algebra. Everything hinges on
κ
L ⊗K L, the tensor product of L with itself over its own subfield K, an object we call the “tensor algebra”.
7

Hashcaster. We compare ring-switching in detail to Hashcaster. Soukhanov’s Hashcaster [Sou24] is a
SNARK for binary (i.e., specifically F2 -valued) witnesses. At the PIOP level, that work introduces a number
of innovations, including an efficient “ternary” sumcheck for domains of power-of-3 size. For the purposes of
this work, we survey just that work’s ideas at the PCS level, which are also important. Indeed, that work
yields an alternative reduction from the problem of evaluating the K-multilinear t(X0 , . . . , Xℓ−1 ) to that
of evaluating its packed L-multilinear t′ (X0 , . . . , Xℓ′ −1 ). For self-containedness, we reproduce that work’s
technique here in some detail; we then compare it to ring-switching, the approach of this work.
We again fix a degree-2κ field extension L / K, and write (βv )v∈Bκ for a basis of L over K. We write
σ ∈ Gal(L / K) for the Frobenius automorphism of L over K. As a notational device, for each v ∈ Bκ , we
Pκ−1
write {v} := i=0 2i · vi .
Hashcaster begins with the same observation as Subsection 1.3 does. That is, for the verifier to assess
?
the evaluation claim s = t(r0 , . . . , rℓ−1 ), it suffices for the prover to send it quantities (ŝv )v∈Bκ respectively
claimed to equal (t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ))v∈Bκ (recall (2)). Indeed, given these, the verifier can check
X
?
e
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · ŝv ,
(15)
s=
v∈Bκ

as usual (just as in (3)).
At this point, Hashcaster diverges. Hashcaster’s idea is to relate the claimed partial evaluations (ŝv )v∈Bκ

to the respective evaluations of the packed multilinear t′ (X0 , . . . , Xℓ′ −1 ) at σ {v} (rκ ), . . . , σ {v} (rℓ−1 ) v∈Bκ ,
the componentwise Galois orbit of the suffix (rκ , . . . , rℓ−1 ).
There are various ways to make this task precise. Hashcaster’s approach hinges on the following matrix
identity:


   





σ

{u}

(βv )


   

   ?  {u} ′
 ·  ŝv  = σ (t )(rκ , . . . , rℓ−1 ).

   

(16)

On the left, we have the 2κ ×2κ matrix whose ({u}, {v})th entry is σ {u} (βv ), the {u}th Galois image of the v th
basis vector. On the right-hand side, we have the vector containing the respective evaluations at (rκ , . . . , rℓ−1 )
of t′ (X0 , . . . , Xℓ′ −1 )’s various “Galois twists”. Indeed, for each u ∈ Bκ , we define σ {u} (t′ )(X0 , . . . , Xℓ′ −1 ) by
the Lagrange basis
σ {u} : w 7→ σ {u} (t′ (w)), for each w ∈ Bℓ′ . It is a nontrivial fact of field theory
h prescription
i
that the matrix σ {u} (βv ) is nonsingular; we refer to Lidl and Niederreiter [LN96, Lem. 3.51]. Assuming

this fact, it’s not too hard to show that (16) holds if and only if the prover is honest (i.e., if each of its
?

claims ŝv = t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ), for v ∈ Bκ , is true).
Upon receiving the prover’s vector of claims (ŝv )v∈Bκ , the verifier could thus compute the matrix transformation (16) on that vector. By moreover “peeling off” the twists σ {u} , for each u 
∈ Bκ , the verifier
could thereby obtain a further vector supposedly equal to t′ σ {−u} (rκ ), . . . , σ {−u} (rℓ−1 ) u∈Bκ , the vector
of evaluations of t′ (X0 , . . . , Xℓ′ −1 ) on the Galois orbit of (rκ , . . . , rℓ−1 ).
In fact, we will do Hashcaster one better. We claim that, up to performing a transposition of the
sort already discussed above in the context of ring-switching, the
 verifier may simplify its job; specifically,
it might directly relate (ŝv )v∈Bκ to t′ σ {v} (rκ ), . . . , σ {v} (rℓ−1 ) v∈B (with no twists necessary). Indeed,
P κ
given P
(ŝv )v∈Bκ , the verifier may freely, as before, decompose ŝv = u∈Bκ ŝu,v · βu (just as in (5)), and write
ŝu := v∈Bκ ŝu,v · βv (just as in (10)). In this setting, we obtain the further identity:


h
|

ŝu
{z

u∈Bκ varying

i 

·
} 

σ {v} (βu )

 h
 ?
=
 |


t′ σ {v} (rκ ), . . . , σ {v} (rℓ−1 )
{z
v∈Bκ varying
?

i

. (17)

}

We claim that (17) also holds if and only if the prover is honest (i.e., if ŝv = t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ) for
each v ∈ Bκ ). Importantly, the right-hand side of (17) directly yields the evaluations of t′ (X0 , . . . , Xℓ′ −1 )
over (rκ , . . . , rℓ−1 )’s Galois orbit, with no “twisting” or “peeling off” necessary.
8

To check the validity of the prover’s claim, then, it’s thus enough for the verifier to compute


h
|

i

sv
{z

v∈Bκ varying

}

:=

h
|

ŝu
{z

u∈Bκ varying

i 

·
} 

σ

{v}



,


(βu )

(18)

i.e. the left-hand side of (17), and then check whether, for each v ∈ Bκ ,


?
sv = t′ σ {v} (rκ ), . . . , σ {v} (rℓ−1 )

(19)

holds.
We’ve made progress, since each right-hand side of (19) (for v ∈ Bκ ) is an evaluation of t′ (X0 , . . . , Xℓ′ −1 ).
On the other hand, we’d prefer to evaluate t′ (X0 , . . . , Xℓ′ −1 ) at just one place, as opposed to over the entire
orbit σ {v} (rκ ), . . . , σ {v} (rℓ−1 ) v∈Bκ . To this end, we again apply a sumcheck-based batching technique, due
this time to Ron-Zewi and Rothblum [RR24, Fig. 3]. Indeed, we note that, for each v ∈ Bκ , (19) is equivalent
to the sum claim:


X
?
sv =
e
fq σ {v} (rκ ), . . . , σ {v} (rℓ−1 ), w0 , . . . , wℓ′ −1 · t′ (w).
(20)
w∈Bℓ′
′′
′′
After sampling batching scalars
 (r0 , . . . , rκ−1 ), the verifier may batch both sides of (20) by the vector
′′
′′
e
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) v∈Bκ , and in this way obtain the batched sum claim:
!


X
X X
?
′′
′′
{v}
{v}
e
fq(v, r ) · sv =
e
fq(v, r ) · e
fq σ (rκ ), . . . , σ (rℓ−1 ), w0 , . . . , wℓ′ −1
· t′ (w).
(21)
v∈Bκ

w∈Bℓ′

v∈Bκ

This is finally something we can run the sumcheck on. Combining these observations, we obtain the protocol
sketched in Figure 4.
P(r, s; t)

V(r, s)

for each v ∈ Bκ , set ŝv := t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ).

(ŝv )v∈Bκ

?

check s =

X

eq(v
e 0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · ŝv .

v∈Bκ

for each v ∈ Bκ , decompose ŝv =

X

ŝu,v · βu .

u∈Bκ

for each u ∈ Bκ , combine ŝu :=

X

ŝu,v · βv .

v∈Bκ

set [
′′

sv

] := [

ŝu

′′

(r0 , . . . , rκ−1 )

h
i
{v}
]· σ
(βu ) as in (18)
′′

′′

κ

sample batching scalars (r0 , . . . , rκ−1 ) ← L .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . P and V run the sumcheck (21). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
at the end of the sumcheck, evaluate


X
′′ 
{v}
′
′
′
eq
e v, r
· eq
e σ
(r), r and t (r ),
v∈Bκ
′

′

where r = (r0 , . . . , rℓ′ −1 ) is the sumcheck challenge,


{v}
{v}
{v}
and we set σ
(r) := σ
(rκ ), . . . , σ
(rℓ−1 ) .

Figure 4: A sketch of the Hashcaster [Sou24] protocol.
This is a perfectly correct and sound protocol, which moreover wards off the tensor-algebraic complexities
of ring-switching (cf. Figure 3). But what about its efficiency? We claim that ring-switching is more efficient
than Hashcaster for both the prover and the verifier. We note that the comparison we undertake below
pits ring-switching against our improved variant of Hashcaster (explained above). The “original” version of
Hashcaster is still worse, albeit just by a bit. (We expand this analysis further in Subsection 3.2 below.)
9

We begin with our protocols’ verifiers. The verifier complexities of Figures 2 and 4 turn out to be almost
identical, except for the Hashcaster verifier’s matrix transformation (18). Hashcaster’s verifier must compute
this matrix product; ours has no analogue of this task. This transformation entails a quadratic number of Lmultiplications in the extension degree 2κ . (There might be an “NTT analogue” for this matrix; we haven’t
investigated this thoroughly.) Thus Hashcaster’s verifier’s number of L-multiplications grows quadratically
in the extension degree 2κ ; ours grows only linearly. We do not know how to modify Hashcaster so as to
make its verifier complexity match ours (other than by replacing it entirely with ring-switching).
Our protocols’ provers tell a similar story. To calculate (ŝv )v∈Bκ , both provers must begin by tensor′
expanding (rκ , . . . , rℓ−1 ), which takes 2ℓ L-by-L multiplications, and then performing 2ℓ L-by-K multiplications and just under 2ℓ L-additions. To prepare the sumcheck (12), our prover must further basis-decompose
′
this tensor, and row-combine the resulting 2κ ×2ℓ K-matrix by the L-vector (f
eq(u, r′′ ))u∈Bκ . We thus further
ℓ
ℓ
obtain again 2 L-by-K multiplications and just under 2 L-additions. To prepare the analogous sumcheck
(21), Hashcaster’s prover must do something more complex. It must first obtain
the respective tensor
′
expansions of each of the elements of the Galois orbit σ {v} (rκ ), . . . , σ {v} (rℓ−1 ) v∈Bκ . This task entails 2ℓ
L-by-L multiplications and 2ℓ Frobenius applications. Finally, it must multiply the row-combination vector
′
(f
eq(u, r′′ ))u∈Bκ by the 2κ × 2ℓ L-matrix it obtains in this way, thereby expending 2ℓ L-by-L multiplications.
Hashcaster’s prover, if implemented naively, is thus about 2κ -fold more costly than ours.
On the other hand, Hashcaster develops various concrete optimizations, which serve to make its prover
more efficient. These all have the same flavor, however; to explain it, we need to develop a bit more theory.
For each Galois extension L / K, the L-algebras
Y
L
(22)
L ⊗K L ∼
=
ρ∈Gal(L/K)

are isomorphic. This fact is classical and important in arithmetic geometry (see e.g. Waterhouse [Wat79]).
We record a proof here, valid in case both K and L are finite. The left-hand ring is exactly our “tensor
algebra”. We understand that ring as an L-algebra by letting L act on the right-hand tensor factor; that is,
we define α · (a0 ⊗ a1 ) := (a0 ⊗ (α · a1 )) on simple tensors (we refer to Subsection 2.5 below for more details
on the tensor algebra). We understand the right-hand ring as an L-algebra via the standard componentwise
action. As for the actual isomorphism, we map simple tensors in the following way:


κ
(a0 ⊗ a1 ) 7→ σ 0 (a0 ) · a1 , . . . , σ 2 −1 (a0 ) · a1 .
(23)
To describe this isomorphism in coordinates, we must pick bases for both algebras in (22). In the left-hand
ring, we pick the L-basis (βu ⊗ 1)u∈Bκ ; as for the right, we use the standard basis.
It’s not hard to show that the matrix of the map (23)—once expressed in coordinates with respect to
these bases (and under the multiply-on-the-right convention)—is nothing other than the matrix







σ {v} (βu )






(24)

of (17); that is, the isomorphism (22) and the transformation (17) are one and the same map. Of course, since
we already know that (24) is nonsingular (see again [LN96, Lem. 3.51]), (22) is sure enough an isomorphism.
Hashcaster turns out to be a parallel instantiation of ring-switching that takes place on the right-hand
ring of (22), as opposed to on the left. (Hashcaster’s various “optimizations” serve to move certain among
its steps to the left.) In this dictionary,
(11) and (20) correspond, (12) and (21) correspond,

 and finally the
final evaluations Au (r0′ , . . . , rℓ′ ′ −1 ) u∈B and e
fq σ {v} (rκ ), . . . , σ {v} (rℓ−1 ), r0′ , . . . , rℓ′ ′ −1 v∈B correspond.
κ
κ
These “correspondences” are not merely suggestive, but are rather entirely rigorous! In each, the relevant
quantities differ exactly by the isomorphism (22). On the other hand, the check (15) (see also (3)) can only
be performed in the left-hand side of (22). That isomorphism’s left-hand side is thus the most natural side
within which to remain throughout. By failing to stay there, Hashcaster imposes upon its prover and verifier
the costs—which are ultimately artificial—of traversing the isomorphism (22). The hard part, of course, is
to find out an analogue on the left-hand side of (22) of the verifier’s final check, and to remain on the left for
good. This is what we’ve done by surfacing the tensor algebra, and by developing the theory of this work.
10

1.4

Binary BaseFold

In order to apply ring-switching, we need a large-field scheme to invoke it on. To this end, we adapt BaseFold
PCS [ZCF24, § 5] to the characteristic 2 setting. To achieve this, we must re-examine the classic additive
NTT of Lin, Chung and Han [LCH14]. We show that that NTT fits into a framework articulated by Haböck,
Levit and Papini [HLP24], though it predates that latter work.
It is essentially folklore—see e.g. Haböck, Levit and Papini [HLP24, § 4.1], Li and Xing [LX24], and
Haböck [Hab24]—that from any sequence of two-to-one collapsing maps, “an” FFT arises. The resulting
FFTs are ad-hoc, “exotic” FFTs; a new one arises for each system of collapsing maps. (See Haböck, Levit
and Papini [HLP24, § 1], who write “this polynomial basis is not the standard monomial one, and there is
no known efficient conversion between the two.”)
FRI’s original, binary rendition [BBHR18a, § 3.2] consumes—as a global parameter—just such a decreasing chain of F2 -linear subspaces. It stands to reason that, by using some arbitrary sequence of collapsing
maps together with its associated exotic FFT, one might make something like binary BaseFold work.
It is much more interesting that one need not turn to nonstandard FFTs in order to make binary BaseFold
work. In fact, a perfectly suitable one is sitting under our proverbial noses: Lin, Chung and Han’s additive
NTT [LCH14]. (Though it’s not essential for us, their NTT’s basis-conversion problem is very well-studied;
Lin, Al-Naffouri and Han [LAH16] report an O(n · log n · log log n)-time algorithm for the task, reproduced
also in Li et al. [Li+18, § 2.5].) Importantly, Lin, Chung and Han’s NTT [LCH14, § III.] is not expressed—
as written—as that which results from an explicit system of collapsing maps. We show below that it can
be made to arise in that way. The collapsing maps we devise, then, are those that we use in our binary
FRI specialization, and also in our binary adaptation of BaseFold PCS. Our choice lets us use Lin, Chung
and Han’s additive NTT—by far the most standard option—in our BaseFold adaptation, as opposed to an
“exotic” FFT.
The problem. BaseFold PCS identifies a new collection between FRI and multilinear evaluation. To
explain this connection, we go “back to the basics”. We explain, by example, the phenomenon that BaseFold
PCS identifies. We also explain what “goes wrong” if one tries to make things work in characteristic 2 without
setting things up right.
P2ℓ −1
Each honest FRI prover begins with the evaluation of some polynomial P (X) := j=0 aj · X j over its
initial domain S (0) . During the course of the protocol, the prover repeatedly “folds” its initial word. In
FRI’s simplest configurations, the prover’s last oracle will be identically constant. In fact, the prover will
send the verifier this latter constant in the clear, at the very end of its “commit phase”. What will the value
of this constant be, as a function of P (X) and of the verifier’s folding challenges?
The answer to this question depends on which “collapsing maps” we choose to use in FRI. In the smooth,
prime-field case, there is a canonical choice: we let S (0) be a large, power-of-2-order multiplicative subgroup
of our field. For our collapsing maps, we repeatedly apply the squaring map X 7→ X 2 .
In the characteristic 2 setting, this choice isn’t available. (The squaring map is actually an automorphism,
the Frobenius.) We must instead let our domains S (i) ⊂ L, for i ∈ {0, . . . , ℓ}, be F2 -linear subspaces of our
large binary field L. Our two-to-one collapsing maps q (i) : S (i) → S (i+1) must be F2 -linear maps.
As we show now, if we choose these maps abitrarily, then BaseFold’s key observation doesn’t go through.
The point is that the initial Reed–Solomon encoding and the subsequent collapsing maps must “correspond”.
For the sake of our illustration, we work in a toy-sized, 8-bit field: the AES field. That is, we work in
the field

F2 [X] / x8 + x4 + x3 + x + 1 ∼
= F28 .
This field’s elements correspond in a one-to-one way with bytes. We set ℓ = 2 and R = 1. We fix the
2-variate input multilinear
t(X0 , X1 ) := 0xde · 1 + 0xad · X0 + 0xbe · X1 + 0xef · X0 · X1 .
We finally fix the evaluation point
(r0 , r1 ) := (0xab, 0xcd).
We note that
t(r0 , r1 ) = 0x89.
11

(25)

In order to set up FRI, we need a Reed–Solomon domain S (0) ⊂ L of dimension ℓ + R = 3, together with
a system of collapsing maps.

S (0)

0x00

0x01

0x02

0x03

0x04

0x05

0x06

0x07

0x00

0x06

0x14

0x12

0x00

0x73

q (0) : X 7→ X · (X + 0x01)

S (1)
q (1) : X 7→ X · (X + 0x06)

S (2)

Figure 5: A possible system of domains and collapsing maps in the AES field.
In Figure 5, we sketch a plausible choice for these maps in the 8-bit AES field. We initialize S (0) :=
⟨0x01, 0x02, 0x04⟩. For our first collapsing map q (0) : S (0) → S (1) , we use X 7→ X 2 + X; this map annihilates
the one-dimensional subspace of S (0) generated by 0x01. For our next map, we annihilate the image in S (1)
(namely 0x06) of the next basis vector of S (0) (namely 0x02).
Figure 5’s parameterization, while coherent—and perfectly suitable for FRI—fails to work for binary
BaseFold. To show why, we give the thing a try. As prescribed by BaseFold PCS, we begin by “flattening”
the input multilinear t(X0 , X1 ) into a univariate polynomial of degree less than 2ℓ . In this way, we obtain
P (X) = 0xde · 1 + 0xad · X + 0xbe · X 2 + 0xef · X 3 .
We next Reed–Solomon-encode—that is, we evaluate—P (X) on the domain S (0) ; finally, we FRI-fold the
resulting codeword using the challenges r0 = 0xab and r1 = 0xcd. This process appears in Figure 6 below.
0xde · 1 + 0xad · X + 0xbe · X 2 + 0xef · X 3
simple evaluation

f (0)

0xde

0x22

0x68

0xc0

0x9a

0x85

0xba

0xf1

0x2c

0x76

0xeb

0xb1

0x36

0x36

fold using Fig. 5 and r0 = 0xab

f (1)
fold using Fig. 5 and r1 = 0xcd

f (2)

✗
Figure 6: If we don’t choose our domains carefully, then FRI-folding fails to capture multilinear evaluation.
The final FRI oracle in Figure 6 is constant, as is liable to hold generically in FRI. On the other hand,
its value is wrong. We already agreed in (25) that t(r0 , r1 ) = 0x89; on the other hand, we obtained 0x36
above. BaseFold depends crucially on these values’ being equal.
12

Our solution.

In Figure 7, we reveal our domains S (0) , S (1) and S (2) and folding maps q (0) and q (1) .

S (0)

0x00

q (0) : 0x7b · X 2 + X

0x01

0x02

0x03

0x05

0x06

0x07

0x00

0x01

0x06

0x07

0x00

0x01



S (1)
q (1) : 0xaa · X 2 + X

0x04



S (2)

Figure 7: A further binary FRI configuration, this time BaseFold-compatible.
The choice procedure underlying Figure 7 is given rigorously in Subsection 4.1 below (see Definition 4.1).
Actually, the method is not hard to describe. At each stage i, we begin with the simple map q (i) : X 7→ X 2 +X,
which annihilates the subspace of S (i) generated by 1. Then, however, we “twist” the map q (i) , so as to

make the first element of its image 1. This choice guarantees to boot that 1 will be in S (i+1) = q (i) S (i) ,
so that the initial choice q (i+1) : X 7→ X 2 + X makes sense (that map too, though, will need to be twisted).
This process can be seen in Figure 7 above. Under the map X 7→ X 2 + X, 0x02 maps to 0x06, whose
inverse is the scaling factor 0x7b. Similarly, under X 7→ X 2 + X, 0x06 maps to 0x12, whose inverse is 0xaa.
The point of our theory is that, if we choose our collapsing maps in the right way—that is, as Figure
7 does—then we recover multilinear evaluation after all. Crucially, we must also replace our initial Reed–
Solomon encoding with Lin–Chung–Han’s variant. We depict this “happy path” in Figure 8.
0xde · 1 + 0xad · X1 (X) + 0xbe · X2 (X) + 0xef · X3 (X)
Lin–Chung–Han [LCH14] additive NTT

f (0)

0xde

0x73

0xe4

0xa6

0xbe

0x47

0xbd

0xab

0xba

0x4e

0xb4

0x40

0x89

0x89

fold using Fig. 7 and r0 = 0xab

f (1)
fold using Fig. 7 and r1 = 0xcd

f (2)

✓
Figure 8: Upon parameterizing FRI carefully, we recover that protocol’s built-in multilinear evaluator.
In Section 4 below, we prove that our collapsing maps work out this way, in general. To carry out that
proof, we must study Lin, Chung and Han’s additive NTT in some detail (the key result is Theorem 4.13).
In fact, we further enrich our binary BaseFold variant in various interesting ways. For example, using an
“oracle-skipping” optimization—which itself exploits a recent tensor-style proximity gap, due to Diamond
and Gruen [DG25]—we shrink that scheme’s proofs by over half. We explain these ideas in full in Section 4.
13

1.5

Concurrent and Subsequent Works

In this subsection, we discuss Brehm et al.’s Blaze [Bre+25].
Blaze. Blaze [Bre+25] is a polynomial commitment scheme for multilinears large large binary fields.
We fix an ℓ-variate multilinear t(X0 , . . . , Xℓ−1 ) over a large binary field L. Using a technique grounded
in code-switching [RR24], Blaze obtains a strictly linear-time commitment procedure, a linear-time prover,
and a polylogarithmic verifier; we sketch its approach. Blaze begins as Brakedown does, except with a wide
matrix—shaped something like 2a × 2ℓ−a , where the matrix height 2a is just polynomial in ℓ. That is, Blaze
inscribes t(X0 , . . . , Xℓ−1 )’s Lagrange coefficients, in row-major order, into that wide matrix. Its prover
encodes that matrix row-wise under a RAA (repeat, accumulate accumulate) code—or under a “packed”
variant of that code—and commits to the resulting matrix, which we presently call M .
Beginning as Brakedown [Gol+23] does, Blaze reduces the problem of evaluating t(X0 , . . . , Xℓ−1 ) at some
point (z0 , . . . , zℓ−1 ) to that of evaluating the message underneath r · M —whatever it may be—at the suffix
(za , . . . , zℓ−1 ); here, r is a length-2a random vector sampled by the verifier.
As of this point, Blaze has shrunk its problem size by a polylogarithmic factor, and so can freely begin
using “heavier”—i.e., quasilinear-time—techniques. The overhead to the verifier of this reduction is proportional to 2a , which is just polylogarithmic in 2ℓ . (This is code-switching in action.) Blaze, indeed, must
now securely evaluate a multilinear whose coefficients are themselves encoded underneath the RAA code.
To this end, it introduces a further protocol, which is based on BaseFold (and in fact on this work’s binary
variant). That is, it commits using binary BaseFold to the claimed RAA codeword rT · M , to the message
supposedly underneath that codeword, and finally to all of the intermediate RAA encoding steps which intervene between those two quantities. It then uses sumcheck-based techniques, as well as the native evaluation
procedure of BaseFold PCS, to check the validity of the RAA encoding and evaluate the committed message.
The Blaze PCS is functionally an alternative to the large-field, binary BaseFold PCS construction we
present in Section 4. Operating over F2128 throughout, Blaze [Bre+25, § 8] reports commitment and proving
times that improve upon binary BaseFold’s by roughly threefold at the ℓ = 28 problem size, though its proofs
are larger. The key point is that though Blaze’s RAA code is very fast to encode, its relative distance is
middling (e.g., just 0.19 at the rate ρ = 14 ). Blaze reports a proof of 2.5 MiB in the ℓ = 28 case, compared
with 1.4 MiB for their binary BaseFold benchmark. In this work, we develop a binary BaseFold variant that
further reduces proof sizes, using the nontrivial enhancement whereby we skip FRI round oracles (see Section
4). Incorporating our oracle-skipping technique, as well as various further concrete proof size optimizations
(described in Subsection 5.2), we obtain a proof size of 0.436 MiB in the ℓ = 28 case. Our enhancements
could be used to improve Blaze’s proof sizes too.
Blaze’s RAA code depends on a randomized, transparent setup, which involves an expensive verification
procedure, itself necessary to bound that setup’s probability of failure. That test must be independently
rerun by each of the protocol’s users; it requires more than a day of computation on a laptop [Bre+25,
§ 1.1]. The outcome of that test could, in theory, be attested to by a further SNARK, or else checked more
quickly with the aid of a GPU-accelerated implementation. Blaze’s RAA setup procedure must be carried
out independently for each instance size and each code rate.
Blaze’s failure analysis assumes that its sampler’s coins are uniform. In order to make that analysis
applicable in practice, Blaze’s sampler must use public, nothing-up-my-sleeve randomness. The security
guarantees of Blaze—i.e., of each of that protocol’s deployments—demand that its sampler do exactly this,
and cease to hold otherwise.
While Blaze supports only cryptographically large fields, its authors note that Blaze PCS is compatible
with this work’s ring-switching compiler. Upon instantiating our ring-switching reduction on Blaze’s largefield PCS—i.e., as opposed to on our binary BaseFold variant—one would obtain an interesting small-field
scheme, an alternative to that which we present in Section 5.
Acknowledgements. We would like to acknowledge our colleagues at Irreducible for their insights and
contributions to the Binius implementation of these techniques. We would like to gratefully thank Benedikt
Bünz, Giacomo Fenzi, Angus Gruen, Ulrich Haböck, Joseph Johnston, Raju Krishnamoorthy, Eugene Rabinovich, Justin Thaler and Benjamin Wilson, whose collective comments and suggestions contributed significantly to this work. We thank Ron Rothblum for patiently explaining code-switching to us.

14

2

Background and Notation

We write N for the nonnegative integers. All fields in this work are finite. We fix a binary field L. For
each ℓ ∈ N, we write Bℓ for the ℓ-dimensional boolean hypercube {0, 1}ℓ ⊂ Lℓ . We occasionally identify Bℓ
Pℓ−1
with the integer range {0, . . . 2ℓ − 1} by mapping v 7→ {v} := i=0 2i · vi . The rings we treat are nonzero
and commutative with unit. For us, an algebra A over a field L is a commutative ring A together with an
|R|
embedding of rings L ,→ A. For L a field and R ⊂ Lϑ a subset, we write µ(R) := |L|
ϑ.

2.1

Multilinear Polynomials

We review various normal forms for multilinear polynomials, following [DP25, § 2.1]. An ℓ-variate polynomial
in L[X0 , . . . , Xℓ−1 ] is multilinear if each of its indeterminates appears with individual degree at most 1; we
write L[X0 , . . . , Xℓ−1 ]⪯1 for the set of multilinear polynomials over L in ℓ indeterminates. Clearly, the set of
monomials (1, X0 , X1 , X0 · X1 , . . . , X0 · · · · · Xℓ−1 ) yields a L-basis for L[X0 , . . . , Xℓ−1 ]⪯1 ; we call this basis
the multilinear monomial basis in ℓ variables.
We introduce the 2 · ℓ-variate polynomial
e
fq(X0 , . . . , Xℓ−1 , Y0 , . . . , Yℓ−1 ) :=

ℓ−1
Y

(1 − Xi ) · (1 − Yi ) + Xi · Yi .

i=0

It is essentially the content of Thaler [Tha22, Fact. 3.5]) that the set (f
eq(X0 , . . . , Xℓ−1 , w0 , . . . , wℓ−1 ))w∈Bℓ
yields a further L-basis of the space L[X0 , . . . , Xℓ−1 ]⪯1 .
For each fixed (r0 , . . . , rℓ−1 ) ∈ Lℓ , the vector (f
eq(r0 , . . . , rℓ−1 , w0 , . . . , wℓ−1 ))w∈Bℓ takes the form
!
ℓ−1
Y
ri · wi + (1 − ri ) · (1 − wi )
= ((1 − r0 ) · · · · · (1 − rℓ−1 ), . . . , r0 · · · · · rℓ−1 ).
i=0

w∈Bℓ

Nℓ−1
We call this vector the tensor product expansion of (r0 , . . . , rℓ−1 ) ∈ Lℓ , and denote it by i=0 (1 − ri , ri ).
We note that it can be computed in 2ℓ L-additions and 2ℓ L-multiplications (see e.g. [Tha22, Lem. 3.8]).
As a notational device, we introduce the further 2 · ℓ-variate polynomial:
mg
on(X0 , . . . , Xℓ−1 , Y0 , . . . , Yℓ−1 ) :=

ℓ−1
Y

1 + (Xi − 1) · Yi ;

i=0

we note that (g
mon(X0 , . . . , Xℓ−1 , w0 , . . . , wℓ−1 ))w∈Bℓ is the multilinear monomial basis in ℓ indeterminates.

2.2

Error-Correcting Codes

We recall details on codes, referring throughout to Guruswami [Gur06]. A code of block length n over the
alphabet Σ is a subset of Σn . In Σn , we write d for the Hamming distance between two vectors (i.e., the
number of components at which they differ). We fix a field L. A linear [n, k, d]-code over L is a k-dimensional
linear subspace C ⊂ Ln for which d(v0 , v1 ) ≥ d holds for
 unequal pair of elements v0 and v1 of C. nThe
 each
; indeed, we note that, for each word u ∈ L , at
unique decoding radius of the [n, k, d]-code C ⊂ Ln is d−1
2
most one codeword v ∈ C satisfies d(u, v) < d2 (this fact is a direct consequence of the triangle inequality).
For u ∈ Ln arbitrary, we write d(u, C) := minv∈C d(u, v) for the distance between u and the code C.
For each linear code C ⊂ Ln and each integer m ≥ 1, we define C’s m-fold interleaved code as the
m
n
subset C m ⊂ (Ln ) ∼
= (Lm ) . We understand this latter set as a length-n block code over the alphabet
m
L . In particular, its elements are essentially matrices in Lm×n each of whose rows is a C-element. We
m−1
write matrices (ui )i=0 ∈ Lm×n row-wise. By definition of C m , two matrices in Lm×n differ at a column if
m−1
m×n
they differ at any of that column’s components.
That
is within distance e to the

 a matrix (ui )i=0 ∈ L
m−1

code C m —in which event we write dm (ui )i=0 , C m ≤ e—thus entails precisely that there exists a subset


m−1
D := ∆m (ui )i=0 , C m , say, of {0, . . . , n − 1}, of size at most e, for which, for each i ∈ {0, . . . , m − 1}, the
row ui admits a codeword vi ∈ C for which ui |{0,...,n−1}\D = vi |{0,...,n−1}\D .
15

We recall Reed–Solomon codes (see [Gur06, Def. 2.3]). For notational convenience, we consider only Reed–
Solomon codes whose message and block lengths are powers of two. We fix nonnegative message length and
ℓ+R
rate parameters ℓ and R, as well as a subsetnS ⊂ L of size 2ℓ+R . We writeoC ⊂ L2
for the Reed–Solomon
ℓ
≺2
ℓ+R ℓ
. That is, RSL,S [2ℓ+R , 2ℓ ] is the
code RSL,S [2
, 2 ], defined to be the set (P (x))x∈S P (X) ∈ L[X]
set of 2ℓ+R -vectors that arise as the vector of respective values of a polynomial of degree less than 2ℓ over S.
ℓ
The distance of RSL,S [2ℓ+R , 2ℓ ] is d = 2ℓ+R − 2ℓ + 1. We write Enc : L[X]≺2 → LS for the code’s encoding
function; it maps each polynomial P (X) to its tuple of evaluations over S.
We recall the Berlekamp–Welch algorithm for Reed–Solomon decoding within the unique decoding radius
(see [Gur06, Rem. 4]).
Algorithm 1 (Berlekamp–Welch [Gur06, Rem. 4].)

1: procedure DecodeReedSolomon (f (x))x∈S


 d−1 
− 1; write Q(X, Y ) := A(X) · Y + B(X).
2:
allocate A(X) and B(X) of degrees 2 and 2ℓ+R − d−1
2
3:
interpret the equalities Q(x, f (x)) = 0, for x ∈ S, as a system of 2ℓ+R equations in 2ℓ+R +1 unknowns.
4:
by finding a nonzero solution of this linear system, obtain values for the polynomials A(X) and B(X).
5:
if A(X) ∤ B(X) then return ⊥.
6:
write P (X) := −B(X)/A(X).
7:
if deg(P (X)) ≥ 2ℓ then return ⊥.
8:
return P (X).




We note that the unknown polynomial Q(X, Y ) above indeed has d−1
+ 1 + 2ℓ+R − d−1
= 2ℓ+R + 1
2
2
coefficients, as required.
On input a word f : S → L for which d(f, C) < d2 , Algorithm 1 necessarily returns the unique polynomial
P (X) of degree less than 2ℓ for which d(f, Enc(P (X))) < d2 holds. Indeed, this is just the correctness of
Berlekamp–Welch algorithm on input assumed to reside within the unique decoding radius (we refer to
[Gur06, Rem. 4] for a thorough treatment of this result).
Interestingly, our above variant of this classical algorithm—the supplementary degree check 7 is atypical—
serves moreover to detect whether its input is in the unique decoding radius. We prove this fact below.
Lemma 2.1. If d(f, C) ≥ d2 , then Algorithm 1 outputs ⊥.
Proof. We fix a map f : S → L for which d(f, C) ≥ d2 ; we suppose for contradiction that Algorithm 1, on the
input f , nonetheless successfully outputs a polynomial P (X) (necessarily of degree less than 2ℓ ). We first note
that the relation P (X) = −B(X)/A(X) implies the factorization Q(X, Y ) = A(X)·(Y − P (X)). Separately,
since deg(P (X)) < 2ℓ , Enc(P (X)) is a codeword; our hypothesis on f thus implies that d(f, Enc(P (X))) ≥ d2 .

 d
On the other hand, by its degree, A(X) can have at most d−1
< 2 roots. We conclude that there necessarily
2
exists some element x∗ ∈ S for which P (x∗ ) ̸= f (x∗ ) and A(x∗ ) ̸= 0 simultaneously hold. Finally, by its
construction, Q(x, f (x)) = 0 necessarily holds for each x ∈ S. Putting these facts together, we see that
0 = Q(x∗ , f (x∗ )) = A(x∗ ) · (f (x∗ ) − P (x∗ )) ̸= 0, a contradiction.

2.3

The Novel Polynomial Basis

We recall in detail the novel polynomial basis of Lin, Chung and Han [LCH14, § II. C.]. We fix again a
binary field L, of degree r, say, over F2 . For our purposes, a subspace polynomial over L is a polynomial
W (X) ∈ L[X] which splits completely over L, and whose roots, each of multiplicity 1, form an F2 -linear
subspace of L. For a detailed treatment of subspace polynomials, we refer to Lidl and Niederreiter [LN96,
Ch. 3. § 4.]. For each subspace polynomial W (X) ∈ L[X], the evaluation map W : L → L is F2 -linear.
ℓ
For each fixed ℓ ∈ {0, . . . , r − 1}, the set L[X]≺2 of polynomials of degree less than 2ℓ is a 2ℓ -dimensional
ℓ
ℓ
vector space over L. Of course, the set (1, X, X 2 , . . . , X 2 −1 ) yields a natural L-basis of L[X]≺2 . Lin,
ℓ
Chung and Han define a further L-basis of L[X]≺2 —called the novel polynomial basis—in the following
way. We fix once and for all an F2 -basis (β0 , . . . , βr−1 ) of L (which we view as an r-dimensional vector
space over its subfield F2 ). For each i ∈ {0, . . . , ℓ}, we write Ui := ⟨β0 , . . . , βi−1 ⟩ for the F2 -linear span of
16

Q
the prefix (β0 , . . . , βi−1 ), and define the subspace vanishing polynomial Wi (X) := u∈Ui (X − u), as well
ci (X) := Wi (X) (we note that βi ̸∈ Ui , so that Wi (βi ) ̸= 0). In words, for each
as its normalized variant W
Wi (βi )
ci (X) moreover satisfies W
ci (X)(βi ) = 1. Finally, for
i ∈ {0, . . . , ℓ}, Wi (X) vanishes precisely on Ui ⊂ L; W
Pℓ−1
ℓ
each j ∈ {0, . . . , 2 − 1}, we write (j0 , . . . , jℓ−1 ) for the bits of j—so that j = k=0 2k · jk holds—and set
Qℓ−1 c
ji
ℓ
Xj (X) := i=0 W
i (X) . We note that, for each j ∈ {0, . . . , 2 − 1}, Xj (X) is of degree j. We conclude that
2ℓ −1
the change-of-basis matrix from (1, X, . . . , X
) to (X0 (X), X1 (X), . . . , X2ℓ −1 (X)) is triangular (with an
ℓ
everywhere-nonzero diagonal), so that this latter list indeed yields a L-basis of L[X]≺2 .
We now fix moreover a rate parameter R ∈ {1, . . . , r − ℓ} and a union S ⊂ L of 2R distinct cosets of Uℓ =
⟨β0 , . . . , βℓ−1 ⟩. For example, we may take as S ⊂ L any affine translate of the ℓ + R-dimensional subspace
⟨β0 , . . . , βℓ+R−1 ⟩. For each S ⊂ L of this form, Lin, Chung and Han [LCH14, § III. B.]’s O(2ℓ+R · ℓ)-time
P2ℓ −1
algorithm serves to compute, on input the polynomial P (X) := j=0 aj · Xj (X) (expressed in coordinates
with respect to the novel polynomial basis), its encoding (P (x))x∈S . In fact, that algorithm takes exactly
2ℓ+R · ℓ L-additions and 2ℓ+R−1 · ℓ L-multiplications [LCH14, § III. D.].
In Remark 4.15 below, we suggest a new interpretation of Lin, Chung and Han’s algorithm [LCH14, § III.]
based on the techniques of this paper. For now, for self-containedness, we record here the key algorithm in
full, in our notation. We note that Algorithm 2’s equivalence with [LCH14, § III.] is not obvious; we explain
the correctness of our description in Remark 4.15 below. In what follows, we fix as above the degree and
P2ℓ −1
rate parameters ℓ and R. We finally fix a polynomial P (X) = j=0 aj · Xj (X); we write b : Bℓ+R → L for
2ℓ −1

(aj )j=0 ’s 2R -fold tiling; in other words, for each v ∈ Bℓ+R , we set b(v0 , . . . , vℓ+R−1 ) := a{(v0 ,...,vℓ−1 )} .
Algorithm 2 (Lin–Chung–Han [LCH14, § III.].)


1: procedure AdditiveNTT (b(v))v∈B
ℓ+R
2:
3:
4:
5:
6:

for i ∈ {ℓ − 1, . . . , 0} (i.e., in downward order) do
for (u, v) ∈ Bℓ+R−i−1 × Bi do
Pℓ+R−i−2
ci (βi+1+k ).
define the twiddle factor t := k=0
uk · W
overwrite first b(u ∥ 0 ∥ v) += t · b(u ∥ 1 ∥ v) and then b(u ∥ 1 ∥ v) += b(u ∥ 0 ∥ v).
return (b(v))v∈Bℓ+R .

We note that the twiddle factor t above depends only on u, and not on v, and can be reused accordingly.
Finally, in the final return statement above, we implicitly identify Bℓ+R ∼
= S using the standard basis
β0 , . . . , βℓ+R−1 of the latter space (see also Subsection 4.1 below).

2.4

FRI

We recall Ben-Sasson, Bentov, Horesh and Riabzev’s [BBHR18a] Fast Reed–Solomon Interactive Oracle Proof
of Proximity (FRI). For L a binary field, and size and rate parameters ℓ and R fixed, FRI yields an IOP of
proximity for the Reed–Solomon code RSL,S [2ℓ+R , 2ℓ ]; here, we require that S ⊂ L be an affine, F2 -linear
subspace (of dimension ℓ + R, of course). That is, FRI yields an IOP for the claim whereby some oracle
ℓ
[f ]—i.e., representing a function f : S → L—is close to a codeword (P (x))x∈S (here, P (X) ∈ L[X]≺2
represents a polynomial of degree less than 2ℓ ). FRI’s verifier complexity is polylogarithmic in 2ℓ . We
abbreviate ρ := 2−R , so that RSL,S [2ℓ+R , 2ℓ ] is of rate ρ.
Internally, FRI makes use of a folding constant η—which we fix to be 1—as well as a fixed, global sequence
of subspaces and maps of the form:
q (0)

q (1)

q (2)

q (ℓ−1)

S = S (0) −−→ S (1) −−→ S (2) −−→ · · · −−−−→ S (ℓ) .

(26)

For each i ∈ {0, . . . , ℓ − 1}, q (i) is a subspace polynomial of degree 2η = 2, whose kernel, which is 1dimensional, is contained in S (i) . By linear-algebraic considerations, S (i+1) ’s F2 -dimension is 1 less than
S (i) ’s is; inductively, we see that each S (i) is of dimension ℓ + R − i. In particular, S (ℓ) is of dimension R.

17

2.5

Tensor Products of Fields

We record algebraic preliminaries, referring throughout to Lang [Lan02, Ch. XVI]. We fix a field extension
L / K. We define the tensor product A := L ⊗K L of L with itself over K as in [Lan02, Ch. XVI § 6]. Here, we
view L as a K-algebra; the resulting object A := L ⊗K L is likewise a K-algebra. We would like to sincerely
thank Benjamin Wilson for first suggesting to us this tensor-theoretic perspective on the tower algebra of
[DP25, § 3.4].
We recall from [Lan02, Ch. XVI, § 1] the natural K-bilinear mapping φ : L × L → L ⊗K L which sends
φ : (α0 , α1 ) 7→ α0 ⊗ α1 . We write φ0 and φ1 for φ’s restrictions to the subsets L × {1} and {1} × L of L × L,
and moreover identify these latter subsets with L. That is, we write φ0 : α 7→ α ⊗ 1 and φ1 : α 7→ 1 ⊗ α, both
understood as maps L → A. We claim that these maps are injective (i.e., that they’re not identically zero).
We follow Lang [Lan02, Ch. XVI, § 2, Prop. 2.3]. The mapping f : L × L → L sending f : (α0 , α1 ) 7→ α0 · α1
is K-bilinear; by the universal property of the tensor product, f induces a K-linear map h : L ⊗K L → L,
for which, for each α ∈ L, h(α ⊗ 1) = f (α, 1) = α · 1 = α holds; we see that α ⊗ 1 = 0 if and only if α = 0.
We assume once and for all that deg(L / K) is a power of 2, say 2κ . We fix a K-basis (βv )v∈Bκ of L. We
moreover impose the simplifying assumption whereby β(0,...,0) = 1. By [Lan02, Ch. XVI, § 2, Cor. 2.4], the
set (βu ⊗ βv )(u,v)∈Bκ ×Bκ yields a K-basis of A. We thus see that each A-element is, concretely, a 2κ × 2κ
arrayPof K-elements. For each a ∈ A given, there is a unique 2κ -tuple of L-elements (av )v∈Bκ for which
a = v∈Bκ av ⊗ βv holds. (Indeed, this is just [Lan02,
P Ch. XVI, § 2, Prop. 2.3].) Similarly, there is a
unique 2κ -tuple of L-elements (au )u∈Bκ for which a = u∈Bκ βu ⊗ au holds. We call the tuples (av )v∈Bκ and
(au )u∈Bκ a’s column and row representations, respectively. We depict the tensor algebra in Figure 9 below.
2κ
⟨1 ⊗ 1⟩

2κ




















⟨1 ⊗ 1⟩

⟨1 ⊗ β2κ −1 ⟩

K

φ1 (L)

K

φ0 (L)














 ⟨β2κ −1 ⊗ 1⟩




K

Figure 9: A schematic representation of our tensor algebra.
The maps φ0 and φ1 respectively embed L into A’s left-hand column and top row. That is, the image
of φ0 : L ,→ A is the set of K-arrays which are 0 except in their respective left-most columns; the image of
φ1 : L ,→ A is the set of K-arrays which are 0 outside of their top rows. We finally characterize concretely
the products φ0 (α) · a P
and φ1 (α) · a, for elements α ∈ L and
P a ∈ A arbitrary. It is a straightforward to
show that φ0 (α) · a = v∈Bκ (α · av ) ⊗ βv and φ1 (α) · a = u∈Bκ βu ⊗ (α · au ) both hold; here, we again
write (av )v∈Bκ and (au )u∈Bκ for a’s column and row representations. That is, φ0 (α) · a differs from a by
column-wise multiplication by α; φ1 (α) · a differs from a by row-wise multiplication by α. In short, φ0
operates on columns; φ1 operates on rows.
We now record a simple polynomial-packing
operation, which is implicit in [DP25, § 3.4]. The basisP
κ
combination procedure (αv )v∈Bκ 7→ v∈Bκ αv ·βv induces a K-isomorphism K 2 → L. By applying this map
in chunks, we get a procedure that associates to each ℓ-variate K-multilinear an ℓ − κ-variate L-multilinear.
Definition 2.2. For each extension L / K, with K-basis (βv )v∈Bκ say, and each multilinear t(X0 , . . . , Xℓ−1 )
over K, we write ℓ′ := ℓ − κ, and define the packed polynomial
X
t′ (X0 , . . . , Xℓ′ −1 ) :=
t(v0 , . . . , vκ−1 , X0 , . . . , Xℓ′ −1 ) · βv .
v∈Bκ

18

The effect of Definition 2.2, in the Lagrange basis, is to replace each 2κ -element chunk of t(X0 , . . . , Xℓ−1 )’s
Lagrange coefficient vector with a single L-element, by basis-combining that chunk.
We emphasize that Definition 2.2’s packing procedure is reversible (see also [DP25, Thm. 3.9]); that is,
t′ (X0 , . . . , Xℓ′ −1 ) can be “unpacked”. We note that Definition 2.2 is essentially the same as [DP25, § 4.3].
We finally write φ1 (t′ )(X0 , . . . , Xℓ′ −1 ) ∈ A[X0 , . . . , Xℓ′ −1 ] for the result of embedding t′ (X0 , . . . , Xℓ′ −1 )
componentwise along the inclusion φ1 : L ,→ A.
A conceptual predecessor of our tensor algebra A appears in Diamond and Posen [DP25, § 3.4]. That
work’s “tower algebra” Aι,κ,τ turns out to be isomorphic, as an algebra, to Tτ ⊗Tι Tι+κ ; here, Tτ , Tι , and Tι+κ
are certain binary fields introduced in that work (“tower fields” of characteristic 2). That work’s “constant”
and “synthetic” embeddings correspond to our embeddings φ0 and φ1 , respectively.

2.6

Proximity Gaps

We turn to proximity gaps, following Ben-Sasson, et al., [Ben+23], Diamond and Posen [DP24], and Diamond
and Gruen [DG25]. Throughout this subsection, we again fix a Reed–Solomon code C := RSL,S [2ℓ+R , 2ℓ ];
we moreover write d := 2ℓ+R − 2ℓ + 1 for C’s distance. In the following results, for notational convenience,
we abbreviate n := 2ℓ+R for the Reed–Solomon code C’s block length.
We recall the notion of proximity gaps, both over affine lines [DG25, Def. 1] and over tensor combinations
[DG25, Def. 2]. The following key result entails that Reed–Solomon codes exhibit proximity gaps for affine
within the unique decoding radius.
lines, for each proximity parameter e ∈ 0, . . . , d−1
2



Theorem 2.3 (Ben-Sasson, et al. [Ben+23, Thm. 4.1]). For each proximity parameter e ∈ 0, . . . , d−1
2
ℓ+R
and each pair of words u0 and u1 in L2
, if
Pr [d((1 − r) · u0 + r · u1 , C) ≤ e] >

r∈L

n
,
|L|



1
then d2 (ui )i=0 , C 2 ≤ e.
Our formulation above of [Ben+23, Thm. 4.1] uses a slightly different parameterization; that is, our line
is of the form (1 − r0 ) · u0 + r0 · u1 , while that result considers lines of the form u′0 + r0 · u′1 . The difference
between these conventions is immaterial, up to the reparameterization which sets u′0 := u0 and u′1 := u1 − u0
(this reparameterization moreover doesn’t affect the conclusion).
Diamond and Gruen [DG25, Thm. 2], making use of a result of Angeris, Evans and Roh [AER24] (see
also [DG25, Thm. 3]), show that each code C for which the conclusion of Theorem 2.3 holds also exhibits
tensor-style proximity gaps in the sense of Diamond and Posen [DP24, Thm. 2] (although they sharpen by
a factor of two that result’s false witness probability). Applying their result to Theorem 2.3, those authors
obtain:



, each
Theorem 2.4 (Diamond–Gruen [DG25, Cor. 1]). For each proximity parameter e ∈ 0, . . . , d−1
2
ℓ+R
tensor arity ϑ ≥ 1, and each list of words u0 , . . . , u2ϑ −1 in L2
, if
 



 
 h
 
d
Pr
 
(r0 ,...,rϑ−1 )∈Lϑ  
 

Nϑ−1

i=0 (1 − ri , ri )


i 

·




u0
..
.







 

 

 

, C  ≤ e > ϑ · n ,
 

|L|
 

 


u2ϑ −1
ϑ

then d2

2.7



2ϑ −1

(ui )i=0 , C 2

ϑ



≤ e.

Security Definitions

We record security definitions. We begin by defining various abstract oracles, following [DP25, § 4.1].

19

L
FUNCTIONALITY 2.5 (FVec
—vector oracle).
An arbitrary alphabet L is given.

• Upon receiving (submit, m, f ) from P, where m ∈ N and f ∈ LBm , output (receipt, L, [f ]) to all
parties, where [f ] is some unique handle onto the vector f .
• Upon receiving (query, [f ], v) from V, where v ∈ Bm , send V (result, f (v)).

λ,ℓ
FUNCTIONALITY 2.6 (FPoly
—polynomial oracle).
A security parameter λ ∈ N and a number-of-variables parameter ℓ ∈ N are given. The functionality
constructs and fixes a field L (allowed to depend on λ and ℓ).

• Upon receiving (submit, t) from P, where t(X0 , . . . , Xℓ−1 ) ∈ L[X0 , . . . , Xℓ−1 ]⪯1 , output
(receipt, [t]) to all parties, where [t] is some unique handle onto the polynomial t.
• On input (query, [t], r) from V, where r ∈ Lℓ , send V (result, t(r0 , . . . , rℓ−1 )).

λ,K,ℓ
FUNCTIONALITY 2.7 (FSFPoly
—small-field polynomial oracle).
A security parameter λ ∈ N, a number-of-variables parameter ℓ ∈ N, and a ground field K are given.
The functionality constructs and fixes a field extension L / K (allowed to depend on λ, ℓ and K).

• Upon receiving (submit, t) from P, where t(X0 , . . . , Xℓ−1 ) ∈ K[X0 , . . . , Xℓ−1 ]⪯1 , output
(receipt, [t]) to all parties, where [t] is some unique handle onto the polynomial t.
• On input (query, [t], r) from V, where r ∈ Lℓ , send V (result, t(r0 , . . . , rℓ−1 )).
An IOP, by definition, is a protocol in which P and V may make free use of the abstract Functionality
2.5; in a PIOP, the parties may instead use Functionality 2.6. Interactive oracle polynomial commitment
schemes (IOPCSs) serve to bridge these two models. They’re IOPs; that is, they operate within the abstract
computational model in which Functionality 2.5 is assumed to exist. On the other hand, they “emulate” the
more-powerful Functionality 2.6, in the sense that each given PIOP—by inlining in place of each of its calls
to Functionality 2.6 an execution of the IOPCS—stands to yield an equivalently secure IOP.
Departing from previous treatments, we define polynomial commitment in the IOP model. For our
purposes, a “polynomial commitment scheme” is an IOP (i.e., a protocol in which the vector oracle is
available to both parties) which captures the commitment, and subsequently the evaluation, of a polynomial.
Our approach contrasts with that taken by various previous works (we note e.g. Setty [Set20] and Diamond
and Posen [DP25]). Those works opt to define polynomial commitment schemes in the plain (random oracle)
model, noting that a plain PCS, upon being inlined into a secure PIOP, yields a sound argument. That
approach absorbs the Merklization process both into the PCS and into the composition theorem. Our
approach bypasses this technicality, and separates these concerns. Indeed, given a PIOP, we may first inline
our IOPCS into it; on the resulting IOP, we may finally invoke generically the compiler of Ben-Sasson,
Chiesa and Spooner [BCS16]. This “two-step” compilation process serves to transform any secure PIOP into
a secure argument in the random oracle model.
We also define the security of IOPCSs differently than do [Set20, Def. 2.11] and [DP25, § 3.5]. Our
definition below requires that E extract t(X0 , . . . , Xℓ−1 ) immediately after seeing A’s commitment (that is,
before seeing r, or observing any evaluation proofs on the part of A). This work’s IOPCS constructions indeed
meet this stricter requirement, owing essentially to their use of Reed–Solomon codes, which are efficiently
decodable. (In the setting of general—that is, not-necessarily-decodable—codes, extraction becomes much
more complicated, and requires rewinding.) On the other hand, our strict rendition of the IOPCS notion
makes its key composability property—that is, the fact whereby a secure IOPCS, upon being inlined into a
secure PIOP, yields a secure IOP—easier to prove. (We believe that this composability property should, on
the other hand, nonetheless hold even under various weakenings of Definition 2.9.)
20

Definition 2.8. An interactive oracle polynomial commitment scheme (IOPCS) is a tuple of algorithms
Π = (Setup, Commit, P, V) with the following syntax:
• params ← Π.Setup(1λ , ℓ). On input the security parameter λ ∈ N and a number-of-variables parameter
ℓ ∈ N, outputs params, which includes, among other things, a field L.
• [f ] ← Π.Commit(params, t). On input params and a multilinear polynomial t(X0 , . . . , Xℓ−1 ) ∈
L[X0 , . . . , Xℓ−1 ]⪯1 , outputs a handle [f ] to a vector.
L
• b ← ⟨P([f ], s, r; t), V([f ], s, r)⟩ is an IOP, in which the parties may jointly leverage the machine FVec
.
ℓ
The parties have as common input a vector handle [f ], an evaluation point (r0 , . . . , rℓ−1 ) ∈ L , and
a claimed evaluation s ∈ L. P has as further input a multilinear polynomial t(X0 , . . . , Xℓ−1 ) ∈
L[X0 , . . . , Xℓ−1 ]⪯1 . V outputs a success bit b ∈ {0, 1}.

The IOPCS Π is complete if the obvious correctness property holds. That is, for each multilinear polynomial t(X0 , . . . , Xℓ−1 ) ∈ L[X0 , . . . , Xℓ−1 ]⪯1 and each honestly generated commitment [f ] ←
Π.Commit(params, t), it should hold that, for each r ∈ Lℓ , and for s := t(r0 , . . . , rℓ−1 ), the honest prover
algorithm induces the verifier to accept with probability 1, so that ⟨P([f ], s, r; t), V([f ], s, r)⟩ = 1.
We now define the security of IOPCSs.
Definition 2.9. For each interactive oracle polynomial commitment scheme Π, security parameter λ ∈ N,
number-of-variables parameter ℓ ∈ N, PPT adversary A, and PPT emulator E, we define the following
experiment:
• The experimenter samples params ← Π.Setup(1λ , ℓ), and gives params, including L, to A and E.
• The adversary, after interacting arbitrarily with the vector oracle, outputs a handle [f ] ← A(params).
• On input A’s record of interactions with the oracle, E outputs t(X0 , . . . , Xℓ−1 ) ∈ L[X0 , . . . , Xℓ−1 ]⪯1 .
• The verifier outputs (r0 , . . . , rℓ−1 ) ← V(params, [f ]); A responds with an evaluation claim s ← A(r).
• By running the evaluation IOP with A as V, the experimenter obtains the bit b ← ⟨A(s, r), V([f ], s, r)⟩.
• The experimenter defines two quantities:
– RealΠ,ℓ
A (λ): is defined to be s if b = 1, and ⊥ otherwise.
– IdealΠ,ℓ
E,A (λ): is defined to be t(r0 , . . . , rℓ−1 ) if t(X0 , . . . , Xℓ−1 ) ̸= ⊥ and b = 1, and ⊥ otherwise.
The IOPCS Π is said to be secure if, for each PPT adversary A, hthere exists a PPT emulator
E and a
i
Π,ℓ
negligible function negl such that, for each λ ∈ N and each ℓ ∈ N, Pr RealΠ,ℓ
(λ)
=
̸
Ideal
(λ)
≤
negl(λ).
A
E,A
We finally record a variant of Definition 2.8 in which the parties may fix a small coefficient field K.
Definition 2.10. A small-field interactive oracle polynomial commitment scheme (small-field IOPCS) is a
tuple of algorithms Π = (Setup, Commit, P, V) with the following syntax:
• params ← Π.Setup(1λ , ℓ, K). On input the security parameter λ ∈ N, a number-of-variables parameter
ℓ ∈ N and a field K, outputs params, which includes, among other things, a field extension L / K.
• [f ] ← Π.Commit(params, t). On input params and a multilinear polynomial t(X0 , . . . , Xℓ−1 ) ∈
K[X0 , . . . , Xℓ−1 ]⪯1 , outputs a handle [f ] to a vector.
L
• b ← ⟨P([f ], s, r; t), V([f ], s, r)⟩ is an IOP, in which the parties may jointly leverage the machine FVec
.
ℓ
The parties have as common input a vector handle [f ], an evaluation point (r0 , . . . , rℓ−1 ) ∈ L , and
a claimed evaluation s ∈ L. P has as further input a multilinear polynomial t(X0 , . . . , Xℓ−1 ) ∈
K[X0 , . . . , Xℓ−1 ]⪯1 . V outputs a success bit b ∈ {0, 1}.

We define the security of small-field IOPCSs Π exactly as in Definition 2.9, except that we require that
E output a polynomial t(X0 , . . . , Xℓ−1 ) ∈ K[X0 , . . . , Xℓ−1 ]⪯1 .
21

3

Ring-Switching

In this section, we formally present ring-switching. First, we review in a bit more detail the material of
Subsection 1.3. Our main goal is to reframe that subsection’s content tensor-theoretically.
As usual, we fix a field extension L/K and a basis (βv )v∈Bκ of L over K. We moreover fix a K-multilinear
t(X0 , . . . , Xℓ−1 ) and an evaluation point (r0 , . . . , rℓ−1 ) over L. Finally, we again write t′ (X0 , . . . , Xℓ′ −1 ) for
t(X0 , . . . , Xℓ−1 )’s packed multilinear (see (1)). Above, we argued that, in order for the verifier to assess the
?

claim t(r0 , . . . , rℓ−1 ) = s, it’s enough for the prover to send values (ŝv )v∈Bκ respectively claimed to equal
(t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ))v∈Bκ . Provided it can verify these latter claims, the verifier must just check (3).
We begin with the following basic fact about the partial evaluations (t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ))v∈Bκ .
For each v ∈ Bκ :
X
e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 )
(27)
t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ) =
w∈Bℓ′

holds, essentially by multilinear extension. In other words, to get t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ), we should
take the weighted sum over w ∈ Bℓ′ —with weights (f
eq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ))w∈Bℓ′ —of the “slice components” t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ). That is, as the chunk index w ∈ Bℓ′ varies, we take, in each case,
the v th component of the wth chunk of t(X0 , . . . , Xℓ−1 )’s Lagrange coefficient vector.
Figure 10 below—in which we horizontally “stack” all 2κ instances of the relationship (27) (i.e., for v ∈ Bκ
varying)—reveals something interesting.

t′ (1, . . . , 1)

+···+

v ∈ Bκ varying

eq(r
e κ , . . . , rℓ−1 , 1, . . . , 1)

=

eq(r
e κ , . . . , rℓ−1 , 0, . . . , 0)

···

t(1, . . . , 1, rκ , . . . , rℓ−1 )

t(0, . . . , 0, rκ , . . . , rℓ−1 )

t′ (0, . . . , 0)

w ∈ Bℓ′ varying

Figure 10: A graphical depiction of t(X0 , . . . , Xℓ−1 )’s partial evaluations.
For each column index v ∈ Bκ , the v th “column slice” of Figure 10 is simply (27). The point is that, for
each column index v ∈ Bκ and each summand index w ∈ Bℓ′ , slice component t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 )
we want is exactly the v th horizontal component of t′ (w) (this is just the definition of packing, i.e. (1)).
In Figure 10’s right-hand side, we have the sum, over varying w ∈ Bℓ′ , of the matrices
e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) ⋆ t′ (w). Here, we again use the ⋆ symbol to denote the “exterior product”
between two L-elements (recall Subsection 1.3). That is, we basis-decompose both operands into K-vectors,
and then take the 2κ × 2κ K-matrix of cross-products between these vectors.
On the other hand, by taking the exact same picture and viewing it row-wise, we obtain relationships that
we can sumcheck. As in Subsection 1.3, for each w ∈ Bℓ′ , we write (Aw,u )u∈Bκ for the basis-decomposition
of e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ); that is, for each w ∈ Bℓ′ , we define these elements so that
X
e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) =
Aw,u · βu
(28)
u∈Bκ

holds (recall also (8)). Interpreting Figure 10 row-wise, we obtain Figure 11 below, which, again, is true
essentially by definition. Indeed, “zooming into” some row u ∈ Bκ of Figure 11, we note that that Figure
11’s right-hand side sums the slice-product Aw,u · t′ (w) over w ∈ Bℓ′ .
22

.
.
.

P

t′ (1, . . . , 1)

t′ (0, . . . , 0)

′
w∈Bℓ′ Aw,(0,...,0) · t (w)

=

′
w∈Bℓ′ Aw,(1,...,1) · t (w)

+···+

eq(r
e κ , . . . , rℓ−1 , 1, . . . , 1)



























P

eq(r
e κ , . . . , rℓ−1 , 0, . . . , 0)

u ∈ Bκ varying




























w ∈ Bℓ′ varying

Figure 11: A row-wise viewpoint into the same exact expression.
The point is that the rows of Figure 11 are amenable
to the sumcheck protocol—provided, that is, that

the verifier can efficiently evaluate Au (r0′ , . . . , rℓ′ ′ −1 ) u∈B , for some random point (r0′ , . . . , rℓ′ ′ −1 ) that arises
κ
during the sumcheck. Here, for each u ∈ Bκ , we define Au (X0 , . . . , Xℓ′ −1 ) to be the multilinear extension
of the map Au : w 7→ Aw,u , for w ∈ Bℓ′ varying. This gets us to where we left off in Subsection 1.3, i.e. to
Figure 3.
In fact, Figures 3, 10 and 11 all arise naturally as tensor-algebraic expressions. We recall the tensor
algebra A := L ⊗K L from Subsection 2.5. Each element of the tensor algebra looks, concretely, like a 2κ × 2κ
array of K-elements. We may freely interpret each such element, say ŝ ∈ A, both column-wise and row-wise.
That is, we can interpret each of ŝ’s columns as an L-element, and so obtain a list (ŝv )v∈Bκ of L-elements;
alternatively, we can interpret each of ŝ’s rows as an L-element, thereby obtaining a further list (ŝu )u∈Bκ .
As should be clear by now, our theory hinges on a viewpoint that maintains both of these perspectives at
once. The tensor algebra furnishes the arena within which we might best do this.
The important thing about the tensor algebra is its multiplication operation, and in particular how that
operation handles columns and rows. Given some L-element α, we can obtain an A-element by inscribing α’s
K-decomposition into the left-hand column of a 2κ × 2κ array. The resulting A-element is φ0 (α). Similarly,
we may equally inscribe α’s K-decomposition into the top row of a 2κ × 2κ K-array; this operation yields
φ1 (α). (We again recall Subsection 2.5.) The important thing about the tensor algebra’s multiplication
structure is that it captures the ⋆ operation. In fact, for each pair of L-elements α0 and α1 ,
α0 ⋆ α1 = φ0 (α0 ) · φ1 (α1 ).

(29)

In (29), the left-hand product is of course the exterior product, whereas the right-hand product is the ambient
multiplication operation in the tensor algebra.
Figures 10 and 11 represent the tensor-algebraic identity
X
φ1 (t′ )(φ0 (rκ ), . . . , φ0 (rℓ−1 )) =
φ0 (f
eq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 )) · φ1 (t′ (w0 , . . . , wℓ′ −1 )).
(30)
w∈Bℓ′

The identity (30) captures, in just one expression, “both views” (i.e., the views respectively expressed by
Figures 10 and 11). Here, we write φ1 (t′ )(X0 , . . . , Xℓ′ −1 ) for the A-valued multilinear defined by the Lagrange
prescription w 7→ φ1 (t′ (w)) (i.e., for w ∈ Bℓ′ varying).
Our key claim is that the right-hand side of Figure 3 is simply

e
fq φ0 (rκ ), . . . , φ0 (rℓ−1 ), φ1 (r0′ ), . . . , φ1 (rℓ′ ′ −1 )
(31)
(recall also (14)). This nontrivial fact is key to our theory; it implies that the verifier may locally compute
the right-hand side of Figure 3 succinctly, by operating in the algebra A itself. In more concrete terms,
the verifier may compute (31) concretely in just 2 · ℓ′ · 2κ L-multiplications, a fact we explain rigorously in
Remark 3.4 below. In Subsection 3.2 below, we argue that ring-switching is concretely and asymptotically
efficient for both the prover and the verifier, and is essentially optimal.
23

3.1

Ring-Switching Protocol

We now record our ring-switching reduction.
CONSTRUCTION 3.1 (Ring-Switching Compiler).

A large-field scheme Π′ = Setup′ , Commit′ , P ′ , V ′ is given as input. We define the small-field scheme
Π = (Setup, Commit, P, V) in the following way.
1. params ← Π.Setup(1λ , ℓ, K). On input 1λ , ℓ, and K, run and output Π′ .Setup′ (1λ , ℓ′ ), where ℓ′ is
such that the field L / K returned by that routine, of degree 2κ over K say, satisfies ℓ′ = ℓ − κ.
2. [f ] ← Π.Commit(params, t). On input t(X0 , . . . , Xℓ−1 ) ∈ K[X0 , . . . , Xℓ−1 ]⪯1 , fix the packed polynomial t′ (X0 , . . . , Xℓ′ −1 ) ∈ L[X0 , . . . , Xℓ′ −1 ]⪯1 as in Definition 2.2; output Π′ .Commit′ (params, t′ ).
We define (P, V) as the following IOP, in which both parties have the common input [f ], s ∈ L, and
(r0 , . . . , rℓ−1 ) ∈ Lℓ , and P has the further input t(X0 , . . . , Xℓ−1 ) ∈ K[X0 , . . . , Xℓ−1 ]⪯1 .
1. P computes ŝ := φ1 (t′ )(φ0 (rκ ), . . . , φ0 (rℓ−1 )) and sends V the A-element ŝ.
2. V decomposes ŝ =:

? P
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · ŝv .
v∈Bκ ŝv ⊗ βv . V requires s =
v∈Bκ e

P

′′
3. V samples batching scalars (r0′′ , . . . , rκ−1
) ← Lκ and sends them to P.
P
4. For each w ∈ Bℓ′ , P
P decomposes e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) =: u∈Bκ Aw,u · βu . P defines the
′′
function A : w 7→ u∈Bκ e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · Aw,u on Bℓ′ and writes A(X0 , . . . , Xℓ′ −1 )
for its multilinear extension. P defines h(X0 , . . . , Xℓ′ −1 ) := A(X0 , . . . , Xℓ′ −1 ) · t′ (X0 , . . . , Xℓ′ −1 ).
P
P
′′
5. V decomposes ŝ =: u∈Bκ βu ⊗ ŝu , and sets s0 := u∈Bκ e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · ŝu .

6. P and V execute the following standard sumcheck loop:
1: for i ∈ {0, . . . , ℓ′ − 1} do

P
′
, X, w0 , . . . , wℓ′ −i−2 .
2:
P sends V the polynomial hi (X) := w∈Bℓ′ −i−1 h r0′ , . . . , ri−1
3:

?

V requires si = hi (0) + hi (1). V samples ri′ ← L, sets si+1 := hi (ri′ ), and sends P ri′ .

7. P computes s′ := t′ (r0′ , . . . , rℓ′ ′ −1 ) and sends V s′ .

P
8. V sets e := e
fq φ0 (rκ ), . . . , φ0 (rℓ−1 ), φ1 (r0′ ), . . . , φ1 (rℓ′ ′ −1 ) and decomposes e =: u∈Bκ βu ⊗ eu .
?

9. V requires sℓ′ =

′′
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · eu
u∈Bκ e

P



· s′ .

10. P and V engage in the evaluation protocol b′ ← ⟨P ′ ([f ], s′ , r′ ; t′ ), V ′ ([f ], s′ , r′ )⟩; V outputs b := b′ .

Theorem 3.2. If Π′ = Setup′ , Commit′ , P ′ , V ′ is complete, then Π = (Setup, Commit, P, V) also is.
Proof. We must prove three main things. First, we must show that, if P constructs ŝ ∈ A honestly, then V’s
? P
check s = v∈Bκ e
fq(r0 , . . . , rκ−1 , v0 , . . . , vκ−1 ) · ŝv will pass.
P
P Further, we must show that V’s quantity s0 :=
′′
′′
fq(u0 , . . . , uκ−1 , r0 , . . . , rκ−1 ) · ŝu will satisfy s0 = w∈Bℓ′ h(w), so that V will accept throughout its
u∈Bκ e
sumcheck. Finally, P
we must show that V’s final check will pass; this
P task amounts to showing′′that e’s′′ rowfq(u0 , . . . , uκ−1 , r0 , . . . , rκ−1 ).
representation e = u∈Bκ βu ⊗ eu will satisfy A(r0′ , . . . , rℓ′ ′ −1 ) = u∈Bκ eu · e
We begin with the first fact above. If P operates as prescribed, then its initial message ŝ ∈ A will satisfy:
X
ŝ := φ1 (t′ )(φ0 (rκ ), . . . , φ0 (rℓ−1 )) =
e
fq(φ0 (rκ ), . . . , φ0 (rℓ−1 ), w0 , . . . , wℓ′ −1 ) · φ1 (t′ )(w).
(32)
w∈Bℓ′

By the definition of φ1 (t′ )(X0 , . . . , Xℓ′ −1 ), for each w ∈ Bℓ′ , we have the column decomposition φ1 (t′ )(w) =
P
′
fq(φ0 (rκ ), . . . , φ0 (rℓ−1 ), w0 , . . . , wℓ′ −1 ) =
v∈Bκ t(v0 , . . . , vκ−1 , w0 , . . . , wℓ −1 ) ⊗ βv . On the other hand, e
′
φ0 (f
eq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ −1 )). Using the column-multiplication rule, we obtain, for each summand
24

w
eq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 )) · φ1 (t′ )(w) =
P∈ Bℓ′ of the sum (32) above, the column decomposition φ0 (f
eq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 )) ⊗ βv . Inlining this expression into the
v∈Bκ (f
sum (32) above, we obtain:
X
e
fq(φ0 (rκ ), . . . , φ0 (rℓ−1 ), w0 , . . . , wℓ′ −1 ) · φ1 (t′ )(w)
(by (32).)
ŝ =
w∈Bℓ′

!
X

X

w∈Bℓ′

v∈Bκ

=

(f
eq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 )) ⊗ βv





X

=

X

e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ) ⊗ βv


v∈Bκ

X

=

(column values.)

(rearranging sums.)

w∈Bℓ′

t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ) ⊗ βv .

(fundamental property of multilinears.)

v∈Bκ

That is, V’s column-decomposition ŝ =

P

v∈Bκ ŝv ⊗ βv will satisfy ŝv = t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ) for each
?
v ∈ Bκ . Assuming now that P’s initial claim s = t(r0 , . . . , rℓ−1 ) is true, we obtain:

s = t(r0 , . . . , rℓ−1 )
X
=
e
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 )

(by the truth of P’s claim.)
(partial multilinear expansion.)

v∈Bκ

=

X

e
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · ŝv .

(by the calculation just carried out.)

v∈Bκ
? P
In particular, V will accept its first check s = v∈Bκ e
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · ŝv .
We turn to the sumcheck. As a notational device, we define the A-valued polynomial:

ĥ(X0 , . . . , Xℓ′ −1 ) := e
fq(φ0 (rκ ), . . . , φ0 (rℓ−1 ), X0 , . . . , Xℓ′ −1 )) · φ1 (t′ )(X0 , . . . , Xℓ′ −1 ).
Informally, we must show that P’s polynomial h(X0 , . . ., Xℓ′ −1 ) above is a “row-combination” of
′′
ĥ(X0 , . . . , Xℓ′ −1 ) by the vector e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) u∈Bκ .
On the one hand, we note immediately that
X
X
e
fq(φ0 (rκ ), . . . , φ0 (rℓ−1 ), w0 , . . . , wℓ′ −1 ) · φ1 (t′ )(w) = φ1 (t′ )(φ0 (rκ ), . . . , φ0 (rℓ−1 )) = ŝ;
ĥ(w) =
w∈Bℓ′

w∈Bℓ′

the last equality holds precisely when P constructs ŝ honestly.
On the other hand, for each w ∈ Bℓ′ :
ĥ(w) = e
fq(φ0 (rκ ), . . . , φ0 (rℓ−1 ), w0 , . . . , wℓ′ −1 ) · φ1 (t′ )(w)
(by definition of ĥ.)
!
X
=
βu ⊗ Aw,u · (1 ⊗ t′ (w))
(by the definitions of φ1 (t′ ) and of Aw,u .)
u∈Bκ

=

X

βu ⊗ (Aw,u · t′ (w)).

(distributing and using the multiplicative structure of A.)

u∈Bκ

We explain in slightly further detail the second equality above. Indeed, we use first the fact—already noted
above—whereby e
fq(φ0 (rκ ), . . . , φ0 (rℓ−1 ), w0 , . . . , wℓ′ −1 ) = φ0 (f
eq(rκ , . . .P
, rℓ−1 , w0 , . . . , wℓ′ −1 )). On the other
hand, since the basis decomposition e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) = u∈Bκ Aw,u · βu holds by definition
of
P the elements Aw,u , the row representation of this quantity’s image under φ0 can be none other than
u∈Bκ βu ⊗ Aw,u , which is what appears above.
Combining the above two calculations, we conclude that, if P is honest, then


!
X
X X
X
X
ŝ =
ĥ(w) =
βu ⊗ (t′ (w) · Aw,u ) =
βu ⊗ 
Aw,u · t′ (w)
(33)
w∈Bℓ′

w∈Bℓ′

u∈Bκ

u∈Bκ

25

w∈Bℓ′

P
P
will hold, so that V’s row decomposition ŝ = u∈Bκ βu ⊗ ŝu will satisfy ŝu = w∈Bℓ′ Aw,u · t′ (w) for each
u ∈ Bκ . We conclude that, if P constructs ŝ correctly, then
X
X
h(w) =
A(w) · t′ (w)
(by definition of h(X0 , . . . , Xℓ′ −1 ).)
w∈Bℓ′

w∈Bℓ′

!
=
=

X

X

w∈Bℓ′

u∈Bκ

X

′′
e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · Aw,u

′′
e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
)·

u∈Bκ

=

X

X

· t′ (w) (by definition of A(X0 , . . . , Xℓ′ −1 ).)

Aw,u · t′ (w)

(interchanging the above sums.)

w∈Bℓ′
′′
e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · ŝu

(by (33) and the remarks below it.)

u∈Bκ

= s0

(by definition of the verifier.)
P

will hold, so that P’s sumcheck claim s0 = w∈Bℓ′ h(w) will be valid, and V will accept throughout the
course of its sumcheck, by the completeness of that latter protocol.
We turn to V’s final check. If P is honest, then s′ = t′ (r0′ , . . . , rℓ′ ′ −1 ) will hold; moreover, by definition
′
′
of the sumcheck, we will have sℓ′ =
V’s final check, it thus suffices to argue that
Ph(r0 , . . . , rℓ′ −1 ). To treat
′
′
′ ′
′
′′
h(r0 , . . . , rℓ′ −1 ) = t (r0 , . . . , rℓ′ −1 ) · u∈Bκ e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · eu will hold; to show this, it in turn
suffices, by definition of h(X0 , . . . , Xℓ′ −1 ), to prove that
X
′′
A(r0′ , . . . , rℓ′ ′ −1 ) =
e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · eu .
u∈Bκ

We proceed as follows. We note first that:

e=e
fq φ0 (rκ ), . . . , φ0 (rℓ−1 ), φ1 (r0′ ), . . . , φ1 (rℓ′ ′ −1 )
(by definition.)
X

′
′
e
fq(φ0 (rκ ), . . . , φ0 (rℓ−1 ), w0 , . . . , wℓ′ −1 ) · e
fq φ1 (r0 ), . . . , φ1 (rℓ′ −1 ), w0 , . . . , wℓ′ −1
(see below.)
=
w∈Bℓ′

=

X

φ0 (f
eq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 )) · φ1 e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 )



(pulling out φ0 and φ1 .)

w∈Bℓ′

!
=

X

X

w∈Bℓ′

u∈Bκ

βu ⊗ Aw,u


· 1⊗e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 )


=

X
u∈Bκ

βu ⊗ 

(again by definition of the Aw,u .)


X

Aw,u · e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 ).

(multiplying in A and rearranging.)

w∈Bℓ′

To achieve the second equality above, we note that the multilinears e
fq(X0 , . . . , Xℓ′ −1 , Y0 , . . . , Yℓ′ −1 ) and
P
′ −1 , w0 , . . . , wℓ′ −1 )· e
′ −1 , Y0 , . . . , Yℓ′ −1 ) are necessarily identical, since they
e
f
q(X
,
.
.
.
,
X
f
q(w
,
.
.
.
,
w
0
ℓ
0
ℓ
w∈Bℓ′
agree identically on the cube B2·ℓ′ .
P
P
We see that the verifier’s row-decomposition e =
u∈Bκ βu ⊗ eu will satisfy eu =
w∈Bℓ′ Aw,u ·
e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 ) for each u ∈ Bκ . We conclude finally V will have
X
X
X
′′
e
fq(r′′ , u) · eu =
e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
)·
Aw,u · e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 )
u∈Bκ

u∈Bκ

w∈Bℓ′

!
=
=

X

X

w∈Bℓ′

u∈Bκ

X

′′
e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · Aw,u

·e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 )

A(w0 , . . . , wℓ′ −1 ) · e
fq(w0 , . . . , wℓ′ −1 , r0′ , . . . , rℓ′ ′ −1 )

w∈Bℓ′

= A(r0′ , . . . , rℓ′ ′ −1 ),
which is exactly what we needed to show. This completes the proof of completeness.
26

Remark 3.3. We explain in slightly more rigorous terms the “information loss” which would result if the
parties merely evaluated t′ (rκ , . . . , rℓ−1 ), as opposed to using the tensor algebra. During Theorem 3.2’s
proof, we show that ŝv = t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ) holds for each v ∈ Bκ . On the other hand,
X
e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) · t′ (w)
t′ (rκ , . . . , rℓ−1 ) =
w∈Bℓ′

=

X

X

e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) ·

t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ) · βv

v∈Bκ

w∈Bℓ′


=

X

v∈Bκ

=


X

X

e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) · t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 ) · βv

w∈Bℓ′

t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ) · βv

v∈Bκ

=

X

ŝv · βv .

v∈Bκ

P
We see that, while the information contained in ŝ = v∈Bκ ŝv ⊗ βv suffices to recover t(r0 , . . . , rℓ−1 ) (as
the
P proof of Theorem 3.2 above shows), the datum t(rκ , . . . , rℓ−1 ) would yield, rather, the basis-combination
independent over L,
v∈Bκ ŝv · βv of ŝ’s columns. Since the K-basis (βv )v∈Bκ is certainly not linearly
P
this latter combination reflects ŝ only “lossfully”. We note that, interestingly, v∈Bκ ŝ · βv = h(ŝ) holds;
here, h : L ⊗K L → L is the canonical K-linear map defined on simple tensors by multiplication (we recall
Subsection 2.5 above). That is, t(rκ , . . . , rℓ−1 ) relates to φ1 (t′ )(φ0 (rκ ), . . . , φ0 (rℓ−1 )) exactly by the map h,
which is of course not injective. We would like to thank Raju Krishnamoorthy for explaining this fact to us.

Remark 3.4. We discuss the verifier’s computation of e := e
fq φ0 (rκ ), . . . , φ0 (rℓ−1 ), φ1 (r0′ ), . . . , φ1 (rℓ′ ′ −1 ) .
Clearly, this computation amounts to O(ℓ′ ) arithmetic operations in the algebra A, and so can be carried out
in polylogarithmic time for the verifier in the worst case. Here, we discuss a concretely efficient procedure by
whose aid the verifier may compute e, at least in the characteristic 2 case. Indeed, we note first the following
identity, valid only in characteristic 2:
e
fq(X0 , . . . , Xℓ′ −1 , Y0 , . . . , Yℓ′ −1 ) :=

′
ℓY
−1

′
ℓY
−1

i=0

i=0

(1 − Xi ) · (1 − Yi ) + Xi · Yi =

1 − Xi − Yi .

This identity suggests the correctness of the following algorithm:
1: initialize the A-element e := 1.
2: for i ∈ {0, . . . , ℓ′ − 1} do update e −= e · φ0 (rκ+i ) + e · φ1 (ri′ ).
3: return e.
Concretely, e · φ0 (rκ+i ) means “scale each column of e by rκ+i ”. Similarly, e · φ1 (ri′ ) means “scale each row
of e by ri′ ”. In practical implementations, e will reside at rest in column form (i.e., as a list of 2κ column
L-elements). Column-scaling will be achieved by componentwise multiplication. Row-scaling will amount to
a transposition, a componentwise multiplication, and a further transposition.
The total cost of this procedure is thus 2 · ℓ′ · 2κ L-by-L multiplications, as well as 2 · ℓ′ transpositions.
We now prove the security of ring-switching.

Theorem 3.5. If Π′ = Setup′ , Commit′ , P ′ , V ′ is secure, then Π = (Setup, Commit, P, V) also is.
Proof. We write E ′ for the emulator for Π′ . We define an emulator E for Π as follows.
1. On input A’s record of interactions with the vector oracle, E internally runs t′ (X0 , . . . , Xℓ′ −1 ) ← E ′ .
2. If t′ (X0 , . . . , Xℓ′ −1 ) = ⊥, then E outputs ⊥ and aborts.
3. By reversing Definition 2.2, E obtains t(X0 , . . . , Xℓ−1 ) ∈ K[X0 , . . . , Xℓ−1 ]⪯1 , which it outputs.

27

We argue that the emulator E defined in this way is secure. If V ′ rejects, then V also does. The probability
with which E ′ outputs ⊥ and V ′ accepts is negligible, by the security of Π′ . So too, therefore, is the probability
with which E outputs ⊥ and V accepts. We thus fix our attention on those executions of the experiment for
which t′ (X0 , . . . , Xℓ′ −1 ) ̸= ⊥; in particular, we assume that t(X0 , . . . , Xℓ−1 ) ̸= ⊥. Similarly, the probability
with which t′ (X0 , . . . , Xℓ′ −1 ) ̸= ⊥, t′ (r′ ) ̸= s′ , and b′ = 1 all hold is negligible, by the security of Π′ . We
thus focus our attention on those executions for which t′ (r′ ) = s′ . We must show that the probability with
which t(r) ̸= s and V accepts is negligible. We assume now that t(r) ̸= s.
We may further restrict our considerations to the set of executions within which P computes its first
message ŝ ̸= φ1 (t′ )(φ0 (rκ ), . . . , φ0 (rℓ−1 )) incorrectly. Indeed, it is shownPdirectly in the course of our proof of
Theorem 3.2 above that, if ŝ = φ1 (t′ )(φ0 (rκ ), . . . , φ0 (rℓ−1 )) holds, then v∈Bκ e
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 )·
ŝv = t(r0 , . . . , rℓ−1 ) also will. In this latter setting,
s ̸= t(r0 , . . . , rℓ−1 )
(by our initial assumption above whereby P’s claim is false.)
X
=
e
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · ŝv
(a consequence of ŝ = φ1 (t′ )(φ0 (rκ ), . . . , φ0 (rℓ−1 )).)
v∈Bκ

will hold, so that V will reject and we’re done. We thus assume that ŝ ̸= φ1 (t′ )(φ0 (rκ ), . . . , φ0 (rℓ−1 )).
′
For the
P sake of notation, we abbreviate s := φ1 (t )(φ0 (rκ ), . . . , φ0 (rℓ−1 )) for this latter quantity, and write
s =: u∈Bκ βu ⊗ su for its row-decomposition.
Our hypothesis whereby ŝ ̸= s entails that the κ-variate polynomial over L
X
S(X0 , . . . , Xκ−1 ) :=
(ŝu − su ) · e
fq(u0 , . . . , uκ−1 , X0 , . . . , Xκ−1 )
u∈Bκ

is not identically zero. Applying Schwartz–Zippel to S(X0 , . . . , Xκ−1 ), we conclude that the probability, over
κ
′′
′′
, which is negligible. We
V’s choice of (r0′′ , . . . , rκ−1
) ← Lκ , that S(r0′′ , . . . , rκ−1
) = 0 will hold is at most |L|
′′
′′
thus assume that S(r0 , . . . , rκ−1 ) ̸= 0, which itself immediately entails that:
s0 :=

X

′′
ŝu · e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) ̸=

u∈Bκ

X

′′
su · e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
)

u∈Bκ

will hold. On the other hand, by an argument identical to one already given during the proof of Theorem
3.2 above, we have that:
X
X
′′
h(w) =
e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · su ;
w∈Bℓ′

u∈Bκ

we again write h(X0 , . . . , Xℓ′ −1 ) := A(X0 , . . . , Xℓ′ −1 ) · t′ (X0 , . . . , Xℓ′ −1 ) (as usual, t(X0 , . . . , Xℓ−1 ) here
refers to what E extracted). Combining the
P above two equations, we conclude—again under our hypothesis
′′
whereby S(r0′′ , . . . , rκ−1
) ̸= 0—that s0 ̸= w∈Bℓ′ h(w). By the soundness of the sumcheck, we conclude that

the probability with which V accepts throughout that protocol and sℓ′ = h r0′ , . . . , rℓ′ ′ −1 holds is at most

2·ℓ′
′
′
′
|L| , which is negligible. We thus assume that sℓ ̸= h r0 , . . . , rℓ′ −1 , or in other words that:
sℓ′ ̸= A(r0′ , . . . , rℓ′ ′ −1 ) · t(r0′ , . . . , rℓ′ ′ −1 ).
P
′′
The proof of Theorem 3.2 already shows that A(r0′ , . . . , rℓ′ ′ −1 ) = u∈Bκ e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · eu .
On the other hand, we’ve already justified our consideration just of those executions within which s′ =
t′ (r0′ , . . . , rℓ′ ′ −1 ) holds. Under exactly this latter condition, therefore, the verifier will obtain:
!
sℓ′ ̸= A(r0′ , . . . , rℓ′ ′ −1 ) · t(r0′ , . . . , rℓ′ ′ −1 ) =

X
u∈Bκ

and so will once again reject. This completes the proof.

28

′′
e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · eu

· s′ ,

3.2

Efficiency

We examine the efficiency of Protocol 3.1. Throughout, we count K-operations. We view the ground field
K as constant in both λ and ℓ. We must first clarify how L relates to K. The soundness error of Protocol
′
3.1 is 2·ℓ|L|+κ . In order for this quantity to be negligible, it’s enough that |L| ≥ 2ω(log λ) hold (we note that
ℓ = O(log λ) must hold, lest the prover’s input length 2ℓ fail to be polynomial in the security parameter).
For simplicity, we assume that deg(L / K) = λ. In this case, |L| = |K|λ ≥ 2λ , so that Protocol 3.1 in fact
ℓ
′
becomes exponentially secure. We moreover obtain 2κ = λ and 2ℓ = 2λ .
We refer to von zur Gathen and Gerhard for [GG13] for complexity-theoretic background. For the purposes of this subsection, we understand L as a univariate extension of K (see [GG13, § 25.4]). Following
[GG13, Def. 8.26], we write M(λ) for the complexity—measured in K-operations—of multiplying two polye
nomials in K[X] of degree at most λ. It is known [GG13, Thm. 8.23] that we may take M(λ) = O(λ).
e
Multiplication in L can be carried out in 6 · M(λ) + O(λ) = O(λ)
K-operations [GG13, Cor. 11.11]. We
e
write Q(λ) = O(M(λ)) = O(λ) for the cost, in K-operations, of L-multiplication. We note that each K-by-L
multiplication takes exactly λ K-multiplications; each L-addition costs exactly λ K-additions.
Prover cost. The prover’s main cost is that of computing the tensor-expansion
h

ℓ−1
O

i

(1 − ri , ri )

.

(34)

i=κ

|

{z

′

}

2ℓ elements
′

In view of the standard algorithm for it (see Subsection 2.1), this task takes 2ℓ L-multiplications and 2ℓ
L-additions. We see that the total number of K-operations associated with this task is
 ℓ
 ′
Q(λ)
2
e
= O(2ℓ ) · O(1),
O 2ℓ · Q(λ) = O
· Q(λ) = O(2ℓ ) ·
λ
λ

′

which is linear in the input length and polylogarithmic in λ.
Having computed (34), the prover must use it in two places. First, to compute ŝ in step 1, the prover must
compute all 2κ partial evaluations ŝv = t(v0 , . . . , vκ−1 , rκ , . . . , rℓ−1 ); i.e., it must compute all 2κ dot-products
+
*
ℓ−1
h
i h
i
O
(1 − ri , ri )
ŝv :=
,
(35)
,
t(v0 , . . . , vκ−1 , w0 , . . . , wℓ′ −1 )
|
{z
}
i=κ
w∈Bℓ′ varying

′

′

for v ∈ Bκ varying. Each dot-product (35) takes 2ℓ L-by-K multiplications and 2ℓ L-additions. The total
cost associated with the dot-products (35) is thus
′

2κ · O(2ℓ ) · λ = O(2ℓ ) · λ

u ∈ Bκ varying

K-operations.
Nκ−1
Finally, to prepare its sumcheck in step 4, the prover must first tensor-expand i=0 (1 − ri′′ , ri′′ )—a task
whose cost is polynomial in λ alone, and which we ignore—and then compute the L-by-K matrix product:













κ−1


h
i 
O
A

′′ ′′
···
A(1,...,1),u 
(1 − ri , ri )
·  (0,...,0),u
.
(36)


i=0












The product (36) yields exactly the table of values of A(X0 , . . . , Xℓ′ −1 ) on the cube Bℓ′ .
29

′

To obtain the 2κ × 2ℓ matrix in the right-hand side of (36), the prover must use (34) again; specifically, it
must simply basis-decompose each component of that tensor. This task is simply a rewiring operation, and
′
is free (recall also (28)). To compute the matrix product (36), the prover must again perform 2κ · 2ℓ = 2ℓ
L-by-K multiplications and 2ℓ L-additions; the total cost of (36) is thus again O(2ℓ ) · λ K-operations.
The sumcheck 6 is ℓ′ -dimensional over L. In view of standard algorithms for this task (we recall the
′
treatment of Thaler [Tha22, § 4.1]), the prover can carry it out in O(2ℓ ) total L-operations. Just as in (34)
e
above, the prover’s total cost during the sumcheck is therefore again O(2ℓ ) · O(1)
total K-operations.
In sum, we see that our prover’s cost is O(2ℓ ) · λ K-operations. Our prover is thus linear (in both the
statement size and the security parameter). Interestingly, our prover is asymptotically dominated by the
respective costs of K-by-L multiplication and L-addition. These tasks are, in practice, cheaper than Lmultiplication is. Upon ignoring these former costs—and considering only the cost of L-multiplication—we
e
would in fact obtain a prover costing just O(2ℓ ) · O(1)
K-multiplications (that is, with just a polylogarithmic,
and not a linear, multiplicative dependence on the security parameter λ).
Verifier cost. To receive ŝ, the verifier must receive 2κ L-elements, whose total
size is that of 22·κ = λ2
Nκ−1
K-elements. In its step 2 above, the verifier must compute the tensor-expansion i=0 (1 − ri , ri ), as well as
dot the resulting tensor with (ŝv )v∈Bκ . These tasks, together, amount to O(2κ ) L-operations, and so cost in
total
e 2)
O(2κ ) · Q(λ) = O(λ) · Q(λ) = O(λ
K-operations. The verifier’s respective combination tasks 5 and 9 are structurally identical to 2, and have
identical costs; we skip our analyses of them.
During the sumcheck 6, the verifier must expend O(ℓ′ ) L-operations; these cost in total O(ℓ′ ) · Q(λ) ≤
O(ℓ) · Q(λ) K-operations. To compute e in step 8, the verifier must perform 2 · 2κ · ℓ′ L-operations (recall
Remark 3.4). These in total cost
e 2)
O(ℓ′ ) · 2κ · Q(λ) ≤ O(ℓ) · O(λ
K-operations. Our verifier is thus logarithmic in the statement size and quadratic in λ (up to a polylogarithmic multiplicative factor).
Other costs. Protocol 3.1’s prover must invoke Π′ ’s commitment procedure exactly once. During Protocol
3.1’s evaluation phase, the prover and the verifier must jointly invoke the underlying large-field commitment
scheme Π′ ’s evaluation procedure exactly once. We ignore all of these costs in this subsection, and only
measure the efficiency of Protocol 3.1 over and above them.

v ∈ Bκ varying

Comparison to Hashcaster. To prepare the sumcheck (21) naively, Hashcaster [Sou24]’s prover would
need to compute the matrix product



Nℓ−1
0
0


i=κ 1 − σ (ri ), σ (ri )









κ−1


i 
h
O


..
(1 − ri′′ , ri′′ )
·
.
(37)

.


i=0












Nℓ−1

2κ −1
2κ −1
1
−
σ
(r
),
σ
(r
)
i
i
i=κ
To compute that matrix product, Hashcaster’s prover would need to expend Θ(2ℓ ) L-multiplications, thereby
incurring Θ(2ℓ ) · Q(λ) K-operations. That prover’s cost would thus be worse than ours by a polylogarithmic
factor in λ, at least. As it turns out, by nontrivially using the Galois transformation (22), Hashcaster is able
to bring its prover cost closer to ours, albeit with various additive polylogarithmic penalties in λ. Its prover,
on the other hand, is much more complicated, and runs the sumcheck using a non-blackbox algorithm.
To compute the Galois transformation (18), Hashcaster’s verifier must expend 22·κ L-multiplications, for
a total cost of
e 3)
22·κ · Q(λ) = λ2 · Q(λ) = O(λ
K-operations. We thus see that Hashcaster’s verifier’s dependence on λ is cubic, as opposed to quadratic.
30

4

Binary BaseFold

In this section, we present our large-field PCS. We have already sketched the main problem in Subsection
1.4 above; here, we record a few further details.
P2ℓ −1
The additive NTT. The additive NTT is the problem of evaluating a polynomial P (X) = j=0 aj · X j
with coefficients in a binary field L, of degree less than 2ℓ say, on some ℓ-dimensional F2 -linear subspace
S ⊂ L. In a classic and farsighted work, Cantor [Can89] introduces the additive NTT and devises an
O(2ℓ · ℓlog2 (3) )-time algorithm for it. In the specific case of ℓ a power of 2, Gao and Mateer [GM10] further
achieve the complexity O(2ℓ · ℓ · log ℓ).
For some time, it was not known whether the characteristic O(2ℓ · ℓ) time complexity of the Cooley–
Tukey algorithm could be attained in the additive case. In a key work, Lin, Chung and Han [LCH14] achieve
exactly this feat, with a caveat: their algorithm interprets its input vector as P (X)’s coefficients not in the
standard monomial basis, but in a novel polynomial basis that those authors introduce. That is, on the
P2ℓ −1
P2ℓ −1
input (a0 , . . . , a2ℓ −1 ), their algorithm evaluates not j=0 aj · X j , but rather j=0 aj · Xj (X), over the
domain S ⊂ L; here, for each j ∈ {0, . . . , 2ℓ − 1}, Xj (X) is an alternate basis polynomial—of degree j—that
2ℓ −1

ℓ

those authors describe. (The polynomials (Xj (X))j=0 span the L-vector space L[X]≺2 , so the “space” of
polynomials that Lin, Chung and Han’s algorithm serves to encode is the same as that of Cantor’s.) Lin,
2ℓ −1

Chung and Han [LCH14] build their basis polynomials (Xj (X))j=0 out of subspace vanishing polynomials.
ci (X), for i ∈ {0, . . . , ℓ}, which respectively vanish identically on an ascending chain
These are polynomials W
of F2 -subspaces U0 ⊂ · · · ⊂ Uℓ of L.
Our binary adaptation of BaseFold PCS ties together two disparate threads: Lin, Chung and Han
[LCH14]’s additive NTT and FRI [BBHR18a]. We recall that binary-field FRI works with the aid of a
sequence of F2 -subspaces S (0) , . . . , S (ℓ) of L, themselves connected by linear subspace polynomials:
q (0)

q (1)

q (2)

q (ℓ−1)

S (0) −−→ S (1) −−→ S (2) −−→ · · · −−−−→ S (ℓ) .
Here, the maps q (0) , . . . , q (ℓ−1) are linear subspace polynomials of degree 2.
Haböck, Levit and Papini [HLP24, § 4.1] implicitly argue that, from any such chain of subspaces, “an”
FFT arises. Our goal is to show that, for an appropriately chosen chain, Lin, Chung and Han’s additive
NTT can itself be made to arise.
Choosing the folding maps. To use BaseFold PCS in characteristic 2, we must use the additive NTT
instead of the standard one, and use binary-field FRI instead of prime-field FRI. Simple enough, but which
domains S (0) , . . . , S (ℓ) and which maps q (0) , . . . , q (ℓ−1) should we use in the latter protocol? FRI [BBHR18a]
does not suggest a canonical choice; each choice there works as well as any other. But BaseFold’s FRI subprotocol is not just a proximity test; it’s also a built-in multilinear evaluator (see Subsection 1.4). BaseFold PCS
relies on the fact whereby a FRI execution which begins on the Reed–Solomon encoding of (a0 , . . . , a2ℓ −1 )
will end on the constant polynomial whose value on S (ℓ) is identically
′
a0 + a1 · r0′ + a2 · r1′ + a3 · r0′ · r1′ + · · · + a2ℓ −1 · r0′ · · · · rℓ−1
,

(38)

′
where (r0′ , . . . , rℓ−1
) are the verifier’s FRI challenges. For maps q (0) , . . . , q (ℓ−1) generically chosen, this fact
will fail to hold.
We recover BaseFold PCS in characteristic 2 by introducing a specialization of binary FRI that works
compatibly with [LCH14]. That is, we introduce a particular choice of the maps q (0) , . . . , q (ℓ−1) which causes
the equality (38) to re-emerge. Interestingly, the right choice of q (0) , . . . , q (ℓ−1) turns out to be that for
which, for each i ∈ {0, . . . , ℓ}, the composition identity

ci = q (i−1) ◦ · · · ◦ q (0)
W
holds. That is, we choose our FRI folding maps q (0) , . . . , q (ℓ−1) so that they “factor” Lin, Chung and Han
[LCH14]’s subspace vanishing polynomials.
31

Oracle-skipping. Standard FRI [BBHR18a, § 3.2] supports arbitrary-arity folding, controlled by a folding
arity parameter η ≥ 1. The parameter η mediates a tradeoff between the number of oracles committed (which
grows like ηℓ ) and the size of each Merkle leaf (which grows like 2η ). The “sweet spot” tends to be around
η = 4, in practical deployments. The effect on proof size at stake—i.e., which one stands to induce, upon
changing η from 1 to something better—is significant (amounting to a halving at least, if not better). In
rough terms, FRI stipulates that, to fold a given oracle using the parameter η, the prover interpolate a
univariate polynomial of degree less than 2η on each coset of the relevant oracle, and finally evaluate the
resulting univariate polynomials collectively at the verifier’s challenge point.
BaseFold PCS does not support the use of univariate higher-arity folding. BaseFold, instantiated using
a FRI folding arity parameter η > 1, would break (38) in essentially two ways. For one, the number of
challenges ri′ available would become too few (something like ηℓ , as opposed to ℓ). Moreover, the relationship
between the list (a0 , . . . , a2ℓ −1 ) and the value of the final constant FRI oracle would become that of a multivariate evaluation—of individual degree less than 2η —over the challenges ri′ , as opposed to of a multilinear
one. For this reason, BaseFold as written remains unable to draw on the proof-size gains available at the
hands of higher-arity folding, which are significant. This fact is noted by WHIR [ACFY25, § 1.1].
We introduce a new, multilinear style of many-to-one FRI folding, different from FRI’s univariate approach [BBHR18a, § 3.2]. We describe our FRI folding variant in Subsection 4.2 below (see in particular
Definitions 4.6 and 4.8). We parameterize our method by a constant ϑ that plays a role analogous to η’s.
Informally, we stipulate that the verifier send ϑ folding challenges, and that the prover fold its oracle, again
coset-wise, using a length-2ϑ tensor combination of the verifier’s challenges over each coset. Equivalently, we
insist that the prover perform standard, 2-to-1 folding ϑ times in succession—consuming ϑ challenges in the
process, as opposed to 1—and commit only to the last among the oracles it obtains in this way. (For this
reason, we call our technique oracle-skipping.) Our folding technique makes necessary a sort of proximity
gap different from that invoked by the standard FRI protocol. Indeed, while the soundness proof [Ben+23,
§ 8.2] of FRI uses the proximity gap [Ben+23, Thm. 1.5] for low-degree parameterized curves, our security
treatment below uses a tensor-folding proximity gap of the sort recently established by Diamond and Posen
[DP24, Thm. 2]. In fact, we use a sharpening of that result due to Diamond and Gruen [DG25, Cor. 1].
Oracle-skipping—understood as a FRI variant, leave aside BaseFold—is simpler, easier to implement and
more sound than higher-arity univariate folding is; on the other hand, its proof of security depends on the
new work [DG25]. We refer to Diamond and Gruen [DG25, § 1.5] for more remarks on these matters.
Practical matters. We examine various further aspects of binary-field FRI.
We first opt to modify FRI itself, so as to induce a Lagrange-style, as opposed to a monomial-style, 2-to-1
folding pattern in the coefficient domain. In our FRI variant, the value of the prover’s final oracle becomes
rather
a0 · (1 − r0 ) · · · · · (1 − rℓ−1 ) + · · · + a2ℓ −1 · r0 · · · · rℓ−1 ,
the evaluation at (r0 , . . . , rℓ−1 ) of the polynomial whose coefficients in the multilinear Lagrange basis—as
opposed to in the multilinear monomial basis—are (a0 , . . . , a2ℓ −1 ). (We explain this further in Remark 4.7.)
Separately, even in the abstract IOP model, we must fix F2 -bases of the respective Reed–Solomon domains
S (i) , in order to interpret our committed functions f (i) : S (i) → L as L-valued strings. That is, we must
implicitly lexicographically flatten each domain S (i) , using some ordered F2 -basis of it, known to both the
prover and the verifier. The choice of these bases matters. Indeed, for F2 -bases of S (i) and S (i+1) chosen
−1
arbitrarily, the fundamental operation which associates to each y ∈ S (i+1) its fiber q (i) ({y}) ⊂ S (i) —which
both the prover and the verifier must perform repeatedly—could come to assume complexity on the order
2
of dim S (i) bit-operations, even after a linear-algebraic preprocessing phase.
We suggest a family of bases for the respective domains S (i) with respect to which the maps q (i) come
to act simply by projecting away their first coordinate. In particular, the application of each map q (i) —in
−1
coordinates—becomes free; the preimage operation q (i) ({y}) comes to amount simply to that of prepending
an arbitrary boolean coordinate to y’s coordinate representation. While bases with these properties can of
course be constructed in FRI even for maps q (i) chosen arbitrarily, our procedure moreover yields a basis
of the initial domain S (0) which coincides with that expected by the additive NTT [LCH14]. In particular,
our prover may use as is the output of the additive NTT as its 0th FRI oracle, without first subjecting that
output to the permutation induced by an appropriate change-of-basis transformation on S (0) .
32

4.1

Using FRI in Novel Polynomial Basis

We begin by proposing a specific construction of those subspace polynomials q (0) , . . . , q (ℓ−1) invoked internally
by FRI. Throughout this section, we fix a binary field L, with F2 -basis (β0 , . . . , βr−1 ). Throughout the
remainder of this subsection—and in fact, the entire paper—we impose the simplifying assumption whereby
β0 = 1. We fix moreover a size parameter ℓ ∈ {0, . . . , r − 1} and a rate parameter R ∈ {1, . . . , r − ℓ}. We
ci (X) ∈ L[X], for i ∈ {0, . . . , ℓ}, which we now view as
finally recall the subspace vanishing polynomials W
ci : L → L, as well as their non-normalized counterparts Wi : L → L (see Subsection 2.3).
F2 -linear maps W
We begin by defining our FRI domains and folding maps.
Definition 4.1. For each i ∈ {0, . . . , ℓ}, we define the domain
ci (⟨β0 , . . . , βℓ+R−1 ⟩).
S (i) := W
Moreover, for each i ∈ {0, . . . , ℓ − 1}, we define
2

q (i) (X) :=

Wi (βi )
· X · (X + 1).
Wi+1 (βi+1 )

For each i ∈ {0, . . . , ℓ − 1}, the map q (i) (X) is a linear subspace polynomial of degree 2. A priori, this
map’s kernel could relate arbitrarily to the domain S (i) ⊂ L; moreover, the image of its restriction
S (i)
 to(i+1)
(i+1)
(i)
(i)
could relate arbitrarily to S
. In the following sequence of results, we prove that in fact q S
=S
(0)
(ℓ−1)
(0)
holds for each i ∈ {0, . . . , ℓ − 1}. In particular, the chain of maps q , . . . , q
and the spaces S , . . . , S (ℓ)
yield a valid global parameterization of the FRI protocol (in the sense of Subsection 2.4).
ci = W
ci+1 of polynomials.
Lemma 4.2. For each i ∈ {0, . . . , ℓ − 1}, we have the equality q (i) ◦ W
Proof. We invoke the following direct calculation:



ci (X) =
q (i) ◦ W

2



Wi (βi )
ci (X) · W
ci (X) + 1
·W
Wi+1 (βi+1 )

(by definition of q (i) .)

2

Wi (X) Wi (X) + Wi (βi )
Wi (βi )
ci .)
·
·
(by definition of W
Wi+1 (βi+1 ) Wi (βi )
Wi (βi )
Wi (X) · (Wi (X) + Wi (βi ))
(cancellation of Wi (βi )2 .)
=
Wi+1 (βi+1 )
Wi+1 (X)
=
(recursive characterization of Wi+1 (X).)
Wi+1 (βi+1 )
ci+1 (X).
ci+1 (X).)
=W
(by definition of W
=

In the second-to-last step, we exploit the recursive identity Wi+1 (X) = Wi (X) · (Wi (X) + Wi (βi )), itself a
basic consequence of the definitions of Wi+1 and Wi and of the linearity of Wi .

Theorem 4.3. For each i ∈ {0, . . . , ℓ − 1}, q (i) S (i) = S (i+1) .
Proof. Using Lemma 4.2, we obtain:




ci (⟨β0 , . . . , βℓ+R−1 ⟩)
q (i) S (i) = q (i) W
ci+1 (⟨β0 , . . . , βℓ+R−1 ⟩)
=W
=S

(i+1)

(by definition of S (i) .)
(by Lemma 4.2.)
(again by definition of S (i+1) .)

.

This completes the proof.
In the following further corollary of Lemma 4.2, we argue that the polynomials q (0) , . . . , q (ℓ−1) collectively
c0 , . . . , W
cℓ , at least provided we assume β0 = 1.
“factor” the normalized subspace polynomials W
33

ci = q (i−1) ◦ · · · ◦ q (0) holds.
Corollary 4.4. For each i ∈ {0, . . . , ℓ}, W
c0 equals the empty composition (namely X itself). To
Proof. In the base case i = 0, we must show that W
show this, we recall first that W0 (X) = X. Moreover:
c0 (X) =
W

X
X
=
= X;
W0 (β0 )
β0

in the last step, we use our global assumption whereby β0 = 1.
ci = q (i−1) ◦ W
ci−1 . Applying this corollary inductively
For i ∈ {1, . . . , ℓ} arbitrary, Lemma 4.2 shows that W
(i−1)
c
to to Wi−1 , we conclude that this latter map in turn equals q
◦ q (i−2) ◦ · · · ◦ q (0) .
We note finally the following result.


ci (βi ), . . . , W
ci (βℓ+R−1 ) is an F2 -basis of the space S (i) .
Corollary 4.5. For each i ∈ {0, . . . , ℓ}, the set W
Proof. Indeed, the subspace Vi := ⟨βi , . . . , βℓ+R−1 ⟩ is clearly a subspace of ⟨β0 , . . . , βℓ+R−1 ⟩, so that in
ci (Vi ) ⊂ W
ci (⟨β0 , . . . , βℓ+R−1 ⟩), which itself equals S (i) (by Definition 4.1). On the other hand, the
turn W
ci to Vi is necessarily injective, since W
ci ’s kernel ⟨β0 , . . . , βi−1 ⟩ intersects Vi trivially. Since S (i)
restriction of W


ci (βi ), . . . , W
ci (βℓ+R−1 ) spans S (i) .
is ℓ + R − i-dimensional, we conclude by a dimension count that W
D
E
ci (βi ), . . . , W
ci (βℓ+R−1 ) = S (i) , for i ∈ {0, . . . , ℓ}, simplify various aspects of our protocol’s
The bases W
implementation. For example, expressed in coordinates with respect to these bases, each map q (i) : S (i) →
S (i+1) acts simply by projecting away its 0th -indexed component (indeed, for each i ∈ {0, . . . , ℓ−1}, q (i) maps
ci (βi ), . . . , W
ci (βℓ+R−1 )) to (0, W
ci+1 (βi+1 ), . . . , W
ci+1 (βℓ+R−1 ))). Similarly, for each i ∈ {0, . . . , ℓ − 1} and
(W
(i+1)
(i)
each y ∈ S
, the two L-elements x ∈ S for which q (i) (x) = y differ precisely at their 0th components,
and elsewhere agree with y’s coordinate representation. Below, we often identify S (i) ∼
= Bℓ+R−i as sets, using
these bases; moreover, where possible, we eliminate altogether the maps q (0) , . . . , q (ℓ−1) from our descriptions.
These measures make our protocol’s description and implementation more transparent.

4.2

FRI Folding, Revisited

We now introduce a new FRI-like folding mechanism. Below, we again write L for a binary field.
Definition 4.6. We fix an index i ∈ {0, . . . , ℓ − 1} and a map f (i) : S (i) → L. For each r ∈ L, we define the
map fold f (i) , r : S (i+1) → L by setting, for each y ∈ S (i+1) :

 

(i)


h
i
x
−x
f
(x
)
1
0
0
·
,
fold f (i) , r : y 7→ 1 − r r · 
(i)
−1
1
f (x1 )
where we write (x0 , x1 ) := q (i)

−1

({y}) for the fiber of q (i) over y ∈ S (i+1) .

Remark 4.7. Definition 4.6’s quantity fold f (i) , r (y) is similar—and yet not equivalent—to FRI’s

interpolant f (i) q(i) −1 ({y}) (r). (FRI’s variant, on the other hand, admits a similar matrix-based charac-

terization.) The essential point is that FRI’s variant induces a monomial fold, as opposed to a Lagrange
fold, in the coefficient domain. If we used FRI’s variant instead of our own, then something like our Lemma
P2ℓ−i−1 −1
(i+1)
4.14 would stay true, albeit rather with the conclusion P (i+1) (X) = j=0
(a2j + ri′ · a2j+1 ) · Xj
(X).
Our entire theory can be carried out in that “parallel” setting, though further complications arise there.
We finally record the following iterated extension of Definition 4.6.
Definition 4.8. We fix a positive folding factor ϑ, an index i ∈ {0, . . ., ℓ − ϑ}, and a map f (i) : S (i) → L.

For each tuple (r0 , . . . , rϑ−1 ) ∈ Lϑ , we abbreviate fold f (i) , r0 , . . . , rϑ−1 := fold · · · fold f (i) , r0 , · · · , rϑ−1 .
We have the following mathematical characterization of this iterated folding operation:
34

Lemma 4.9. For each positive folding factor ϑ, each index i ∈ {0, . . . , ℓ − ϑ}, and each y ∈ S (i+ϑ) , there is
a 2ϑ × 2ϑ invertible matrix My ,which depends only on y ∈ S (i+ϑ) , such that, for each function f (i) : S (i) → L
and each tuple (r0 . . . , rϑ−1 ) ∈ Lϑ of folding challenges, we have the matrix identity:

 

f (i) (x0 )

 


h
i 
Nϑ−1

 

..
·
fold f (i) , r0 , . . . , rϑ−1 (y) =
·
,



M
(1
−
r
,
r
)
.
y
j j
j=0

 

(i)
f (x2ϑ −1 )
−1
where the right-hand vector’s values (x0 , . . . , x2ϑ −1 ) represent the fiber q (i+ϑ−1) ◦ · · · ◦ q (i)
({y}) ⊂ S (i) .
Proof. We prove the result by induction on ϑ. In the
 base caseϑ = 1, the claim is a tautology, in view of

x1 −x0
 is invertible, since its determinant x1 − x0
Definition 4.6. We note that that definition’s matrix 
−1
1
is nonzero (and in fact equals 1, a fact we shall use below).
We thus fix a folding factor ϑ > 1, and suppose that the claim holds for ϑ − 1. We write (z0 , z1 ) :=
−1
(i+ϑ−1) −1
({y}), as well as (x0 , . . . , x2ϑ −1 ) := q (i+ϑ−1) ◦ · · · ◦ q (i)
({y}). Unwinding Definition 4.8, we
q

recursively express the relevant quantity fold f (i) , r0 , . . . , rϑ−1 (y)—which, for typographical reasons, we call
f—in the following way:
# "
#
"

h
i z
−z0
fold f (i) , r0 , . . . , rϑ−2 (z0 )
1
·
f = 1 − rϑ−1 rϑ−1 ·

(i)
−1

fold f

1

, r0 , . . . , rϑ−2 (z1 )


h

= 1 − rϑ−1

i 
z1
rϑ−1 · 

−1

|

−z0
1

 
·



Nϑ−2

j=0 (1 − rj , rj )

Nϑ−2

j=0 (1 − rj , rj )

{z

 Mz

0

 ·




Mz

f (i) (x0 )



 
 
·
 

..
.



.


f (i) (x2ϑ −1 )

1

}

these matrices may be interchanged.

 

In the second step above, we apply the inductive hypothesis on both z0 and z1 . That hypothesis furnishes the nonsingular, 2ϑ−1 × 2ϑ−1 matrices Mz0 and Mz1 ; we note moreover that the union of the fibers
−1
−1
−1
q (i+ϑ−2) ◦ · · · ◦ q (i)
({z0 }) and q (i+ϑ−2) ◦ · · · ◦ q (i)
({z1 }) is precisely q (i+ϑ−1) ◦ · · · ◦ q (i)
({y}). Interchanging the two matrices bracketed above, we further reexpress this quantity as:

 
 

(i) (x )
f
0
" N
#
 diag(z ) diag(−z )   Mz
 

h
i
ϑ−2

 
 

.
j=0 (1 − rj , rj )
.
= 1 − rϑ−1 rϑ−1 ·
·
·
·





.
Nϑ−2
.






(1
−
r
,
r
)
j
j
j=0
1

diag(−1)

0

diag(1)

0

Mz

1

f (i) (x2ϑ −1 )

By the standard
recursive substructure of the tensor product, the product of the left-hand two matrices
Nϑ−1
equals exactly j=0 (1 − rj , rj ). On the other hand, the product of the two 2ϑ × 2ϑ nonsingular matrices
above is itself nonsingular, and supplies the required 2ϑ × 2ϑ matrix My .
We emphasize that, in Lemma 4.9, the matrix My depends only on y ∈ S (i+ϑ) —and of course on ϑ and
i ∈ {0, . . . , ℓ − ϑ}—and not on the map f (i) or the folding challenges (r0 , . . . , rϑ−1 ) ∈ Lϑ .
Remark 4.10. Interestingly, the matrix My of Lemma 4.9 is nothing other than that of the inverse additive
NTT [LCH14, § III. C.] on the coset (x0 , . . . , x2ϑ −1 ); i.e., it’s the matrix which, on input the evaluations of
some polynomial of degree less than 2ϑ on the set of elements (x0 , . . . , x2ϑ −1 ), returns the coefficients—with
respect to the ith -order novel basis (see Remark 4.16 below)—of that polynomial.
Remark 4.11. On input some map f (i) : S (i) → L—expressed as a list of values let’s say, via the identification S (i) ∼
= Bℓ+R−i discussed above—and a tuple (r0 , . . . , rϑ−1 ) of folding challenges, one can efficiently
compute the table of values of fold f (i) , r0 , . . . , rϑ−1 : S (i+ϑ) → L. Indeed, Definitions 4.6 and 4.8 directly
suggest a ϑ-pass, Θ(|S (i) |)-time algorithm for this task. Lemma 4.9 is not interesting algorithmically, but
mathematically; it appears in our security proof below (see Theorem 4.17, and in particular Proposition
4.21).
35

4.3

Our Large-Field IOPCS

We now present our binary adaptation of BaseFold’s IOPCS [ZCF24, § 5], itself based on the material of our
Subsections 4.1 and 4.2 above. In order to present a notationally simpler variant of our protocol, we assume
below that ϑ | ℓ; this requirement is not necessary.
CONSTRUCTION 4.12 (Binary BaseFold IOPCS).
We define Π = (Setup, Commit, P, V) as follows.
1. params ← Π.Setup(1λ , ℓ). On input 1λ and ℓ, choose a constant, positive rate parameter R ∈ N
and a binary field L/F2 whose degree r (say) over F2 satisfies r = ω(log λ) and r ≥ ℓ+R. Initialize
L
the vector oracle FVec
. Fix a folding factor ϑ | ℓ and a repetition parameter γ = ω(log λ). Fix
an arbitrary F2 -basis (β0 , . . . , βr−1 ) of L. Write (X0 (X), . . . , X2ℓ −1 (X)) for the resulting novel
ℓ
L-basis of L[X]≺2 , and fix the domains S (0) , . . . , S (ℓ) and the polynomials q (0) , . . . , q (ℓ−1) as in
ℓ+R
Subsection 4.1. Write C (0) ⊂ L2
for the Reed–Solomon code RSL,S (0) [2ℓ+R , 2ℓ ].
2. [f ] ← Π.Commit(params, t). On input t(X0 , . . . , Xℓ−1 ) ∈ L[X0 , . . . , Xℓ−1 ]⪯1 , use t’s Lagrange
coefficients
P(t(w))w∈Bℓ as the coefficients, in the novel polynomial basis, of a univariate polynomial
P (X) := w∈Bℓ t(w) · X{w} (X), say. Using Algorithm 2, compute the Reed–Solomon codeword
L
f : S (0) → L defined by f : x 7→ P (x). Submit (submit, ℓ + R, f ) to the vector oracle FVec
. Upon
L
receiving (receipt, ℓ + R, [f ]) from FVec , output the handle [f ].
We define (P, V) as the following IOP, in which both parties have the common input [f ], s ∈ L, and
(r0 , . . . , rℓ−1 ) ∈ Lℓ , and P has the further input t(X0 , . . . , Xℓ−1 ) ∈ L[X0 , . . . , Xℓ−1 ]⪯1 .
1. P writes h(X0 , . . . , Xℓ−1 ) := e
fq(r0 , . . . , rℓ−1 , X0 , . . . , Xℓ−1 ) · t(X0 , . . . , Xℓ−1 ).
2. P and V both abbreviate f (0) := f and s0 := s, and execute the following loop:
1: for i ∈ {0, . . . , ℓ − 1} do
P
′
2:
P sends V the polynomial hi (X) := w∈Bℓ−i−1 h(r0′ , . . . , ri−1
, X, w0 , . . . , wℓ−i−2 ).
3:
4:
5:
6:

?

′
′
V requires si = hi (0) + hi (1). V samples ri′ ← L, sets si+1
 := hi (ri ), and sends P ri .
(i+1)
(i+1)
(i) ′
P defines f
:S
→ L as the function fold f , ri of Definition 4.6.
if i + 1 = ℓ then P sends c := f (ℓ) (0, . . . , 0) to V.
L
else if ϑ | i + 1 then P submits (submit, ℓ + R − i − 1, f (i+1) ) to the oracle FVec
.
?

′
3. V requires sℓ = e
fq(r0 , . . . , rℓ−1 , r0′ , . . . , rℓ−1
) · c.

4. V carries out the following FRI-querying procedure:
1: for γ repetitions do
2:
V samples v ← Bℓ+R randomly.
3:
for i ∈ {0, ϑ, . . . , ℓ − ϑ} (i.e., taking ϑ-sized steps) do

4:
for each u ∈ Bϑ , V sends query, [f (i) ], (u0 , . . . , uϑ−1 , vi+ϑ , . . . , vℓ+R−1 ) to the oracle.
5:
6:
7:

?

if i > 0 then V requires ci = f (i) (vi , . . . , vℓ+R−1 ).

′
V defines ci+ϑ := fold f (i) , ri′ , . . . , ri+ϑ−1
(vi+ϑ , . . . , vℓ+R−1 ).
?

V requires cℓ = c.

In our commitment procedure above, we make sense of the prover’s commitment of f by identify∼ Bℓ+R as sets (as we discussed above); similarly, in the prover’s line 6 above, we identify
ing S (0) =
Bℓ+R−i−1 ∼
= S (i+1) . Conversely, in its lines 4 and 6 above, the verifier must identify the Bℓ+R−i -elements
(u0 , . . . , uϑ−1 , vi+ϑ , . . . , vℓ+R−1 )u∈Bϑ with S (i) -elements—and the Bℓ+R−i−ϑ -element (vi+ϑ , . . . , vℓ+R−1 )
with an S (i+ϑ) -element—in order to apply Definition 4.8.
We note that, in line 6, V has precisely the

′
information it needs to compute fold f (i) , ri′ , . . . , ri+ϑ−1
(vi+ϑ , . . . , vℓ+R−1 ) (namely, the values of f (i) on
−1
∼
the fiber (u0 , . . . , uϑ−1 , vi+ϑ , . . . , vℓ+R−1 )
({(vi+ϑ , . . . , vℓ+R−1 )})).
= q (i+ϑ−1) ◦ · · · ◦ q (i)
u∈Bϑ

36

The completness of Construction 4.12’s evaluation IOP is not straightforward. For instance, it is simply
not obvious what the folding operation of line 4 does to the coefficients of the low-degree polynomial P (i) (X)
underlying f (i) . (Though our folding operation departs slightly from FRI’s—we refer to Remark 4.7 for a
discussion of this fact—the conceptual obstacle is essentially the same.) Indeed, the completeness proof of
generic FRI [BBHR18a, § 4.1.1] establishes that the folded function f (i+1) captures the evaluations of some
polynomial P (i+1) (X) of appropriate degree on the domain S (i+1) . But which one? The proof of [BBHR18a,
§ 4.1.1] fails to constructively answer this question, in that it invokes the generic characteristics of the
multivariate reduction—called Q(i) (X, Y )—of P (i) (X) by Y − q (i) (X). (We refer to e.g. von zur Gathen and
Gerhard [GG13, Alg. 21.11] for a thorough treatment of multivariate division.) It seems infeasible to analyze
by hand the execution of the multivariate division algorithm with sufficient fidelity as to determine with any
precision the result P (i+1) (Y ) = Q(i) (ri′ , Y ) (though we don’t rule out the prospect whereby a proof could
in principle be achieved in this way).
ℓ−i
Instead, we introduce new, carefully-selected L-bases of the spaces L[X]≺2 , for i ∈ {0, . . . , ℓ} (the
so-called “higher-order” novel polynomial bases). As it turns out, the respective coefficients of P (i) (X) and
P (i+1) (X) with respect to these bases bear a relationship more disposed to yield to our scrutiny (i.e., just
that which results from folding by ri′ ). Proceeding by induction, we obtain the characterization of c we need.
Theorem 4.13. The IOPCS Π = (Setup, Commit, P, V) of Construction 4.12 is complete.
P
Proof. Provided that P is honest,
Ps = t(r0 , . . . , rℓ−1 ) will hold. Since t(r0 . . . , rℓ−1 ) = w∈Bℓ h(w), this
guarantee implies that s = s0 = w∈Bℓ h(w) will hold, so that, by the completeness of the sumcheck, V’s
?

′
′
′
checks si = hi (0)+hi (1) will pass. Finally, sℓ = h(r0′ , . . . , rℓ−1
)=e
fq(r0 , . . . , rℓ−1 , r0′ , . . . , rl−1
)·t(r0′ , . . . , rℓ−1
)
?

′
too will hold. To argue the completeness of V’s check sℓ = e
fq(r0 , . . . , rℓ−1 , r0′ , . . . , rℓ−1
) · c above, it thus
′
′
suffices to argue that, for P honest, c = t(r0 , . . . , rℓ−1 ) will hold.
We introduce a family of further polynomial bases. For each i ∈ {0, . . . , ℓ − 1}, we define the ith -order
(i) (i+1)
c (i) , . . . , W
c (i)
subspace vanishing polynomials W
◦ q (i) , . . . , q (ℓ−2) ◦ · · · ◦ q (i) ,
0
ℓ−i−1 as the polynomials X, q , q
(i)
(i+k−1)
(i)
c := q
respectively (that is, W
◦ · · · ◦ q , for each k ∈ {0, . . . , ℓ − i − 1}). Finally, we define the
k
Qℓ−i−1 c (i) jk
(i)
ith -order novel polynomial basis by setting X :=
W
, for each j ∈ {0, . . . , 2ℓ−i − 1} (here, again,
j

k=0

k

we write (j0 , . . . , jℓ−i−1 ) for the bits of j). We adopt the notational convention whereby the ℓth -order basis
(ℓ)
consists simply of the constant polynomial X0 (X) = 1. Below, we use a certain inductive relationship

2ℓ−i −1

2ℓ−i−1 −1
(i)
(i+1)
between the bases Xj (X)
and Xj
(X)
; that is, for each j ∈ {0, . . . , 2ℓ−i−1 − 1}, the
j=0
j=0


(i)
(i)
(i+1) (i)
(i+1) (i)
polynomials X2j (X) and X2j+1 (X) respectively equal Xj
q (X) and X · Xj
q (X) .
Lemma 4.14. Fix an index i ∈ {0, . . . , ℓ − 1}. If f (i) : S (i) → L is exactly the evaluation over S (i) of the
P2ℓ−i −1
(i)
polynomial P (i) (X) = j=0 aj ·Xj (X), then, under honest prover behavior, f (i+1) : S (i+1) → L is exactly
P2ℓ−i−1 −1
(i+1)
the evaluation over S (i+1) of the polynomial P (i+1) (X) = j=0
((1 − ri′ ) · a2j + ri′ · a2j+1 ) · Xj
(X).
Proof. Given P (i) (X) as in the hypothesis of the lemma, we introduce the even and odd refinements
P2ℓ−i−1 −1
P2ℓ−i−1 −1
(i+1)
(i+1)
(i+1)
(i+1)
(X) :=
a2j+1 · Xj
(X) of P (i) (X). We
P0
(X) :=
a2j · Xj
(X) and P1
j=0
j=0
note the following key polynomial identity:
(i+1)

P (i) (X) = P0

(i+1)

(q (i) (X)) + X · P1

(q (i) (X));

(39)

This identity is a direct consequence of the definitions of the higher-order novel polynomial bases.
We turn to the proof of the lemma. We claim that f (i+1) (y) = P (i+1) (y) holds for each y ∈ S (i+1) , where
(i+1)
P
(X) is as in the lemma’s hypothesis. To this end, we let y ∈ S (i+1) be arbitrary; we moreover write
−1
(x0 , x1 ) := q (i) ({y}) for the fiber of q (i) over y. We begin by examining the values P (i) (x0 ) and P (i) (x1 ).
For each b ∈ {0, 1} we have:




(i+1)
(i+1)
P (i) (xb ) = P0
q (i) (xb ) + xb · P1
q (i) (xb )
(by the identity (39).)
(i+1)

= P0

(i+1)

(y) + xb · P1

(using q (i) (xb ) = y.)

(y).
37

Using now our assumption whereby f (i) (xb ) = P (i) (xb ) for each b ∈ {0, 1}, and unwinding the prescription
of Definition 4.6, we obtain:

 

(i)
i
h
x
−x
P
(x
)
1
0
0
·

f (i+1) (y) = 1 − ri′ ri′ · 
(by our hypothesis on f (i) , and by Definition 4.6.)
(i)
−1
1
P (x1 )

 
 

(i+1)
i
h
x1 −x0
1 x0
P0
(y)
·
·
 (by the calculation just performed above.)
= 1 − ri′ ri′ · 
(i+1)
−1
1
1 x1
P1
(y)


i P (i+1) (y)
h

= 1 − ri′ ri′ ·  0
(cancellation of inverse matrices.)
(i+1)
P1
(y)
= P (i+1) (y).

(i+1)

(by the definitions of P0


(i+1)

(X), P1

(X), and P (i+1) (X).)





x1 −x0
1 x0
 and 
 are inverses; there,
To achieve the third equality above, we note that the matrices 
−1
1
1 x1

we use the guarantee x1 − x0 = 1, a basic consequence of Definition 4.1 (or rather of ker q (i) = {0, 1}).

ℓ−1

2ℓ −1
(0)
c (0)
Applying Corollary 4.4, we note finally that W
and
X
themselves equal precisely the
j
k
k=0

j=0

standard subspace vanishing and novel basis polynomials, respectively. It follows that in the base case
i = 0 of Lemma 4.14—and again assuming honest behavior by the prover—we have that f (0) will equal
P
(0)
the evaluation over S (0) of P (0) (X) := P (X) =
w∈Bℓ t(w) · X{w} (X). Applying Lemma 4.14 repeatedly,
that f (ℓ) will equal the evaluation over S (ℓ) of the constant polynomial
P we conclude by induction
′
′
′
′
fq(w0 , . . . , wℓ−1 , r0 , . . . , rℓ−1 ) · t(w) = t(r0′ , . . . , rℓ−1
), so that c = t(r0′ , . . . , rℓ−1
) will hold, as desired.
w∈Bℓ e
The completeness of the verifier’s query phase is self-evident (and is just as in [BBHR18a, § 4.1.1]); we
note that V applies to each oracle f (i) the same folding procedure that P does. This completes the proof of
completeness.
Remark 4.15. Using the techniques of Subsection 4.1 and of Theorem 4.13 above, we are able to suggest a
new explanation of the additive NTT algorithm of Lin, Chung and Han [LCH14, § III.], and of its correctness;
we note also our Algorithm 2 above. (Li et al. [Li+18, Alg. 2] present yet a further—and very interesting—
perspective, which differs both from ours and from Lin–Chung–Han’s.) We fix an index i ∈ {0, . . . , ℓ−1} and
P2ℓ−i −1
(i)
a polynomial P (i) (X) := j=0 aj · Xj (X) expressed with respect to the ith -order novel basis. The key

idea is that the values of P (i) (X) on the domain S (i) can be derived—using only O 2ℓ+R−i L-operations—
(i+1)
(i+1)
given the values of P (i) (X)’s even and odd refinements P0
(X) and P1
(X) (as in the proof of Lemma
(i+1)
4.14) over the domain S
. This is a direct consequence of the identity (39) above. Indeed, applying that
−1
identity, we see that, for y ∈ S (i+1) arbitrary, with fiber (x0 , x1 ) := q (i) ({y}), say, we have the equalities
(i+1)
(i+1)
(i+1)
(i+1)
(y). Since x0 and x1 in fact
P (i) (x0 ) := P0
(y) + x0 · P1
(y) and P (i) (x1 ) := P0
(y) + x1 · P1
(i)
(i)
differ by exactly 1, we see that P (x1 ) can be computed from P (x0 ) using a single further L-addition.
We recover the key butterfly diagram of [LCH14, Fig. 1. (a)] (see also Algorithm 2 above) upon carrying
out this procedure recursively, with the convention whereby we flatten (using the space’s canonical basis)
and interleave the two copies of S (i+1) at each instance. The base case of the recursion consists of the
2ℓ -fold interleaving of the domain S (ℓ) , into which P (0) ’s coefficients are tiled 2R times. The final stage of
the butterfly diagram yields the desired evaluation of P (0) (X) on S (0) . Algorithm 2’s twiddle factors in its
ith stage, then, are nothing other than the respective first lifts x0 of y, as the image y = q (i) (x0 ) varies
Pℓ+R−i−2
ci (βi+1+k ), for
throughout S (i+1) . These latter elements x0 , in turn, take precisely the form k=0
uk · W
(i+1)
th
∼
ci (βi ) = 1
u ∈ Bℓ+R−i−1 = S
arbitrary; indeed, we suppress throughout the 0 canonical basis element W
(i)
of S , since that element is subsumed into each butterfly. We find it interesting that the same polynomial
identity underlies both the correctness of [LCH14, § III.] and our above analysis of FRI’s folding.

38

Remark 4.16. Though it seems inessential to the proof of Theorem 4.13, it is interesting to note that, for

2ℓ−i −1
(i)
each i ∈ {0, . . . , ℓ − 1}, the ith -order basis Xj
is itself a novel polynomial basis in its own right,
j=0


ci (βi ), . . . , W
ci (βℓ−1 ) . Equivalently, the ith -order subspace
namely that attached to the set of vectors W

ℓ−i−1
c (i)
vanishing polynomials W
are simply the subspace vanishing polynomials attached to this latter
k
k=0
D
E


ci (βi ), . . . , W
ci (βi+k−1 ) ⊂ ker W
c (i) certainly
set of vectors. Indeed, for each k ∈ {0, . . . , ℓ − i − 1}, W
k
c (i) ◦ W
ci = q (i+k−1) ◦ · · · ◦ q (i) ◦ W
ci = W
ci+k , which annihilates ⟨β0 , . . . , βi+k−1 ⟩ (here, we use
holds, since W
k
(i)
c
c (i) = q (i+k−1) ◦ · · · ◦ q (i) ’s kernel can be of
the definition of Wk and Lemma 4.2). On the other hand, W
k
ci (βi ), . . . , W
ci (βi+k−1 ) are linearly
dimension at most k (say by degree considerations), while the vectors W
independent (a consequence of Corollary 4.5). We conclude that the above containment is an equality.

ℓ−i−1
c (i)
are normalized. Indeed, using Lemma 4.2 again, we see that,
Finally, the subspace polynomials W
k
k=0 



(i)
c
ci (βi+k ) = q (i+k−1) ◦ · · · ◦ q (i) ◦ W
ci (βi+k ) = W
ci+k (βi+k ) = 1 holds.
for each k ∈ {0, . . . , ℓ − i − 1}, W
W
k
We now prove the security of Construction 4.12. Our key technical results below (see Propositions 4.21
and 4.24), essentially, jointly constitute a variant of FRI’s soundness statement [BBHR18a, § 4.2.2]. Our
proofs of these results incorporate—albeit in an attenuated way—various ideas present in [BBHR18a, § 4.2.2]
and [Ben+23, § 8.2]. We also introduce a number of new ideas, which, by and large, pertain to our new
folding technique (see Subsection 4.2).
We note that our protocol seems not to admit a security proof which invokes that of FRI in a strictly
blackbox manner. Rather, our security argument—and, it would seem, any conceivable analysis of Construction 4.12—must inevitably concern itself not merely with the distance from the code of A’s initial committed
word, but moreover with the consistency of its oracles, and in particular with whether its final oracle value
c relates as it should to its initial oracle.
Theorem 4.17. The IOPCS Π = (Setup, Commit, P, V) of Construction 4.12 is secure.
Proof. We define a straight-line emulator E as follows.
1. By inspecting A’s messages to the vector oracle, E immediately recovers the function f : S (0) → L
underlying the handle [f ] output by A.
2. E runs the Berlekamp–Welch decoder (i.e., Algorithm 1) on the word f : S (0) → L. If that algorithm
outputs P (X) = ⊥, then E outputs ⊥ and aborts.
P
3. Otherwise, E re-expresses the Berlekamp–Welch output polymomial P (X) = w∈Bℓ tw · X{w} (X) in
coordinates with respect to the novel polynomial basis. E writes t(X0 , . . . , Xℓ−1 ) ∈ L[X0 , . . . , Xℓ−1 ]⪯1
for the multilinear whose Lagrange coordinates are (tw )w∈Bℓ . E outputs t(X0 , . . . , Xℓ−1 ) and halts.
We begin by defining various notions, adapting [BBHR18a, § 4.2.1]. For each i ∈ {0, ϑ, . . . , ℓ} (i.e.,
ℓ+R−i
ascending in ϑ-sized steps), we write C (i) ⊂ L2
for the Reed–Solomon code RSL,S (i) [2ℓ+R−i , 2ℓ−i ]. We
(i)
ℓ+R−i
ℓ−i
recall that C is of distance di := 2
−2 +1. We write f (0) , f (ϑ) , . . . , f (ℓ−ϑ) for the oracles committed
(ℓ)
(ℓ)
by A; we moreover write f : S → L for the identically-c
 function (here, c ∈ L is A’s final FRI message).
For each i ∈ {0, ϑ, . . . , ℓ − ϑ}, we write ∆ f (i+ϑ) , g (i+ϑ) ⊂ S (i+ϑ) for the disagreement set between the

ℓ+R−i−ϑ
elements f (i+ϑ) and g (i+ϑ) of L2
; that is, ∆ f (i+ϑ) , g (i+ϑ) is the set of elements y ∈ S (i+ϑ) for which

f (i+ϑ) (y) ̸= g (i+ϑ) (y). We moreover write ∆(i) f (i) , g (i) ⊂ S (i+ϑ) for the fiber-wise disagreement set of the

ℓ+R−i
. That is, ∆(i) f (i) , g (i) ⊂ S (i+ϑ) denotes the set of elements y ∈ S (i+ϑ)
elements f (i) and g (i) of L2
−1
({y}) ⊂ S (i) are not
for which the respective restrictions of f (i) and g (i) to the fiber q (i+ϑ−1) ◦ · · · ◦ q (i)



(i)
(i) (i)
(i)
(i)
(i) :=
ming(i) ∈C (i) ∆ f , g
. We note that, if d(i) f (i) , C (i) <
identically equal. We define d f , C

di+ϑ
d f (i) , Ck(i) < d2i a fortiori holds. (Each offending fiber contributes at most 2ϑ errors; on the other
2 , then j



hand, 2ϑ · di+ϑ2 −1 ≤ di2−1 .) In any case, in case the oracle f (i) : S (i) → L is such that d f (i) , L(i) < d2i

happens to hold, we write f (i) ∈ L(i) for the unique codeword for which d f (i) , f (i) < d2i .
We record the following key compliance condition:
39

Definition 4.18. For each index i ∈ {0, ϑ, . . . , ℓ − ϑ}, we say that A’s ith oracle f (i) is compliant if the



(i+ϑ)
′
conditions d(i) f (i) , C (i) < d2i , d f (i+ϑ) , C (i+ϑ) < di+ϑ
= fold f (i) , ri′ , . . . , ri+ϑ−1
all hold.
2 , and f
We first argue that if any among A’s oracles i ∈ {0, ϑ, . . . , ℓ − ϑ} is not compliant, then V will accept
with negligible probability at most. This is exactly Proposition 4.24 below. In order to prepare for that
proposition, we record a sequence of lemmas. We begin with the following elementary fact.

Lemma 4.19. For each i ∈ {0, ϑ, . . . , ℓ − ϑ}, if d f (i) , C (i) < d2i , then, for each tuple of folding challenges



′
′
′
, fold f (i) , ri′ , . . . , ri+ϑ−1
) ∈ Lϑ , we have that ∆ fold f (i) , ri′ , . . . , ri+ϑ−1
(ri′ , . . . , ri+ϑ−1
⊂ ∆(i) f (i) , f (i) .

Proof. We proceed by contraposition; we fix an element y ̸∈ ∆(i) f (i) , f (i) . By definition of that latter set,
we conclude immediately that the restrictions f (i) (q(i+ϑ−1) ◦···◦q(i) )−1 ({y}) = f (i) (q(i+ϑ−1) ◦···◦q(i) )−1 ({y}) are
identically equal. Applying Definition 4.8, we see under this guarantee
that, regardless of the challenges

′
′
′
(y) = fold f (i) , ri′ , . . . , ri+ϑ−1
(y) necessarily also holds.
), fold f (i) , ri′ , . . . , ri+ϑ−1
(ri′ , . . . , ri+ϑ−1
We now define a sequence of bad folding events. Our definition of Ei is case-based, and depends on the
status of f (i) . If f (i) is within the (fiber-wise) unique decoding radius, then Ei captures the event whereby the
generic inclusion of Lemma 4.19 becomes strict. Otherwise, Ei captures the “bad batching” event whereby
′
fold(f (i) , ri′ , . . . , ri+ϑ−1
) becomes close to C (i+ϑ) .
Definition 4.20. For each i ∈ {0, ϑ, . . . , ℓ − ϑ}, we define the bad subset Ei ⊂ Lϑ as the set of tuples
′
(ri′ , . . . , ri+ϑ−1
) ∈ Lϑ for which, as the case may be:




d
′
′
, fold f (i) , ri′ , . . . , ri+ϑ−1
.
: ∆(i) f (i) , f (i) ̸⊂ ∆ fold f (i) , ri′ , . . . , ri+ϑ−1
in case d(i) f (i) , C (i) < i+ϑ
2



d
′
in case d(i) f (i) , C (i) ≥ i+ϑ
: d fold f (i) , ri′ , . . . , ri+ϑ−1
, C (i+ϑ) < di+ϑ
2 .
2
|Ei |
We now bound the bad subsets Ei of Definition 4.20. We recall that µ(Ei ) := |L|
ϑ denotes the probability
ϑ
mass of the set Ei ⊂ L .

Proposition 4.21. For each i ∈ {0, ϑ, . . . , ℓ − ϑ}, µ(Ei ) ≤ ϑ ·

|S (i+ϑ) |
|L|

holds.

Proof. We treat separately the two cases of Definition 4.20.

We begin with the first case. We fix an element y ∈ ∆(i) f (i) , f (i) , we moreover write Eiy ⊂ Lϑ for


′
′
′
the set of tuples (ri′ , . . . , ri+ϑ−1
) ∈ Lϑ for which y ̸∈ ∆ fold f (i) , ri′ , . . . , ri+ϑ−1
, fold f (i) , ri′ , . . . , ri+ϑ−1
.
ϑ
. This latter claim suffices to complete the proof of the first case; indeed, since
We argue that µ(Eiy ) ≤ |L|
 ϑ
S
ϑ
Ei = y∈∆(i) (f (i) ,f (i) ) Eiy , assuming the claim, we conclude that µ(Ei ) ≤ ∆(i) f (i) , f (i) · |L|
≤ |S (i+ϑ) | · |L|
.

(i)
(i)
(i)
(i)
(i)
For y ∈ ∆ f , f
chosen as above, we apply Lemma 4.9 to the words f and f . Applying that
′
lemma, we see that (ri′ , . . . , ri+ϑ−1
) ∈ Eiy holds if and only if we have the following matrix identity:

0=

h

Nϑ−1

′
′
j=0 (1 − ri+j , ri+j )

i 

·


 
My

f (i) (x0 ) − f (i) (x0 )
..
.




 

 
·
,

 
f (i) (x2ϑ −1 ) − f (i) (x2ϑ −1 )

(40)

−1

where we again write (x0 , . . . , x2ϑ −1 ) := q (i+ϑ−1) ◦ · · · ◦ q (i)
({y}). Our hypothesis y ∈ ∆(i) f (i) , f (i)
entails precisely that the right-hand vector of (40) is not identically zero. By Lemma 4.9, My is nonsingular; we conclude that the image of the right-hand vector of (40) under My is likewise not identically zero. Writing (a0 , . . . , a2ϑ −1 ) for this latter vector—which, we repeat, is not zero—we conclude
y
ϑ
ϑ
that
) :=
P Ei ⊂ L is precisely the vanishing locus in L of the ϑ-variate polynomial s(X0 , . . . , Xϑ−1
ϑ
a
·
e
f
q(X
,
.
.
.
,
X
,
v
,
.
.
.
,
v
)
over
L.
Since
s(X
,
.
.
.
,
X
)’s
values
on
the
cube
{0,
1}
⊂
Lϑ
0
ϑ−1
0
ϑ−1
0
ϑ−1
{v}
v∈Bϑ
are exactly (a0 , . . . , a2ϑ −1 ), s(X0 , . . . , Xϑ−1 ) is certainly not zero. Applying the Schwartz–Zippel lemma to
ϑ
s(X0 , . . . , Xϑ−1 ), we conclude that the relevant locus Eiy ⊂ Lϑ is of mass at most µ(Eiy ) ≤ |L|
, as required.

40


We turn to the second case of Definition 4.20; in particular, we assume that d(i) f (i) , C (i) ≥ di+ϑ
2 . We

2ϑ −1
(i+ϑ)
define an interleaved word fj
—i.e., a 2ϑ × 2ℓ+R−i−ϑ matrix, with entries in L—in the following
j=0

way. For each y ∈ S (i+ϑ) , writing My for the matrix guaranteed to exist by Lemma 4.9, we define the
column:

 



(i+ϑ)
f (i) (x0 )
f0
(y)

 




 



..
..
(41)
.
·
 := 

My
.
.

 



(i+ϑ)
f (i) (x2ϑ −1 )
f2ϑ −1 (y)

2ϑ −1
(i+ϑ)
We note that the resulting 2ϑ × 2ℓ+R−i−ϑ matrix fj
—i.e., that whose columns are given by the
j=0

′
respective left-hand sides of (41), for y ∈ S (i+ϑ) varying—satisfies, for each (ri′ , . . . , ri+ϑ−1
) ∈ Lϑ ,


(i+ϑ)
f
0







 h
i
Ni+ϑ−1


(i) ′
′
.
′
′
fold f , ri , . . . , ri+ϑ−1 =
·
.

..
(1 − rj , rj )
j=i






(i+ϑ)
f2ϑ −1

(42)

Indeed, this is essentially the content of Lemma 4.9, which we apply here jointly to all elements y ∈ S (i+ϑ) .

2ϑ −1
(i+ϑ)
We claim that the interleaved word fj
constructed in this way is far from the interleaved code
j=0

ϑ

C

(i+ϑ) 2

.
(i)

Lemma 4.22. Under our hypothesis d

f

(i)

,C

(i)



2ϑ
≥ di+ϑ
2 , we have d



(i+ϑ)
fj

2ϑ −1
j=0

ϑ

,C

(i+ϑ) 2



≥ di+ϑ
2 .


2ϑ −1
2ϑ
(i+ϑ)
Proof. We fix an arbitrary interleaved codeword gj
∈ C (i+ϑ) . We define a “lift” g (i) ∈ C (i)
j=0

2ϑ −1
P2ℓ−i−ϑ −1
(i+ϑ)
(i+ϑ)
of gj
in the following way. Writing, for each j ∈ {0, . . . , 2ϑ − 1}, Pj
(X) := k=0
aj,k ·
j=0

(i+ϑ)

Xk
(X) for the polynomial—expressed in coordinates with respect to the i + ϑth -order novel polynomial
(i+ϑ)
(i+ϑ)
basis—for which gj
= Enc(Pj
) holds, we define
P

(i)

(X) :=

ϑ
2X
−1 2ℓ−i−ϑ
X−1

j=0

(i)

aj,k · Xk·2ϑ +j ;

k=0

that is, P (i) ’s list of ith -order coefficients is precisely the 2ϑ -fold interleaving of the polynomials
(i+ϑ)
(i+ϑ)
(X), . . . , P2ϑ −1 (X)’s respective lists of i + ϑth -order coefficients. Finally, we define g (i) := Enc(P (i) ).
P0
2ϑ −1

(i+ϑ)
We argue that the codeword g (i) ∈ C (i) constructed in this way stands in relation to gj
just as
j=0
2ϑ −1

(i+ϑ)
(i.e., it also satisfies a matrix identity analogous to (41) for each y ∈ S (i+ϑ) ). To
f (i) does to fj
j=0

prove this, we fix an arbitrary element y ∈ S (i+ϑ) ; we moreover fix a row-index j ∈ {0, . . . , 2ϑ − 1}. We write
Pϑ−1
(i+ϑ)
(j0 , . . . , jϑ−1 ) for the bits of j (i.e., so that j = k=0 2k ·jk holds). We first note that the functions gj
and

(i)
(i+ϑ)
fold g , j0 , . . . , jϑ−1 agree identically over the domain S
. Indeed, this is a direct consequence of Lemma
(i+ϑ)
4.14 and of the construction of g (i) (gj
(y)’s underlying polynomial’s coefficients are the j th refinement
of g (i) ’s underlying polynomial’s). On the other hand, applying Lemma 4.9 to y ∈ S (i+ϑ) and g (i) , with the
folding tuple (j0 , . . . , jϑ−1 ), we see that the dot product between My ’s j th row and g (i) (x0 ), . . . , g (i) (x2ϑ −1 )

(i+ϑ)
is exactly fold g (i) , j0 , . . . , jϑ−1 (y) = gj
(y), where the latter equality was just argued.
41


Since g (i) ∈ C (i) is a codeword, our hypothesis d(i) f (i) , C (i) ≥ di+ϑ
applies to it. That hypothesis
2
di+ϑ
(i+ϑ)
entails precisely that, for at least 2 elements y ∈ S
, the restrictions f (i) (q(i+ϑ−1) ◦···◦q(i) )−1 ({y}) and
g (i) (q(i+ϑ−1) ◦···◦q(i) )−1 ({y}) are not identically equal. For each such y ∈ S (i+ϑ) , since My is nonsingular (and

2ϑ −1

2ϑ −1
(i+ϑ)
(i+ϑ)
since both f (i) and g (i) satisfy (41)), we conclude that the columns fj
(y)
and gj
(y)
j=0
j=0  ϑ


2ϑ −1
ϑ
2 −1
(i+ϑ)
(i+ϑ)
2ϑ
(i+ϑ) 2
are in turn unequal. Since gj
was arbitrary, we conclude that d
fj
,C
≥
j=0

j=0

di+ϑ
2 .

Applying Lemma 4.22, we conclude directly that the contraposition
2.4 is fulfilled with
j of Theorem
k
di+ϑ −1
(i+ϑ)
2ℓ+R−i−ϑ
respect to the code C
⊂L
, the proximity parameter e :=
, and the interleaved word
2

2ϑ −1
(i+ϑ)
. That theorem’s contraposition immediately implies that the set Ei ⊂ Lϑ consisting of those
fj
j=0
 (i+ϑ) 
′
′
tuples (ri′ , . . . , ri+ϑ−1
) ∈ Lϑ for which d fold f (i) , ri′ , . . . , ri+ϑ−1
,C
< di+ϑ
holds—and here, we use
2
(i+ϑ)
ℓ+R−i−ϑ
S
|
|
(42)—is of mass at most µ(Ei ) ≤ ϑ · 2 |L|
= ϑ · |L| , as required. This completes the proof of the
proposition.
ℓ+R

Proposition 4.23. The probability that any among the bad events E0 , Eϑ , . . . , Eℓ−ϑ occurs is at most 2|L| .
Proof. Applying Proposition 4.21, we upper-bound the quantity of interest as:

ϑ
ϑ
2ϑ
2ℓ+R
ϑ
· (|Sϑ | + · · · + |Sℓ |) =
· 2ℓ+R−ϑ + · · · + 2R ≤
· ϑ
· 2ℓ+R−ϑ ≤
,
|L|
|L|
|L| 2 − 1
|L|
which completes the proof. In the last two steps, we use the geometric series formula and the inequality
ϑ
≤ 1 (which holds for each ϑ ≥ 1), respectively.
2ϑ −1
In light of Proposition 4.23, we freely assume that none of the events E0 , Eϑ , . . . , Eℓ−ϑ occurs. Under
this assumption, we finally turn to the following key proposition.
Proposition 4.24. If any of A’s oracles is not compliant, then V accepts with at most negligible probability.
Proof. We suppose that at least one of A’s oracles is not compliant; we write i∗ ∈ {0, ϑ, . . . , ℓ − ϑ} for the
index of A’s highest-indexed noncompliant oracle.
 d∗

∗
∗
Lemma 4.25. For i∗ ∈ {0, ϑ, . . . , ℓ − ϑ} as above, we have d fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 , f (i +ϑ) ≥ i 2+ϑ .
∗
∗
∗ 
∗
∗
d∗
Proof. Assuming first that d(i ) f (i ) , C (i ) < i 2+ϑ , we write f (i ) ∈ C (i ) for the codeword for which
∗
∗
∗
∗ 
∗ 
d∗
∆(i ) f (i ) , f (i ) < i 2+ϑ . We note that d f (i ) , f (i ) < d2i∗ a fortiori holds; by Definition 4.18 and our

∗
∗
choice of i∗ , we thus must have in turn f (i +ϑ) ̸= fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 . On the other hand, by Lemma


∗
∗
∗ 
∗
∗
d∗
4.19, ∆(i ) f (i ) , f (i ) < i 2+ϑ implies that d fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 , fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 <


∗
di∗ +ϑ
(i∗ ) ′
, ri∗ , . . . , ri′∗ +ϑ−1 , f (i +ϑ) is at least:
2 . Finally, by the reverse triangle inequality, d fold f
 ∗
 ∗


 ∗

 ∗

d f (i +ϑ) , fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 − d fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 , fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 .

∗
∗
∗
Since f (i +ϑ) and fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 are unequal codewords in C (i +ϑ) , this quantity in turn is
d∗
d∗
greater than or equal to di∗ +ϑ − i 2+ϑ ≥ i 2+ϑ , and the proof of the first case is complete.
∗
∗
∗ 
∗
d
In the case d(i ) f (i ) , C (i ) ≥ i 2+ϑ , our assumption whereby Ei∗ didn’t occur implies, by def

∗
∗
∗
∗
d∗
inition, that d fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 , C (i +ϑ) ≥ i 2+ϑ . Since f (i +ϑ) ∈ C (i +ϑ) is a codeword,


∗
∗
d∗
d fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 , f (i +ϑ) ≥ i 2+ϑ in particular holds, and the proof is again complete.
 (i∗ +ϑ) 
∗
′
Lemma 4.26. Whenever its suffix (vi∗ +ϑ , . . . , vℓ+R−1 ) ∈ ∆ fold f (i ) , ri′∗ , . . . , ri+ϑ−1
,f
, V rejects.

42

Proof. We fix an iteration of the query phase’s outer loop for which the lemma’s hypothesis holds. We fix
an arbitrary index i ∈ {i∗ , i∗ + ϑ, . . . , ℓ − ϑ}. If V rejects before finishing the inner loop 3’s ith iteration, then
there’s nothing to prove. We argue that, conditioned on V reaching the end of its ith iteration, we have the
inductive conclusion ci+ϑ ̸= f (i+ϑ) (vi+ϑ , . . . , vℓ+R−1 ) as of the end of that
 iteration.
∗
In the base case i = i∗ , V assigns ci∗ +ϑ := fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 (vi∗ +ϑ , . . . , vℓ+R−1 ) inline on line 6.

∗
′
On the other hand, the hypothesis of the lemma is precisely fold f (i ) , ri′∗ , . . . , ri+ϑ−1
(vi∗ +ϑ , . . . , vℓ+R−1 ) ̸=
∗
(i∗ +ϑ)
(i
+ϑ)
f
(vi∗ +ϑ , . . . , vℓ+R−1 ); we conclude immediately that ci∗ +ϑ ̸= f
(vi∗ +ϑ , . . . , vℓ+R−1 ) will hold as of
the end of the i∗th iteration, as desired.
We fix an index i ∈ {i∗ +ϑ, . . . , ℓ−ϑ}. As of the beginning of the ith iteration, by induction, we have the hypothesis ci ̸= f (i) (vi , . . . , vℓ+R−1 ). If f (i) (vi , . . . , vℓ+R−1 ) = f (i) (vi , . . . , vℓ+R−1 ) moreover holds, then we see
immediately that V will reject on line 5; indeed, in this case ci ̸= f (i) (vi , . . . , vℓ+R−1 ) = f (i) (vi , . . . , vℓ+R−1 )
will hold. We conclude that, conditioned on V reaching the end of its ith iteration, we necessarily
have

f (i) (vi , . . . , vℓ+R−1 ) ̸= f (i) (vi , . . . , vℓ+R−1 ), or in other words (vi , . . . , vℓ+R−1 ) ∈ ∆ f (i) , f (i) . This guar
antee implies a fortiori that (vi+ϑ , . . . , vℓ+R−1 ) ∈ ∆(i) f (i) , f (i) , by definition of this latter set. Using our assumption whereby
the event Ei didn’t occur, we conclude in turn that (vi+ϑ , .. . , vℓ−1 ) ∈

′
′
′
. Since f (i+ϑ) = fold f (i) , ri′ , . . . , ri+ϑ−1
(a conse∆ fold f (i) , ri′ , . . . , ri+ϑ−1
, fold f (i) , ri′ , . . . , ri+ϑ−1
 (i+ϑ) 
∗
(i) ′
′
quence of the maximality of i ), this latter set itself equals ∆ fold f , ri , . . . , ri+ϑ−1 , f
. We conclude

(i+ϑ)
′
(vi+ϑ , . . . , vℓ+R−1 ), so that, after its assignment on
that fold f (i) , ri′ , . . . , ri+ϑ−1
(vi+ϑ , . . . , vℓ+R−1 ) ̸= f
(i+ϑ)
(vi+ϑ , . . . , vℓ+R−1 ), thereby preserving the inductive hypothesis.
line 6, V will obtain ci+ϑ ̸= f
Carrying through the induction, we see finally that either V will abort before finishing its inner loop 3,
or else it will have cℓ ̸= f (ℓ) (vℓ , . . . , vℓ+R−1 ) as of its final check 7. Since c = f (ℓ) (vℓ , . . . , vℓ+R−1 ) holds
?

identically for each v ∈ BR (by definition of this latter oracle), we see that V will reject its check cℓ = c.
We return to the proposition. Lemma 4.25 guarantees (i.e., assuming Ei∗ doesn’t occur) that ci∗ +ϑ ∈


∗
∗
d∗
∆ fold f (i ) , ri′∗ , . . . , ri′∗ +ϑ−1 , f (i +ϑ) will hold with probability at least |S (i∗1+ϑ) | · i 2+ϑ ≥ 12 − 2·21R in each
of the verifier’s query iterations. By Lemma 4.26, the verifier will reject in each such iteration (i.e., assuming
γ
none of the events Ei∗ +ϑ , . . . , Eℓ−ϑ occurs). We see that V will accept with probability at most 12 + 2·21R ,
which is negligible (we recall that R is a positive constant). This completes the proof of the proposition.
In light of Proposition 4.24,
we assume that all of A’s oracles are compliant. Under this assumption, we

note first that d f (0) , C (0) < d20 will hold. We see that Algorithm 1 will terminate successfully in step 2
above. We write t(X0 , . . . , Xℓ−1 ) ∈ L[X0 , . . . , Xℓ−1 ]⪯1 for the polynomial output by E in that step.
′
We now argue that c = t(r0′ , . . . , rℓ−1
) will hold. To this end, we apply Definition 4.18 repeatedly.
P
(0)
(0)
will be the encoding of P (0) (X) =
In the base case i = 0, we note that f
w∈Bℓ t(w) · X{w} (X),
precisely by E’s construction of (t(w))w∈Bℓ . On the other hand, for each i ∈ {0, ϑ, . . . , ℓ − ϑ}, writing
ℓ−i

P (i) (X) ∈ L[X]≺2 for
which Enc(P (i) ) = f (i) holds, and using our assumption f (i+ϑ) =
 the polynomial for (i+ϑ)
(i) ′
′
fold f , ri , . . . , ri+ϑ−1 , we conclude that f
will be exactly the encoding of that polynomial P (i+ϑ) (X) ∈
ℓ−i−ϑ
L[X]≺2
which results from repeatedly applying to P (i) (X) the conclusion of Lemma 4.14 (with the
′
folding challenges ri′ , . . . , ri+ϑ−1
). Carrying out the induction, we see that f (ℓ) will itself be identically equal

P
′
′
′
′
) indeed will hold.
to w∈Bℓ e
fq w0 , . . . , wℓ−1 , r0 , . . . , rℓ−1
· t(w) = t(r0′ , . . . , rℓ−1
), so that c = t(r0′ , . . . , rℓ−1
ℓ
We write (r0 , . . . , rℓ−1 ) ∈ L for the evaluation point output by V and s ∈ L for A’s response. To finish
the proof, we argue that the probability with which s ̸= t(r0 , . . . , rℓ−1 ) and V accepts is negligible. We
assume that s ̸= t(r0 , . . . , rℓ−1 ).
As in Construction 4.12, we write h(X0 , . . . , Xℓ−1 ) := e
fq(r0 , . . . , rℓ−1P
, X0 , . . . , Xℓ−1 ) · t(X0 , . . . , Xℓ−1 )
(here, t(X0 , . . . , Xℓ−1 ) refers to what E extracted).
Since
t(r
,
.
.
.
,
r
)
=
0
ℓ−1
w∈Bℓ h(w), our assumption s ̸=
P
t(r0 , . . . , rℓ−1 ) amounts to the condition s ̸= w∈Bℓ h(w). The soundness analysis of the sumcheck (we refer
to Thaler [Tha22, § 4.1]) states that, under this very assumption, the probability that the verifier accepts its
?

2·ℓ
′
checks si = hi (0) + hi (1) and sℓ = h(r0′ , . . . , rℓ−1
) holds is at most |L|
over V’s choice of its folding challenges
′
′
′
′
′
′
(r0 , . . . , rℓ−1 ). We thus assume that sℓ ̸= h(r0 , . . . , rℓ−κ−1 ) = e
fq(r0 , . . . , rℓ−1 , r0′ , . . . , rℓ−1
) · t(r0′ , . . . , rℓ−1
).
′
′
Our conclusion whereby c = t(r0 , . . . , rℓ−1 ), established above, thus implies that V will reject its check
?

′
sℓ = e
fq(r0 , . . . , rℓ−1 , r0′ , . . . , rℓ−1
) · c. This completes the proof of the theorem.

43

Remark 4.27. In our proof of Theorem 4.17 above, our emulator E runs the Berlekamp–Welch decoder on
the adversary-supplied word f : S (0) → L (see its step 2). Most analyses of that algorithm (see e.g. [Gur06,
Rem. 4]) assume input guaranteed to reside within the unique decoding radius, and implicitly leave undefined
the algorithm’s behavior on arbitrary words. The behavior of Algorithm 1 on a general word f : S (0) → L
is not obvious. As far as our proof of Theorem 4.17 is concerned, we need merely the guarantee whereby,
regardless of its input, Algorithm 1—and hence also E—will run in strict polynomial time. (This guarantee
follows self-evidently from Algorithm 1’s description.) Indeed, if A submits a word f outside of the unique
decoding radius, then—as our Propositions 4.23 and 4.24 above show—V will reject with overwhelming
probability in any case, so that E’s output ultimately doesn’t matter. As it happens—and as we show in
Lemma 2.1 above—on input f outside of the unique decoding radius, Algorithm 1 will return ⊥.

4.4

Efficiency

We discuss the efficiency of Construction 4.12. We count L-operations throughout. Though the choices
deg(L / F2 ) = ω(log λ) and γ = ω(log λ) suffice to make that construction’s soundness error negligible, we in
fact set deg(L / F2 ) = λ and γ = λ. These latter choices make Construction 4.12 exponentially secure, and
make its efficiency easier to analyze. As usual, we understand the positive integers R and ϑ as constants.
Prover cost. In its commitment phase 2, our prover must use Lin, Chung and Han’s additive NTT [LCH14]
to encode its length-2ℓ vector (t(w))w∈Bℓ onto S (0) (see also Algorithm 2 above). For this task, 2ℓ+R−1 · ℓ
L-multiplications and 2ℓ+R · ℓ L-additions suffice (see also Subsection 2.3).
To prepare its sumcheck 2, the prover must tensor-expand (f
eq(r0 , . . . , rℓ−1 , w0 , . . . , wℓ−1 ))w∈Bℓ ; this task
ℓ
ℓ
takes 2 L-additions and 2 L-multiplications (recall Subsection 2.1). Our prover can carry out that sumcheck
itself also in O(2ℓ ) time (we refer to Thaler [Tha22, § 4.1]). Our prover is thus linear-time.
Verifier cost. To carry out the sumcheck 2, Construction 4.12’s verifier must expend just O(ℓ) L′
operations. It can compute e
fq(r0 , . . . , rℓ−1 , r0′ , . . . , rℓ−1
) in step 3 in again O(ℓ) L-operations. During its
querying phase 4, the verifier must finally, for γ = λ repetitions, make 2ϑ · ϑℓ = O(ℓ) queries to the IOP oracle

and perform O 2ϑ · ϑℓ = O(ℓ) L-operations. Its total cost during that phase is thus O(λ · ℓ) L-operations.
The BCS transform. In practice, we must use Ben-Sasson, Chiesa and Spooner’s transformation [BCS16]
to turn Construction 4.12 into an interactive protocol in the random oracle model. The resulting compiled
protocol imposes further costs on both parties, as we now explain. First, its prover must moreover Merklehash both f (0) during its commitment phase and the positive-indexed oracles f (ϑ) , . . . , f (ℓ−ϑ) during its
evaluation phase; these commitments represent total work on the order of O 2ℓ+R = O 2ℓ hash invocations.
During the querying phase, both parties must handle Merkle paths. During each query repetition, the
total length of all Merkle paths sent (measured in digests) is on the order of O((ℓ + R)2 ) = O(ℓ2 ). Since
there are γ = λ total repetitions, the total cost for both parties during the querying phase is thus O(λ · ℓ2 )
hash operations.

5

Unrolled Small-Field IOPCS

In this section, we describe a one-shot small-field IOPCS construction. This construction inlines the largefield IOPCS of Section 4 into the ring-switching reduction of Section 3. We moreover streamline the resulting
combination, by applying a few optimizations. That is, we unify Construction 4.12’s sumcheck with that
already required within Construction 3.1.

5.1

Combined Small-Field Protocol

We present our full combined protocol below. Our protocol directly instantiates the generic small-field
template of Definition 2.10, though we insist below that the small field K supplied to that template be
binary.

44

CONSTRUCTION 5.1 (Combined Small-Field IOPCS).
We define Π = (Setup, Commit, P, V) as follows.
1. params ← Π.Setup(1λ , ℓ, K). On input 1λ , ℓ, and K, choose a constant, positive rate parameter
R ∈ N and an extension field L/K of power-of-two degree 2κ (say) over K, whose degree, say r, over
L
F2 satisfies r = ω(log λ) and r ≥ ℓ+R. Write ℓ′ := ℓ−κ and A := L⊗K L. Initialize the oracle FVec
.
′
′
Fix a folding factor ϑ | ℓ and a repetition parameter γ = ω(log λ). Write (X0 (X), . . . , X2ℓ −1 (X))
ℓ′

′

′

for the novel L-basis of L[X]≺2 , and fix S (0) , . . . , S (ℓ ) and q (0) , . . . , q (ℓ −1) as in Subsection 4.1.
′
′
ℓ′ +R
for the Reed–Solomon code RSL,S (0) [2ℓ +R , 2ℓ ].
Write C (0) ⊂ L2
2. [f ] ← Π.Commit(params, t). On input t(X0 , . . . , Xℓ−1 ) ∈ K[X0 , . . . , Xℓ−1 ]⪯1 , construct as in
Definition 2.2 the packed polynomial t′ (X0 , . . . , Xℓ′ −1 ) ∈ L[X0 , . . . , Xℓ′ −1 ]⪯1 . Write P (X) :=
P
′
v∈Bℓ′ t (v)·X{v} (X) for its univariate flattening. Using Algorithm 2, compute the Reed–Solomon
codeword f : S (0) → L defined by f : x 7→ P (x). Submit (submit, ℓ′ + R, f ) to the vector oracle
L
FVec
. Upon receiving (receipt, ℓ′ + R, [f ]) from the oracle, output the commitment [f ].
We define (P, V) as the following IOP, in which both parties have the common input [f ], s ∈ L, and
(r0 , . . . , rℓ−1 ) ∈ Lℓ , and P has the further input t(X0 , . . . , Xℓ−1 ) ∈ L[X0 , . . . , Xℓ−1 ]⪯1 .
1. P computes ŝ := φ1 (t′ )(φ0 (rκ ), . . . , φ0 (rℓ−1 )) and sends V the A-element ŝ.
2. V decomposes ŝ =:

? P
fq(v0 , . . . , vκ−1 , r0 , . . . , rκ−1 ) · ŝv .
v∈Bκ ŝv ⊗ βv . V requires s =
v∈Bκ e

P

′′
3. V samples batching scalars (r0′′ , . . . , rκ−1
) ← Lκ and sends them to P.
P
4. For each w ∈ Bℓ′ , P
P decomposes e
fq(rκ , . . . , rℓ−1 , w0 , . . . , wℓ′ −1 ) =: u∈Bκ Aw,u · βu . P defines the
′′
function A : w 7→ u∈Bκ e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · Aw,u on Bℓ′ and writes A(X0 , . . . , Xℓ′ −1 )
for its multilinear extension. P defines h(X0 , . . . , Xℓ′ −1 ) := A(X0 , . . . , Xℓ′ −1 ) · t′ (X0 , . . . , Xℓ′ −1 ).
P
P
′′
5. V decomposes ŝ =: u∈Bκ βu ⊗ ŝu , and sets s0 := u∈Bκ e
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
) · ŝu .

6. P and V both abbreviate f (0) := f , and execute the following loop:
1: for i ∈ {0, . . . , ℓ′ − 1} do

P
′
, X, w0 , . . . , wℓ′ −i−2 .
2:
P sends V the polynomial hi (X) := w∈Bℓ′ −i−1 h r0′ , . . . , ri−1
?

′
′
V requires si = hi (0) + hi (1). V samples ri′ ← L, sets si+1
 := hi (ri ), and sends P ri .
(i+1)
(i+1)
(i) ′
P defines f
:S
→ L as the function fold f , ri of Definition 4.6.
′
5:
if i + 1 = ℓ′ then P sends c := f (ℓ ) (0, . . . , 0) to V.
6:
else if ϑ | i + 1 then P submits (submit, ℓ′ + R − i − 1, f (i+1) ) to the oracle.

P
7. V sets e := e
fq φ0 (rκ ), . . . , φ0 (rℓ−1 ), φ1 (r0′ ), . . . , φ1 (rℓ′ ′ −1 ) and decomposes e =: u∈Bκ βu ⊗ eu .

3:
4:

?

8. V requires sℓ′ =

′′
) · eu
fq(u0 , . . . , uκ−1 , r0′′ , . . . , rκ−1
u∈Bκ e

P



· c.

9. V executes the following querying procedure:
1: for γ repetitions do
2:
V samples v ← Bℓ′ +R randomly.
3:
for i ∈ {0, ϑ, . . . , ℓ′ − ϑ} (i.e., taking ϑ-sized steps) do

4:
for each u ∈ Bϑ , V sends query, [f (i) ], (u0 , . . . , uϑ−1 , vi+ϑ , . . . , vℓ′ +R−1 ) to the oracle.
5:
6:
7:

?

if i > 0 then V requires ci = f (i) (vi , . . . , vℓ′ +R−1 ).

′
V defines ci+ϑ := fold f (i) , ri′ , . . . , ri+ϑ−1
(vi+ϑ , . . . , vℓ′ +R−1 ).
?

V requires cℓ′ = c.

The completeness and security of Construction 5.1 are a consequence of techniques that we’ve already
expounded in Sections 3 and 4 above.
45

5.2

Efficiency

In this subsection, we discuss the concrete efficiency of our combined Construction 5.1.
We make use of various concrete proof size optimizations in our implementation. For example, for each
oracle i ∈ {0, ϑ, . . . , ℓ′ − ϑ}, we opt to send the entire j th layer of the Merkle tree—as opposed to just its
root—for an appropriately chosen constant j ≥ 0. This optimization is a fixed-cost–variable-cost tradeoff.
As j grows, the size of each Merkle commitment grows (exponentially); on the other hand, each of the γ
subsequently sent paths becomes shorter. The optimal truncation height turns out to be j := ⌈log2 (γ)⌉.
Each path ultimately sent is of size ℓ′ + R − i − ϑ − j.
As soon as i ∈ {0, ϑ, . . . , ℓ′ − ϑ} becomes so large that j > ℓ′ + R − i − ϑ holds, this optimization
becomes nonsensical; at this point, we instruct our prover rather to terminate FRI early and send its entire
message to the verifier in the clear. (This measure further allows us to drop our assumption ϑ | ℓ′ , which
we picked up merely for notational convenience in the first place.) It can be shown that this termination
strategy outperforms—i.e., yields smaller proofs than—each strategy that terminates strictly after it. On
the other hand, terminating even earlier can do good in some settings. The optimal termination point seems
complicated to predict in general, i.e. as a function of θ, ℓ′ , R, γ and j (as well as of global parameters like
L, K, and the hash digest width).
Concrete soundness. In order to select the query repetition parameter γ, we have to understand the
concrete security of our protocol. It follows essentially from the proofs of Theorems 3.5 and 4.17 that
Construction 5.1’s concrete soundness error is bounded from above by

γ
′
2ℓ +R
1
1
κ + 2 · ℓ′
+
+
+
.
(43)
|L|
|L|
2 2 · 2R
Above, the first summand comes from ring-switching and the sumcheck; the latter two reflect Propositions
4.23 and 4.24, respectively. For each desired concrete security level Ξ, we thus set γ minimally so that
(43) becomes bounded from above by Ξ. (Clearly, this is possible only when L is sufficiently large that
′
′
2ℓ +R
Ξ > κ+2·ℓ
|L| + |L| holds.) We say in this case that Construction 5.1 attains − log2 (Ξ) bits of security.
Proof sizes. In Table 1 below, we tabulate various proof sizes. We compare [DP25, Cons. 3.11] and
Construction 5.1 on input polynomials t(X0 , . . . , Xℓ−1 ) over F2 . In each case, we process evaluation claims
over F2128 , and attain 100 bits of provable security. We vary both the number of variables ℓ and the rate
parameter R. We recall that ρ = 21R ; we thus test the rates ρ = 12 , ρ = 14 , and ρ = 18 in each case.
In our benchmarks of Construction 5.1, we use the folding factor ϑ := 4 and the Merkle cap height j := 8.
Num. Variables ℓ

Log Inverse Rate R

[DP25, Cons. 3.11]

Construction 5.1

1

0.922 MiB

0.202 MiB

2

0.758 MiB

0.143 MiB

3

0.681 MiB

0.129 MiB

1

3.562 MiB

0.342 MiB

2

2.935 MiB

0.237 MiB

3

2.629 MiB

0.210 MiB

1

14.050 MiB

0.514 MiB

2

11.598 MiB

0.351 MiB

3

14.376 MiB

0.336 MiB

24 (2 MiB)

28 (32 MiB)

32 (512 MiB)

Table 1: Proof sizes of polynomial commitment schemes for F2 -multilinears.
46

Concrete performance. We concretely benchmark this work’s Construction 5.1, as well as the univariateFRI-based scheme Plonky3 . Our benchmarks of our scheme use Binius, an open-source SNARK that uses
this work as its PCS.
In each of our performance benchmarks below,
 we use a 128-bit field. In this work, we use the GHASH
128
7
2
∼
[GLL19] field
F
[X]
/
X
+
X
+
X
+
X
+
1
= F2128 . In Plonky3, we use the quartic extension Fp [X] /
2

4
X − 11 ∼
= Fp4 of the Baby Bear prime field Fp , where p := 231 − 227 + 1.
Throughout our concrete benchmarks, we use the code rate ρ = 21 and attain 100 bits of provable
security. We work exclusively in the unique-decoding regime. Both [DP25, Cons. 3.11] and this work have
been proven secure only in that regime (thus far). As for Plonky3, we note that it’s impossible to obtain
100 bits of provable security in the list-decoding regime over a field of merely 128 bits. The best available
proximity gap in that regime, namely [Ben+23, Thm. 5.1], has a batching error [Ben+23, (5.3)] whose
numerator grows quadratically in the problem size. On each reasonably large problem instance, that result
will yield an error term greater than or equal to 2−100 , thus making the protocol’s total error greater than
2−100 regardless of the number of queries made. Our benchmarks below thus reflect the best-possible proof
size attainable in Plonky3, conditioned on the 100-bit security level and the use of a 128-bit field.
We benchmark this work’s Construction 5.1 on 28-variate polynomials over F2 . We benchmark Plonky3’s
performance on batches consisting of 16 polynomials over Baby Bear, each of degree 224 . The total number
of coefficients is thus the same—i.e., 228 —in all benchmarks.
We explain this methodology further. The most natural benchmark would have compared our scheme’s
performance on 28-variate multilinear polynomials to Plonky3’s on single, degree-228 univariate polynomials.
On the other hand, Plonky3’s FRI-PCS implementation is heavily optimized towards the batched case. In
order to be more fair to Plonky3, we run that work in the batched setting. As it happens, in our own
(non-batched) scheme, we incorporate a straightforward interleaving optimization that serves to reduce by 4
the number of butterfly stages our commitment phase must carry out. In sum, both our scheme (operating
on single multilinear polynomials) and Plonky3’s (operating on size-16 batches of univariate polynomials)
must perform NTTs of essentially the same shape and size. This fact makes our benchmarks comparable.
Plonky3 does not implement the higher-arity FRI folding or Merkle-caps optimizations. In order to make
it easier to make apples-to-apples comparisons between our work and theirs, we benchmark our scheme both
with and without these optimizations enabled. We note that these optimizations are free wins, and yield a
monotonically superior protocol. There is no reason—in any practical deployment—that we would voluntarily
switch them off; the purpose is just for comparison. We emphasize that our proofs become much larger when
these optimizations are disabled. We write “Cons. 5.1*” for the unoptimized version of our scheme, i.e. for
which Merkle caps and oracle-skipping are both switched off (in the language used above, we set ϑ = 1 and
also j = 0).
In our benchmarks, we use an AWS machine of type c7i.8xlarge, with a 4th -generation Intel Xeon
Scalable processor, 32 virtual cores, and 64 GiB of memory. Both the Binius and Plonky3 implementations leverage AVX-512 accelerated instructions; Binius moreover uses the Intel Carry-less Multiplication
(CLMUL) instruction set extension. In all schemes, we use SHA-256 for Merklization. We present both
singlethreaded (ST) and multithreaded (MT) benchmarks. We present our results in Table 2 below.
Protocol

Proof Size

Commit (ST)

Commit (MT)

Prove (ST)

Prove (MT)

Verify (ST)

Plonky3

1.931 MiB

24,893 ms

3,991 ms

13,257 ms

1,794 ms

18.2 ms

Cons. 5.1

0.351 MiB

144 ms

44 ms

160 ms

71 ms

0.7 ms

Cons. 5.1*

0.930 MiB

143 ms

44 ms

181 ms

75 ms

1.8 ms

Table 2: Concrete performance benchmarks.
Our scheme outperforms the state-of-the-art scheme Plonky3 by between one and two orders of magnitude,
across all computational performance metrics. Our proofs are also smaller than theirs, by more than fivefold.

47

References
[ACFY25]

Gal Arnon, Alessandro Chiesa, Giacomo Fenzi, and Eylon Yogev. “WHIR: Reed–Solomon
Proximity Testing with Super-Fast Verification”. In: Advances in Cryptology – EUROCRYPT
2025. Ed. by Serge Fehr and Pierre-Alain Fouque. Cham: Springer Nature Switzerland, 2025,
pp. 214–243. isbn: 978-3-031-91134-7.

[AER24]

Guillermo Angeris, Alex Evans, and Gyumin Roh. A Note on Ligero and Logarithmic Randomness. Cryptology ePrint Archive, Paper 2024/1399. 2024. url: https://eprint.iacr.
org/2024/1399.

[AHIV23]

Scott Ames, Carmit Hazay, Yuval Ishai, and Muthuramakrishnan Venkitasubramaniam.
“Ligero: lightweight sublinear arguments without a trusted setup”. In: Designs, Codes and
Cryptography (2023). doi: 10.1007/s10623-023-01222-8.

[BBHR18a]

Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev. “Fast Reed–Solomon Interactive Oracle Proofs of Proximity”. In: International Colloquium on Automata, Languages,
and Programming. Ed. by Ioannis Chatzigiannakis, Christos Kaklamanis, Dániel Marx, and
Donald Sannella. Vol. 107. Leibniz International Proceedings in Informatics. Dagstuhl, Germany: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, 2018, 14:1–14:17. doi: 10.4230/
LIPIcs.ICALP.2018.14.

[BBHR18b]

Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev. Scalable, transparent, and
post-quantum secure computational integrity. Cryptology ePrint Archive, Paper 2018/046. 2018.
url: https://eprint.iacr.org/2018/046.

[BCG20]

Jonathan Bootle, Alessandro Chiesa, and Jens Groth. “Linear-Time Arguments with Sublinear
Verification from Tensor Codes”. In: Theory of Cryptography. Ed. by Rafael Pass and Krzysztof
Pietrzak. Cham: Springer International Publishing, 2020, pp. 19–46. isbn: 978-3-030-64378-2.
doi: 10.1007/978-3-030-64378-2_2.

[BCS16]

Eli Ben-Sasson, Alessandro Chiesa, and Nicholas Spooner. “Interactive Oracle Proofs”. In:
International Conference on Theory of Cryptography. Vol. 9986. Berlin, Heidelberg: SpringerVerlag, 2016, pp. 31–60. isbn: 978-3-662-53644-5. doi: 10.1007/978-3-662-53644-5_2.

[Ben+23]

Eli Ben-Sasson, Dan Carmon, Yuval Ishai, Swastik Kopparty, and Shubhangi Saraf. “Proximity
Gaps for Reed–Solomon Codes”. In: Journal of the ACM 70.5 (Oct. 2023). doi: 10.1145/
3614423.

[BGKS19]

Eli Ben-Sasson, Lior Goldberg, Swastik Kopparty, and Shubhangi Saraf. DEEP-FRI: Sampling
Outside the Box Improves Soundness. Cryptology ePrint Archive, Paper 2019/336. 2019. url:
https://eprint.iacr.org/2019/336.

[Bre+25]

Martijn Brehm, Binyi Chen, Ben Fisch, Nicolas Resch, Ron D. Rothblum, and Hadas Zeilberger. “Blaze: Fast SNARKs from Interleaved RAA Codes”. In: Advances in Cryptology –
EUROCRYPT 2025. Ed. by Serge Fehr and Pierre-Alain Fouque. Cham: Springer Nature
Switzerland, 2025, pp. 123–152. isbn: 978-3-031-91134-7.

[Can89]

David G Cantor. “On arithmetical algorithms over finite fields”. In: Journal of Combinatorial
Theory, Series A 50.2 (1989), pp. 285–300. doi: https://doi.org/10.1016/0097-3165(89)
90020-4.

[CBBZ23]

Binyi Chen, Benedikt Bünz, Dan Boneh, and Zhenfei Zhang. “HyperPlonk: Plonk with LinearTime Prover and High-Degree Custom Gates”. In: Advances in Cryptology – EUROCRYPT
2023. Ed. by Carmit Hazay and Martijn Stam. Vol. 14005. Lecture Notes in Computer Science.
Cham: Springer Nature Switzerland, 2023.

[Chi+20]

Alessandro Chiesa, Yuncong Hu, Mary Maller, Pratyush Mishra, Noah Vesely, and Nicholas
Ward. “Marlin: Preprocessing zkSNARKs with Universal and Updatable SRS”. In: Advances
in Cryptology – EUROCRYPT 2020. Ed. by Anne Canteaut and Yuval Ishai. Lecture Notes in
Computer Science. Full version. Cham: Springer International Publishing, 2020, pp. 738–768.
isbn: 978-3-030-45721-1. doi: 10.1007/978-3-030-45721-1_26.

48

[COS20]

Alessandro Chiesa, Dev Ojha, and Nicholas Spooner. “Fractal: Post-quantum and Transparent
Recursive Proofs from Holography”. In: Advances in Cryptology – EUROCRYPT 2020. Ed. by
Anne Canteaut and Yuval Ishai. Cham: Springer International Publishing, 2020, pp. 769–793.
isbn: 978-3-030-45721-1. doi: 10.1007/978-3-030-45721-1{\_}27.

[DG25]

Benjamin E. Diamond and Angus Gruen. “Proximity Gaps in Interleaved Codes”. In: IACR
Communications in Cryptology 1.4 (Jan. 13, 2025). issn: 3006-5496. doi: 10.62056/a0ljbkrz.

[DP24]

Benjamin E. Diamond and Jim Posen. “Proximity Testing with Logarithmic Randomness”. In:
IACR Communications in Cryptology 1.1 (2024). issn: 3006-5496. doi: 10.62056/aksdkp10.

[DP25]

Benjamin E. Diamond and Jim Posen. “Succinct Arguments over Towers of Binary Fields”.
In: Advances in Cryptology – EUROCRYPT 2025. Ed. by Serge Fehr and Pierre-Alain Fouque.
Cham: Springer Nature Switzerland, 2025, pp. 93–122. isbn: 978-3-031-91134-7.

[GG13]

Joachim von zur Gathen and Jürgen Gerhard. Modern Computer Algebra. 3rd Edition. Cambridge University Press, 2013.

[GLL19]

Shay Gueron, Adam Langley, and Yehuda Lindell. AES-GCM-SIV: Nonce Misuse-Resistant
Authenticated Encryption. RFC 8452. Apr. 2019.

[GM10]

Shuhong Gao and Todd Mateer. “Additive Fast Fourier Transforms Over Finite Fields”. In:
IEEE Transactions on Information Theory 56.12 (2010), pp. 6265–6272. doi: 10.1109/TIT.
2010.2079016.

[Gol+23]

Alexander Golovnev, Jonathan Lee, Srinath Setty, Justin Thaler, and Riad S. Wahby. “Brakedown: Linear-Time and Field-Agnostic SNARKs for R1CS”. In: Advances in Cryptology –
CRYPTO 2023. Ed. by Helena Handschuh and Anna Lysyanskaya. Cham: Springer Nature
Switzerland, 2023, pp. 193–226. doi: 10.1007/978-3-031-38545-2_7.

[Gur06]

Venkatesan Guruswami. Algorithmic Results in List Decoding. Vol. 2. Foundations and Trends
in Theoretical Computer Science 2. now publishers, 2006. doi: 10.1561/0400000007.

[GWC19]

Ariel Gabizon, Zachary J. Williamson, and Oana Ciobotaru. PLONK: Permutations over
Lagrange-bases for Oecumenical Noninteractive arguments of Knowledge. Cryptology ePrint
Archive, Paper 2019/953. 2019. url: https://eprint.iacr.org/2019/953.

[Hab22]

Ulrich Haböck. A summary on the FRI low degree test. Cryptology ePrint Archive, Paper
2022/1216. 2022. url: https://eprint.iacr.org/2022/1216.

[Hab24]

Ulrich Haböck. A note on the G-FFT. Cryptology ePrint Archive, Paper 2024/1036. 2024. url:
https://eprint.iacr.org/2024/1036.

[HLP24]

Ulrich Haböck, David Levit, and Shahar Papini. Circle STARKs. Cryptology ePrint Archive,
Paper 2024/278. 2024. url: https://eprint.iacr.org/2024/278.

[LAH16]

S. -J. Lin, T. Y. Al-Naffouri, and Y. S. Han. “FFT Algorithm for Binary Extension Finite
Fields and Its Application to Reed–Solomon Codes”. In: IEEE Transactions on Information
Theory 62.10 (2016), pp. 5343–5358. doi: 10.1109/TIT.2016.2600417.

[Lan02]

Serge Lang. Algebra. Revised Third Edition. Vol. 211. Graduate Texts in Mathematics.
Springer, 2002.

[LCH14]

Sian-Jheng Lin, Wei-Ho Chung, and Yunghsiang S. Han. “Novel Polynomial Basis and Its Application to Reed–Solomon Erasure Codes”. In: IEEE 55th Annual Symposium on Foundations
of Computer Science. 2014, pp. 316–325. doi: 10.1109/FOCS.2014.41.

[Li+18]

Wen-Ding Li, Ming-Shing Chen, Po-Chun Kuo, Chen-Mou Cheng, and Bo-Yin Yang. “Frobenius Additive Fast Fourier Transform”. In: ACM International Symposium on Symbolic and
Algebraic Computation. 2018. isbn: 9781450355506. doi: 10.1145/3208976.3208998.

[LN96]

Rudolf Lidl and Harald Niederreiter. Finite Fields. Ed. by G.-C. Rota. 2nd. Vol. 20. Encyclopedia of Mathematics and its Applications. Cambridge University Press, 1996. doi: 10.1017/
CBO9780511525926.

49

[LX24]

Songsong Li and Chaoping Xing. “Fast Fourier transform via automorphism groups of rational
function fields”. In: ACM-SIAM Symposium on Discrete Algorithms. 2024, pp. 3836–3859. doi:
10.1137/1.9781611977912.135. url: https://epubs.siam.org/doi/abs/10.1137/1.
9781611977912.135.

[MBKM19]

Mary Maller, Sean Bowe, Markulf Kohlweiss, and Sarah Meiklejohn. “Sonic: Zero-Knowledge
SNARKs from Linear-Size Universal and Updatable Structured Reference Strings”. In: Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security.
2019, pp. 2111–2128. isbn: 9781450367479. doi: 10.1145/3319535.3339817.

[RR24]

Noga Ron-Zewi and Ron Rothblum. “Local Proofs Approaching the Witness Length”. In:
Journal of the ACM 71.3 (June 2024). doi: 10.1145/3661483. url: https://doi.org/10.
1145/3661483.

[Set20]

Srinath Setty. “Spartan: Efficient and General-Purpose zkSNARKs Without Trusted Setup”.
In: Advances in Cryptology – CRYPTO 2020. Ed. by Daniele Micciancio and Thomas Ristenpart. Cham: Springer International Publishing, 2020, pp. 704–737. isbn: 978-3-030-56877-1.
doi: 10.1007/978-3-030-56877-1_25.

[Sou24]

Lev Soukhanov. “Hashcaster”. Unpublished report. Sept. 2024. url: https://hackmd.io/
@levs57/SJ4fuZMD0#Hashcaster.

[Tha22]

Justin Thaler. Proofs, Arguments and Zero-Knowledge. Vol. 4. Foundations and Trends in
Privacy and Security 2–4. now publishers, 2022.

[Wat79]

William C. Waterhouse. “The Normal Basis Theorem”. In: The American Mathematical
Monthly 86.3 (1979), pp. 212–212. url: http://www.jstor.org/stable/2321527.

[Xie+19]

Tiacheng Xie, Jiaheng Zhang, Yupeng Zhang, Charalampos Papamanthou, and Dawn Song.
“Libra: Succinct Zero-Knowledge Proofs with Optimal Prover Computation”. In: Advances in
Cryptology – CRYPTO 2019. Berlin, Heidelberg: Springer-Verlag, 2019, pp. 733–764. isbn:
978-3-030-26953-1. doi: 10.1007/978-3-030-26954-8_24.

[ZCF24]

Hadas Zeilberger, Binyi Chen, and Ben Fisch. “BaseFold: Efficient Field-Agnostic Polynomial
Commitment Schemes from Foldable Codes”. In: Advances in Cryptology – CRYPTO 2024.
Berlin, Heidelberg: Springer-Verlag, 2024, pp. 138–169. isbn: 978-3-031-68402-9. doi: 10.1007/
978-3-031-68403-6_5.

[ZXZS20]

Jiaheng Zhang, Tiancheng Xie, Yupeng Zhang, and Dawn Song. “Transparent Polynomial
Delegation and Its Applications to Zero Knowledge Proof”. In: IEEE Symposium on Security
and Privacy. 2020, pp. 859–876. isbn: 2375-1207. doi: 10.1109/SP40000.2020.00052.

50

